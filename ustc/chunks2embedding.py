from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue
from langchain_core.documents import Document
from embedding import QwenEmbedding
import os
import logging
from md2chunks import parse_markdown_file, parse_markdown_file_api
from typing import List, Dict, Any, Tuple, Optional
from qdrant_client.models import ScoredPoint

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class EnhancedQdrantVectorStore:
    """å®Œå…¨è‡ªå®šä¹‰çš„å‘é‡å­˜å‚¨ç±»ï¼Œé€‚é…QdrantæœåŠ¡å™¨æ¨¡å¼"""

    def __init__(
            self,
            client: Any,
            collection_name: str,
            embedding: Any
    ):
        self.client = client
        self.collection_name = collection_name
        self.embedding_model = embedding

    def create_collection_if_not_exists(self, vector_size: int = 896):
        """åˆ›å»ºæ”¯æŒå¤šå‘é‡çš„é›†åˆï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰"""
        try:
            # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨ - ä½¿ç”¨collections_list API
            collections = self.client.get_collections()
            existing_collections = [col.name for col in collections.collections]
            
            if self.collection_name in existing_collections:
                print(f"âœ… é›†åˆ {self.collection_name} å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")
                return
        except Exception as e:
            print(f"âš ï¸ æ£€æŸ¥é›†åˆæ—¶å‡ºé”™: {e}")
        
        # åˆ›å»ºé›†åˆ
        try:
            print(f"ğŸ”„ åˆ›å»ºé›†åˆ {self.collection_name}")
            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config={
                    "title": VectorParams(size=vector_size, distance=Distance.COSINE),
                    "content": VectorParams(size=vector_size, distance=Distance.COSINE)
                }
            )
            print(f"âœ… é›†åˆ {self.collection_name} åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            print(f"âŒ åˆ›å»ºé›†åˆå¤±è´¥: {e}")
            # å¦‚æœé›†åˆå·²å­˜åœ¨ï¼Œå¿½ç•¥é”™è¯¯
            if "already exists" in str(e).lower():
                print(f"âœ… é›†åˆ {self.collection_name} å·²å­˜åœ¨")
            else:
                raise e

    def weighted_hybrid_search(
            self,
            query: str,
            k: int = 5,
            title_weight: float = 0.7,
            content_weight: float = 0.3
    ) -> List[Tuple[Document, float]]:
        """åŠ æƒèåˆæœç´¢æ ‡é¢˜å’Œå†…å®¹ï¼ˆå…¼å®¹Qdrant 1.7.0+ï¼‰"""
        try:
            # è·å–æŸ¥è¯¢å‘é‡
            query_vector = self.embedding_model.embed_query(query)

            # åˆ†åˆ«æœç´¢æ ‡é¢˜å’Œå†…å®¹ - ä½¿ç”¨æ–°ç‰ˆAPIæ ¼å¼
            title_results = self.client.search(
                collection_name=self.collection_name,
                query_vector=("title", query_vector),  # âœ… æ–°ç‰ˆAPIæ ¼å¼
                query_filter=None,
                with_payload=True,
                with_vectors=False,
                limit=k * 3
            )

            content_results = self.client.search(
                collection_name=self.collection_name,
                query_vector=("content", query_vector),  # âœ… æ–°ç‰ˆAPIæ ¼å¼
                query_filter=None,
                with_payload=True,
                with_vectors=False,
                limit=k * 3
            )

            # åˆå¹¶ç»“æœå¹¶è®¡ç®—åŠ æƒåˆ†æ•°
            combined_results = {}
            
            # å¤„ç†æ ‡é¢˜æœç´¢ç»“æœ
            for result in title_results:
                doc_id = result.id
                if doc_id not in combined_results:
                    combined_results[doc_id] = {
                        'title_score': result.score * title_weight,
                        'content_score': 0.0,
                        'payload': result.payload,
                        'total_score': result.score * title_weight
                    }
                else:
                    combined_results[doc_id]['title_score'] = result.score * title_weight
                    combined_results[doc_id]['total_score'] += result.score * title_weight

            # å¤„ç†å†…å®¹æœç´¢ç»“æœ
            for result in content_results:
                doc_id = result.id
                if doc_id not in combined_results:
                    combined_results[doc_id] = {
                        'title_score': 0.0,
                        'content_score': result.score * content_weight,
                        'payload': result.payload,
                        'total_score': result.score * content_weight
                    }
                else:
                    combined_results[doc_id]['content_score'] = result.score * content_weight
                    combined_results[doc_id]['total_score'] += result.score * content_weight

            # ä¸ºQAå¯¹ç±»å‹çš„å†…å®¹å¢åŠ Qçš„æƒé‡
            for doc_id, result in combined_results.items():
                payload = result['payload']
                # æ£€æŸ¥æ˜¯å¦ä¸ºQAå¯¹
                if payload.get('metadata', {}).get('type') == 'qa':
                    # QAå¯¹ä¸­é—®é¢˜æƒé‡å¢åŠ 
                    result['total_score'] *= 1.5  # å¢åŠ 50%çš„æƒé‡
                    # å¦‚æœæ ‡é¢˜åˆ†æ•°è¾ƒé«˜ï¼ˆé—®é¢˜åŒ¹é…ï¼‰ï¼Œè¿›ä¸€æ­¥å¢åŠ æƒé‡
                    if result['title_score'] > result['content_score']:
                        result['total_score'] *= 1.3  # é—®é¢˜åŒ¹é…å†å¢åŠ 30%æƒé‡

            # æŒ‰æ€»åˆ†æ’åºå¹¶è¿”å›å‰kä¸ªç»“æœ
            sorted_results = sorted(
                combined_results.items(),
                key=lambda x: x[1]['total_score'],
                reverse=True
            )[:k]

            # è½¬æ¢ä¸ºDocumentå¯¹è±¡
            documents = []
            for doc_id, result in sorted_results:
                payload = result['payload']
                # å¯¹äºQAå¯¹ï¼Œè¿”å›å®Œæ•´çš„é—®ç­”å†…å®¹
                if payload.get('metadata', {}).get('type') == 'qa':
                    # æ„å»ºå®Œæ•´çš„QAå¯¹å†…å®¹
                    question = payload.get('metadata', {}).get('question', '')
                    answer = payload.get('metadata', {}).get('answer', '')
                    qa_content = f"é—®é¢˜ï¼š{question}\n\nç­”æ¡ˆï¼š{answer}"
                    
                    # åˆ›å»ºDocumentå¯¹è±¡ï¼Œæ ‡è®°ä¸ºQAå¯¹
                    doc = Document(
                        page_content=qa_content,
                        metadata={
                            **payload.get('metadata', {}),
                            'is_qa_pair': True,  # æ ‡è®°è¿™æ˜¯QAå¯¹
                            'source_type': 'qa_knowledge_base'  # æ ‡è®°æ¥æºç±»å‹
                        }
                    )
                else:
                    # æ™®é€šæ–‡æ¡£
                    doc = Document(
                        page_content=payload.get('page_content', ''),
                        metadata=payload.get('metadata', {})
                    )
                
                documents.append((doc, result['total_score']))

            return documents

        except Exception as e:
            logger.error(f"åŠ æƒèåˆæœç´¢å¤±è´¥: {e}")
            # é™çº§åˆ°ç®€å•å‘é‡æœç´¢
            return self._simple_vector_search(query, k)

    def _simple_vector_search(self, query: str, k: int) -> List[Tuple[Document, float]]:
        """ç®€å•çš„å‘é‡æœç´¢ä½œä¸ºé™çº§æ–¹æ¡ˆ"""
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_vector = self.embedding.embed_query(query)
            
            # æ‰§è¡Œå‘é‡æœç´¢
            search_results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_vector,
                limit=k
            )
            
            # è½¬æ¢ä¸ºDocumentå¯¹è±¡
            documents = []
            for result in search_results:
                payload = result.payload
                metadata = payload.get("metadata", {})
                
                # å¯¹äºQAå¯¹ï¼Œè¿”å›å®Œæ•´çš„é—®ç­”å†…å®¹
                if metadata.get('type') == 'qa':
                    question = metadata.get('question', '')
                    answer = metadata.get('answer', '')
                    qa_content = f"é—®é¢˜ï¼š{question}\n\nç­”æ¡ˆï¼š{answer}"
                    
                    doc = Document(
                        page_content=qa_content,
                        metadata={
                            **metadata,
                            'is_qa_pair': True,
                            'source_type': 'qa_knowledge_base'
                        }
                    )
                else:
                    doc = Document(
                        page_content=payload.get('page_content', ''),
                        metadata=metadata
                    )
                
                documents.append((doc, result.score))
            
            return documents
            
        except Exception as e:
            logger.error(f"ç®€å•å‘é‡æœç´¢ä¹Ÿå¤±è´¥: {e}")
            return []

    def _document_from_scored_point(self, scored_point: ScoredPoint) -> Document:
        payload = scored_point.payload
        metadata = payload.get("metadata", {})
        return Document(
            page_content=payload.get("page_content", ""),
            metadata=metadata
        )

    def delete(self, filter: Optional[Filter] = None) -> None:
        """åˆ é™¤æ»¡è¶³æ¡ä»¶çš„ç‚¹"""
        self.client.delete(
            collection_name=self.collection_name,
            points_selector=filter
        )


def convert_to_langchain_docs(chunks):
    docs = []
    for chunk in chunks:
        doc_id = f"{chunk['source']}_chunk_{hash(chunk['content_text'][:100])}"
        docs.append(
            Document(
                page_content=chunk["content_text"],
                metadata={
                    "title": chunk["title_text"],
                    "content": chunk["content_text"],
                    "level": chunk["level"],
                    "parent_title": chunk["parent_title"],
                    "path": chunk["path"],
                    "source": chunk["source"],
                    "id": doc_id
                }
            )
        )
    return docs


def embedding_init(
        host: str = "localhost",
        port: int = 6333,
        collection_name: str = "nsrl_tech_docs"
):
    """
    åˆå§‹åŒ–Qdrantå®¢æˆ·ç«¯ï¼ˆæœåŠ¡å™¨æ¨¡å¼ï¼‰

    å‚æ•°:
    - host: QdrantæœåŠ¡å™¨ä¸»æœºåœ°å€
    - port: QdrantæœåŠ¡å™¨ç«¯å£
    - collection_name: é›†åˆåç§°
    """
    # è¿æ¥åˆ°QdrantæœåŠ¡å™¨
    if host.startswith(('http://', 'https://')):
        # ä»URLä¸­æå–ä¸»æœºå
        host = host.split('://')[1].split(':')[0]
    client = QdrantClient(host=host, port=port)

    # åˆ›å»ºåµŒå…¥æ¨¡å‹å®ä¾‹
    embedding_model = QwenEmbedding()

    # åˆå§‹åŒ–å¢å¼ºå‹å‘é‡å­˜å‚¨
    vector_store = EnhancedQdrantVectorStore(
        client=client,
        collection_name=collection_name,
        embedding=embedding_model
    )

    # ç¡®ä¿é›†åˆå­˜åœ¨
    vector_store.create_collection_if_not_exists()

    return vector_store


def upsert_md_file(file_path: str, vector_store: EnhancedQdrantVectorStore):
    """ä¸Šä¼ Markdownæ–‡ä»¶åˆ°Qdrant"""
    source_name = os.path.basename(file_path)
    chunks = parse_markdown_file(file_path)  # ä½¿ç”¨åŸæ¥çš„å‡½æ•°
    docs = convert_to_langchain_docs(chunks)

    # å‡†å¤‡å¤šå‘é‡æ•°æ®ç‚¹
    points = []
    for i, doc in enumerate(docs):
        title_vector = vector_store.embedding_model.embed_query(
            f"æ ‡é¢˜: {doc.metadata['path']}"
        )
        content_vector = vector_store.embedding_model.embed_query(
            f"å†…å®¹: {doc.metadata['content']}"
        )

        # ç”Ÿæˆå”¯ä¸€IDï¼ˆé¿å…å“ˆå¸Œå†²çªï¼‰
        point_id = hash(doc.metadata["id"]) % (2 ** 63)  # ç¡®ä¿IDä¸ºæ­£æ•´æ•°

        points.append(PointStruct(
            id=point_id,
            vector={
                "title": title_vector,
                "content": content_vector
            },
            payload={
                "page_content": doc.page_content,
                "metadata": doc.metadata
            }
        ))

    # æ‰¹é‡ä¸Šä¼ 
    operation_info = vector_store.client.upsert(
        collection_name=vector_store.collection_name,
        points=points,
        wait=False  # ä¸ç­‰å¾…æ“ä½œå®Œæˆï¼Œé¿å…è¶…æ—¶
    )

    print(f"âœ… å·²ä¸Šä¼  {len(points)} ä¸ªæ–‡æ¡£å—åˆ°é›†åˆ {vector_store.collection_name}")
    print(f"ï¿½ï¿½ æ“ä½œè¯¦æƒ…: {operation_info}")
    return operation_info


def delete_by_source(source_name: str, vector_store: EnhancedQdrantVectorStore):
    """æŒ‰æ¥æºåç§°åˆ é™¤æ–‡æ¡£"""
    filter_condition = Filter(
        must=[
            FieldCondition(
                key="metadata.source",
                match=MatchValue(value=source_name)
            )
        ]
    )

    # æ‰§è¡Œåˆ é™¤
    operation_info = vector_store.client.delete(
        collection_name=vector_store.collection_name,
        points_selector=filter_condition
    )

    print(f"âœ… å·²åˆ é™¤æ¥æºä¸º {source_name} çš„æ‰€æœ‰æ–‡æ¡£å—")
    print(f"ï¿½ï¿½ æ“ä½œè¯¦æƒ…: {operation_info}")
    return operation_info


def list_all_collections(host: str = "localhost", port: int = 6333):
    """åˆ—å‡ºæ‰€æœ‰é›†åˆ"""
    client = QdrantClient(host=host, port=port)
    collections = client.get_collections().collections
    print("ï¿½ï¿½ å½“å‰Qdrantä¸­çš„é›†åˆ:")
    for collection in collections:
        print(f"- {collection.name} (ç‚¹æ•°: {collection.points_count})")
    return collections


def get_collection_info(collection_name: str, host: str = "localhost", port: int = 6333):
    """è·å–é›†åˆè¯¦ç»†ä¿¡æ¯"""
    client = QdrantClient(host=host, port=port)
    try:
        info = client.get_collection(collection_name)
        print(f" é›†åˆ {collection_name} è¯¦æƒ…:")
        print(f"ç‚¹æ•°: {info.points_count}")
        print(f"çŠ¶æ€: {info.status}")
        print(f"é…ç½®: {info.config}")
        return info
    except Exception as e:
        print(f"âŒ è·å–é›†åˆä¿¡æ¯å¤±è´¥: {str(e)}")
        return None


def upsert_qa_pair(qa_content: str, metadata: Dict[str, Any], vector_store: EnhancedQdrantVectorStore):
    """ä¸Šä¼ é—®ç­”å¯¹åˆ°Qdrant"""
    try:
        # ä»metadataä¸­è·å–æ–‡æ¡£åï¼Œå¦‚æœæ²¡æœ‰åˆ™ç”Ÿæˆé»˜è®¤å
        document_name = metadata.get('document_name', f"qa_{hash(qa_content) % 10000}")
        # ç¡®ä¿æ–‡æ¡£åä»¥.mdç»“å°¾
        if not document_name.endswith('.md'):
            document_name = f"{document_name}.md"
        
        # ç”Ÿæˆå”¯ä¸€IDï¼ˆåŸºäºé—®é¢˜å’Œç­”æ¡ˆçš„ç»„åˆï¼‰
        qa_id = hash(f"{metadata.get('question', '')}{metadata.get('answer', '')}") % (2 ** 63)
        
        # æ›´æ–°metadataä¸­çš„sourceå­—æ®µä¸ºæ–‡æ¡£å
        metadata['source'] = document_name
        
        # ç”Ÿæˆå‘é‡
        title_vector = vector_store.embedding_model.embed_query(
            f"é—®é¢˜: {metadata.get('question', '')}"
        )
        content_vector = vector_store.embedding_model.embed_query(
            f"å†…å®¹: {qa_content}"
        )
        
        # æ„å»ºæ•°æ®ç‚¹
        point = PointStruct(
            id=qa_id,
            vector={
                "title": title_vector,
                "content": content_vector
            },
            payload={
                "page_content": qa_content,
                "metadata": metadata
            }
        )
        
        # ä¸Šä¼ åˆ°å‘é‡æ•°æ®åº“
        operation_info = vector_store.client.upsert(
            collection_name=vector_store.collection_name,
            points=[point],
            wait=True
        )
        
        print(f"âœ… å·²ä¸Šä¼ é—®ç­”å¯¹åˆ°é›†åˆ {vector_store.collection_name}")
        print(f"æ–‡æ¡£å: {document_name}")
        print(f"é—®é¢˜: {metadata.get('question', '')[:50]}...")
        print(f"æ“ä½œè¯¦æƒ…: {operation_info}")
        return operation_info
        
    except Exception as e:
        print(f"âŒ ä¸Šä¼ é—®ç­”å¯¹å¤±è´¥: {str(e)}")
        raise e


def upsert_md_file_with_source(file_path: str, vector_store: EnhancedQdrantVectorStore, source_name: str):
    """ä¸Šä¼ Markdownæ–‡ä»¶åˆ°Qdrantï¼Œä½¿ç”¨æŒ‡å®šçš„sourceåç§°"""
    chunks = parse_markdown_file(file_path)  # ä½¿ç”¨åŸæ¥çš„å‡½æ•°
    docs = convert_to_langchain_docs(chunks)
    
    # ä¿®æ”¹æ‰€æœ‰æ–‡æ¡£çš„sourceå­—æ®µä¸ºæŒ‡å®šçš„source_name
    for doc in docs:
        doc.metadata['source'] = source_name

    # å‡†å¤‡å¤šå‘é‡æ•°æ®ç‚¹
    points = []
    for i, doc in enumerate(docs):
        title_vector = vector_store.embedding_model.embed_query(
            f"æ ‡é¢˜: {doc.metadata['path']}"
        )
        content_vector = vector_store.embedding_model.embed_query(
            f"å†…å®¹: {doc.metadata['content']}"
        )

        # ç”Ÿæˆå”¯ä¸€IDï¼ˆé¿å…å“ˆå¸Œå†²çªï¼‰
        point_id = hash(doc.metadata["id"]) % (2 ** 63)  # ç¡®ä¿IDä¸ºæ­£æ•´æ•°

        points.append(PointStruct(
            id=point_id,
            vector={
                "title": title_vector,
                "content": content_vector
            },
            payload={
                "page_content": doc.page_content,
                "metadata": doc.metadata
            }
        ))

    # æ‰¹é‡ä¸Šä¼ 
    operation_info = vector_store.client.upsert(
        collection_name=vector_store.collection_name,
        points=points,
        wait=True  # ç­‰å¾…æ“ä½œå®Œæˆï¼Œç¡®ä¿æ•°æ®è¢«æ­£ç¡®ç´¢å¼•
    )

    print(f"âœ… å·²ä¸Šä¼  {len(points)} ä¸ªæ–‡æ¡£å—åˆ°é›†åˆ {vector_store.collection_name}")
    print(f" æ“ä½œè¯¦æƒ…: {operation_info}")
    return operation_info


def upsert_md_file_with_original(file_path: str, vector_store: EnhancedQdrantVectorStore):
    """ä¸Šä¼ Markdownæ–‡ä»¶åˆ°Qdrantï¼ŒåŒæ—¶å­˜å‚¨åŸæ–‡ä»¶å†…å®¹ç”¨äºé¢„è§ˆ"""
    source_name = os.path.basename(file_path)
    chunks = parse_markdown_file(file_path)  # ä½¿ç”¨åŸæ¥çš„å‡½æ•°
    docs = convert_to_langchain_docs(chunks)
    
    # è¯»å–åŸæ–‡ä»¶å®Œæ•´å†…å®¹
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            original_content = f.read()
    except Exception as e:
        print(f"âš ï¸ è¯»å–åŸæ–‡ä»¶å†…å®¹å¤±è´¥: {e}")
        original_content = ""

    # å‡†å¤‡å¤šå‘é‡æ•°æ®ç‚¹
    points = []
    for i, doc in enumerate(docs):
        title_vector = vector_store.embedding_model.embed_query(
            f"æ ‡é¢˜: {doc.metadata['path']}"
        )
        content_vector = vector_store.embedding_model.embed_query(
            f"å†…å®¹: {doc.metadata['content']}"
        )

        # ç”Ÿæˆå”¯ä¸€IDï¼ˆé¿å…å“ˆå¸Œå†²çªï¼‰
        point_id = hash(doc.metadata["id"]) % (2 ** 63)  # ç¡®ä¿IDä¸ºæ­£æ•´æ•°

        points.append(PointStruct(
            id=point_id,
            vector={
                "title": title_vector,
                "content": content_vector
            },
            payload={
                "page_content": doc.page_content,
                "metadata": doc.metadata,
                "original_content": original_content,  # æ·»åŠ åŸæ–‡ä»¶å†…å®¹
                "source_name": source_name  # æ·»åŠ æºæ–‡ä»¶å
            }
        ))

    # æ‰¹é‡ä¸Šä¼ 
    operation_info = vector_store.client.upsert(
        collection_name=vector_store.collection_name,
        points=points,
        wait=True  # ç­‰å¾…æ“ä½œå®Œæˆï¼Œç¡®ä¿æ•°æ®è¢«æ­£ç¡®ç´¢å¼•
    )

    print(f"âœ… å·²ä¸Šä¼  {len(points)} ä¸ªæ–‡æ¡£å—åˆ°é›†åˆ {vector_store.collection_name}")
    print(f"ğŸ“„ åŸæ–‡ä»¶å†…å®¹å·²å­˜å‚¨ï¼Œæ–‡ä»¶å: {source_name}")
    print(f" æ“ä½œè¯¦æƒ…: {operation_info}")
    return operation_info