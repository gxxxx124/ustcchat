from enum import Enum
from fastapi import FastAPI, HTTPException, status, APIRouter
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Dict, List, Optional, Any, AsyncGenerator, TypedDict, Annotated
import uuid
import logging
from contextlib import asynccontextmanager
import os
import requests
import tempfile
import shutil
import operator
import re
import json
import asyncio
from email._header_value_parser import parse_message_id
from operator import add
from psycopg_pool import AsyncConnectionPool
from langchain_core.messages import ToolMessage, HumanMessage, SystemMessage, AIMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver
from qdrant_client import QdrantClient
from qdrant_client.http.models import VectorParams, Distance, FieldCondition, MatchValue
from oss2 import Auth, Bucket
from starlette.responses import StreamingResponse

from chunks2embedding import (
    embedding_init,
    upsert_md_file,
    delete_by_source,
    list_all_collections,
    get_collection_info
)
from pdf2md import pdf2md
from pdf import (
    register_user_document_tool,
    get_user_document_tool,
    get_user_document_tool_by_session,
    list_user_document_tools
)
from searxng_server import create_search_tool

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("unified-service")

# ======================
# å…¨å±€å…±äº«èµ„æº
# ======================
qdrant_client = None
checkpointer = None
rag_tool_cache = {}
web_search_tool = None

# OSS é…ç½®
# OSS é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
OSS_ACCESS_KEY_ID = os.getenv("OSS_ACCESS_KEY_ID", "")
OSS_ACCESS_KEY_SECRET = os.getenv("OSS_ACCESS_KEY_SECRET", "")
OSS_ENDPOINT = os.getenv("OSS_ENDPOINT", "https://oss-cn-hangzhou.aliyuncs.com")
OSS_BUCKET = os.getenv("OSS_BUCKET", "")

# æœ¬åœ°ä¸´æ—¶è·¯å¾„
LOCAL_DIR = "/home/easyai/oss"
os.makedirs(LOCAL_DIR, exist_ok=True)

# ======================
# åˆ›å»ºè·¯ç”±
# ======================
kb_router = APIRouter(prefix="/kb", tags=["çŸ¥è¯†åº“ç®¡ç†"])
agent_router = APIRouter(prefix="/agent", tags=["å¯¹è¯Agent"])

# ======================
# å…±äº«å·¥å…·å‡½æ•°
# ======================
def get_current_knowledge_base_info(kb_name: str):
    """åªè·å–æŒ‡å®šçŸ¥è¯†åº“çš„æ–‡æ¡£ä¿¡æ¯ï¼ˆä¸åŒ…å«å…¶ä»–çŸ¥è¯†åº“ï¼‰"""
    global qdrant_client
    try:
        # ä½¿ç”¨å…¨å±€Qdrantå®¢æˆ·ç«¯

        if qdrant_client is None:
            qdrant_client = QdrantClient(host="localhost", port=6333)
        
        # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
        try:
            qdrant_client.get_collection(kb_name)
        except:
            return {
                "name": kb_name,
                "exists": False,
                "points_count": 0,
                "documents": [],
                "document_count": 0
            }
        
        # è·å–çŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰æ–‡æ¡£å—ï¼ˆåˆ†é¡µè·å–æ‰€æœ‰æ•°æ®ï¼‰
        all_points = []
        offset = None
        while True:
            # ä½¿ç”¨scroll APIè·å–æ•°æ®
            points, next_offset = qdrant_client.scroll(
                collection_name=kb_name,
                limit=100,
                with_payload=True,
                with_vectors=False,
                offset=offset
            )
            all_points.extend(points)
            if not next_offset:
                break
            offset = next_offset
        
        # æå–å”¯ä¸€æ–‡æ¡£å
        document_names = set()
        for point in all_points:
            try:
                # å°è¯•è®¿é—®sourceå­—æ®µ
                if "metadata" in point.payload and "source" in point.payload["metadata"]:
                    document_names.add(point.payload["metadata"]["source"])
            except Exception as e:
                logger.warning(f"å¤„ç†ç‚¹æ—¶å‡ºé”™: {str(e)}")
        
        return {
            "name": kb_name,
            "exists": True,
            "points_count": len(all_points),
            "documents": list(document_names),
            "document_count": len(document_names)
        }
    except Exception as e:
        logger.error(f"è·å–çŸ¥è¯†åº“ä¿¡æ¯å¤±è´¥: {str(e)}", exc_info=True)
        return {
            "name": kb_name,
            "exists": False,
            "points_count": 0,
            "documents": [],
            "document_count": 0
        }

def download_pdf_from_oss(file_key, local_path):
    """ä»OSSä¸‹è½½æ–‡ä»¶"""
    try:
        # åˆå§‹åŒ–OSSå®¢æˆ·ç«¯
        auth = Auth(OSS_ACCESS_KEY_ID, OSS_ACCESS_KEY_SECRET)
        bucket = Bucket(auth, OSS_ENDPOINT, OSS_BUCKET)
        
        bucket.get_object_to_file(file_key, local_path)
        logger.info(f"æ–‡ä»¶ä¸‹è½½æˆåŠŸ: {local_path}")
        return True
    except Exception as e:
        logger.error(f"æ–‡ä»¶ä¸‹è½½å¤±è´¥: {str(e)}")
        return False

def get_rag_tool(knowledge_base_name: str):
    """æ ¹æ®çŸ¥è¯†åº“åç§°è·å–æˆ–åˆ›å»ºRAGå·¥å…·"""
    if knowledge_base_name in rag_tool_cache:
        return rag_tool_cache[knowledge_base_name]
    
    # åˆ›å»ºæ–°çš„RAGå·¥å…·å®ä¾‹
    from rag_tool import create_rag_tool
    rag_tool = create_rag_tool(
        host="localhost",
        port=6333,
        collection_name=knowledge_base_name
    )
    rag_tool_cache[knowledge_base_name] = rag_tool
    return rag_tool

def extract_highest_similarity(tool_response: str) -> float:
    """ä»å·¥å…·å“åº”ä¸­æå–æœ€é«˜ç›¸ä¼¼åº¦"""
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾æ‰€æœ‰ç›¸ä¼¼åº¦å€¼
    similarity_values = re.findall(r"ç›¸ä¼¼åº¦: ([\d.]+)", tool_response)
    if not similarity_values:
        logger.warning("æœªåœ¨å·¥å…·å“åº”ä¸­æ‰¾åˆ°ç›¸ä¼¼åº¦ä¿¡æ¯")
        return 0.0
    
    # è½¬æ¢ä¸ºæµ®ç‚¹æ•°å¹¶è¿”å›æœ€å¤§å€¼
    try:
        similarities = [float(val) for val in similarity_values]
        highest = max(similarities)
        logger.info(f"æ£€æµ‹åˆ°æœ€é«˜ç›¸ä¼¼åº¦: {highest:.4f}")
        return highest
    except ValueError:
        logger.error("æ— æ³•è§£æç›¸ä¼¼åº¦å€¼")
        return 0.0

# ======================
# çŸ¥è¯†åº“ç®¡ç†APIè·¯ç”±
# ======================

# 1. å®šä¹‰APIè¯·æ±‚æ¨¡å‹
class KnowledgeBaseAction(str, Enum):
    CREATE = "create"
    UPLOAD = "upload"
    DELETE_DOCUMENT = "delete_document"
    DELETE = "delete"

class KnowledgeBaseRequest(BaseModel):
    action: KnowledgeBaseAction
    name: str
    document_name: Optional[str] = None

# 2. ä¿®å¤KnowledgeBaseResponseå®šä¹‰
class KnowledgeBaseResponse(BaseModel):
    success: bool
    message: str
    data: Optional[Dict[str, Any]] = None

# 6. ç®€åŒ–åçš„APIç«¯ç‚¹
@kb_router.post("/api/knowledge-base", response_model=KnowledgeBaseResponse)
async def manage_knowledge_base(request: KnowledgeBaseRequest):
    """ç»Ÿä¸€çš„çŸ¥è¯†åº“ç®¡ç†ç«¯ç‚¹ï¼Œåªè¿”å›å½“å‰çŸ¥è¯†åº“ä¿¡æ¯"""
    global qdrant_client
    try:
        if request.action == KnowledgeBaseAction.CREATE:
            # æ£€æŸ¥é›†åˆæ˜¯å¦å·²å­˜åœ¨

            if qdrant_client is None:
                qdrant_client = QdrantClient(host="localhost", port=6333)
            
            try:
                # å°è¯•è·å–é›†åˆï¼Œå¦‚æœå­˜åœ¨åˆ™è¿”å›exists
                qdrant_client.get_collection(request.name)
                status = "exists"
                message = f"çŸ¥è¯†åº“ '{request.name}' å·²å­˜åœ¨"
            except:
                # é›†åˆä¸å­˜åœ¨ï¼Œåˆ›å»ºå®ƒ
                try:
                    qdrant_client.create_collection(
                        collection_name=request.name,
                        vectors_config={
                            "title": VectorParams(size=1024, distance=Distance.COSINE),
                            "content": VectorParams(size=1024, distance=Distance.COSINE)
                        }
                    )
                    status = "created"
                    message = f"çŸ¥è¯†åº“ '{request.name}' åˆ›å»ºæˆåŠŸ"
                except Exception as e:
                    logger.error(f"åˆ›å»ºçŸ¥è¯†åº“å¤±è´¥: {str(e)}")
                    return KnowledgeBaseResponse(
                        success=False,
                        message=f"åˆ›å»ºçŸ¥è¯†åº“å¤±è´¥: {str(e)}",
                        data={"name": request.name}
                    )
            
            # ä»…è·å–å½“å‰çŸ¥è¯†åº“çš„ä¿¡æ¯
            current_kb = get_current_knowledge_base_info(request.name)
            return KnowledgeBaseResponse(
                success=True,
                message=message,
                data={
                    "name": request.name,
                    "status": status,
                    "document_count": current_kb["document_count"],
                    "points_count": current_kb["points_count"],
                    "documents": current_kb["documents"]
                }
            )
        
        elif request.action == KnowledgeBaseAction.DELETE:
            try:

                if qdrant_client is None:
                    qdrant_client = QdrantClient(host="localhost", port=6333)
                
                qdrant_client.delete_collection(request.name)
                message = f"çŸ¥è¯†åº“ '{request.name}' å·²åˆ é™¤"
                return KnowledgeBaseResponse(
                    success=True,
                    message=message,
                    data={
                        "name": request.name,
                        "document_count": 0,
                        "points_count": 0,
                        "documents": []
                    }
                )
            except Exception as e:
                logger.error(f"åˆ é™¤çŸ¥è¯†åº“å¤±è´¥: {str(e)}")
                return KnowledgeBaseResponse(
                    success=False,
                    message=f"åˆ é™¤çŸ¥è¯†åº“å¤±è´¥: {str(e)}",
                    data={"name": request.name}
                )
        
        elif request.action == KnowledgeBaseAction.UPLOAD:
            if not request.document_name:
                return KnowledgeBaseResponse(
                    success=False,
                    message="ä¸Šä¼ æ–‡æ¡£éœ€è¦æä¾› document_name",
                    data={"name": request.name}
                )
            
            # ä¸‹è½½æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
            oss_path = f"knowledge-documents/{request.name}/{request.document_name}.pdf"
            try:
                # ç”Ÿæˆä¸´æ—¶æ–‡ä»¶å
                orginal_file_name = f"{request.document_name}.pdf"
                local_input = os.path.join(LOCAL_DIR, orginal_file_name)
                if not download_pdf_from_oss(oss_path, local_input):
                    return KnowledgeBaseResponse(
                        success=False,
                        message="ä¸‹è½½æ–‡ä»¶å¤±è´¥",
                        data={"name": request.name}
                    )
                
                # ä¸‹è½½æ–‡ä»¶
                pdf2md(local_input)
                tempfile = f'/home/easyai/OCRFlux/localworkspace/markdowns/{request.document_name}/{request.document_name}.md'
                
                # ç›´æ¥è°ƒç”¨æ‚¨å·²æœ‰çš„å‡½æ•°
                vector_store = embedding_init(collection_name=request.name)
                operation_info = upsert_md_file(tempfile, vector_store)
                
                # ä»…è·å–å½“å‰çŸ¥è¯†åº“çš„ä¿¡æ¯
                current_kb = get_current_knowledge_base_info(request.name)
                return KnowledgeBaseResponse(
                    success=True,
                    message=f"æ–‡æ¡£ '{request.document_name}' å·²ä¸Šä¼ åˆ°çŸ¥è¯†åº“ '{request.name}'",
                    data={
                        "name": request.name,
                        "document": request.document_name,
                        "details": str(operation_info),
                        "document_count": current_kb["document_count"],
                        "points_count": current_kb["points_count"],
                        "documents": current_kb["documents"]
                    }
                )
            finally:
                if os.path.exists(local_input):
                    os.remove(local_input)
        
        elif request.action == KnowledgeBaseAction.DELETE_DOCUMENT:
            if not request.document_name:
                return KnowledgeBaseResponse(
                    success=False,
                    message="åˆ é™¤æ–‡æ¡£éœ€è¦æä¾› document_name",
                    data={"name": request.name}
                )
            
            # ç›´æ¥è°ƒç”¨æ‚¨å·²æœ‰çš„å‡½æ•°
            deletename = f'{request.document_name}.md'
            vector_store = embedding_init(collection_name=request.name)
            operation_info = delete_by_source(deletename, vector_store)
            
            # ä»…è·å–å½“å‰çŸ¥è¯†åº“çš„ä¿¡æ¯
            current_kb = get_current_knowledge_base_info(request.name)
            return KnowledgeBaseResponse(
                success=True,
                message=f"æ–‡æ¡£ '{request.document_name}' å·²ä»çŸ¥è¯†åº“ '{request.name}' ä¸­åˆ é™¤",
                data={
                    "name": request.name,
                    "document": request.document_name,
                    "details": str(operation_info),
                    "document_count": current_kb["document_count"],
                    "points_count": current_kb["points_count"],
                    "documents": current_kb["documents"]
                }
            )
    except Exception as e:
        logger.error(f"å¤„ç†è¯·æ±‚å¤±è´¥: {str(e)}", exc_info=True)
        return KnowledgeBaseResponse(
            success=False,
            message=f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}",
            data={"action": request.action, "name": request.name}
        )

# 7. ç®€åŒ–å…¶ä»–ç«¯ç‚¹
@kb_router.get("/api/knowledge-bases", response_model=KnowledgeBaseResponse)
async def list_knowledge_bases():
    """åˆ—å‡ºæ‰€æœ‰çŸ¥è¯†åº“ - ä¿æŒä¸å˜ï¼Œå› ä¸ºè¿™æ˜¯å¦ä¸€ä¸ªåŠŸèƒ½"""
    try:
        global qdrant_client
        if qdrant_client is None:
            qdrant_client = QdrantClient(host="localhost", port=6333)
        
        collections = qdrant_client.get_collections().collections
        kb_list = []
        for collection in collections:
            kb_info = get_current_knowledge_base_info(collection.name)
            kb_list.append({
                "name": collection.name,
                "points_count": kb_info["points_count"],
                "document_count": kb_info["document_count"]
            })
        
        return KnowledgeBaseResponse(
            success=True,
            message="çŸ¥è¯†åº“åˆ—è¡¨è·å–æˆåŠŸ",
            data={
                "knowledge_bases": kb_list,
                "total": len(kb_list)
            }
        )
    except Exception as e:
        logger.error(f"è·å–çŸ¥è¯†åº“åˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)
        return KnowledgeBaseResponse(
            success=False,
            message=f"è·å–çŸ¥è¯†åº“åˆ—è¡¨å¤±è´¥: {str(e)}",
            data={}
        )

@kb_router.get("/api/knowledge-base/{kb_name}/documents", response_model=KnowledgeBaseResponse)
async def list_documents(kb_name: str):
    """åˆ—å‡ºçŸ¥è¯†åº“ä¸­çš„æ–‡æ¡£ - ä¿æŒä¸å˜"""
    try:
        # ä»…è·å–å½“å‰çŸ¥è¯†åº“çš„ä¿¡æ¯
        current_kb = get_current_knowledge_base_info(kb_name)
        if not current_kb["exists"]:
            return KnowledgeBaseResponse(
                success=False,
                message=f"çŸ¥è¯†åº“ '{kb_name}' ä¸å­˜åœ¨",
                data={"name": kb_name}
            )
        
        return KnowledgeBaseResponse(
            success=True,
            message=f"çŸ¥è¯†åº“ '{kb_name}' ä¸­çš„æ–‡æ¡£åˆ—è¡¨",
            data={
                "knowledge_base": kb_name,
                "documents": current_kb["documents"],
                "total": current_kb["document_count"],
                "points_count": current_kb["points_count"]
            }
        )
    except Exception as e:
        logger.error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)
        return KnowledgeBaseResponse(
            success=False,
            message=f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}",
            data={"name": kb_name}
        )

# ======================
# å¯¹è¯Agent APIè·¯ç”±
# ======================

# 1. é…ç½®æ¨¡å‹
inference_server_url = "http://localhost:11434/v1"
model = ChatOpenAI(
    model="qwen3:4b",
    openai_api_key="none",
    openai_api_base=inference_server_url,
    max_tokens=300,  # å‡å°‘æœ€å¤§tokenæ•°ï¼Œé™ä½æ˜¾å­˜å ç”¨
    temperature=0.7,  # é™ä½æ¸©åº¦æé«˜å†³ç­–ç¨³å®šæ€§
    timeout=30.0,  # æ·»åŠ è¶…æ—¶é˜²æ­¢å¡æ­»
    # æ·»åŠ æ˜¾å­˜ä¼˜åŒ–å‚æ•°
    request_timeout=30.0,
    max_retries=2
)

# 3. å®šä¹‰çŠ¶æ€
class AgentState(TypedDict):
    messages: Annotated[list, add]
    tool_call_count: Annotated[int, operator.add]  # æ·»åŠ å·¥å…·è°ƒç”¨è®¡æ•°å™¨
    knowledge_base_name: str  # æ–°å¢å­—æ®µï¼Œç”¨äºå­˜å‚¨å½“å‰ä¼šè¯ä½¿ç”¨çš„çŸ¥è¯†åº“åç§°
    user_document_tools: List[str]  # æ–°å¢å­—æ®µï¼Œç”¨äºå­˜å‚¨å½“å‰ä¼šè¯å¯ç”¨çš„ç”¨æˆ·æ–‡æ¡£å·¥å…·åç§°
    web_search_enabled: bool  # æ–°å¢ï¼šè®°å½•webæœç´¢æ˜¯å¦å¯ç”¨

# 4. ä¿®æ”¹æ¨¡å‹è°ƒç”¨èŠ‚ç‚¹
async def call_model(state: AgentState):
    """æ¨¡å‹è‡ªä¸»å†³ç­–æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·ï¼ŒåŒ…å«å‚æ•°éªŒè¯å’ŒçŠ¶æ€æ›´æ–°"""
    messages = state["messages"]
    knowledge_base_name = state.get("knowledge_base_name", "test")
    
    # æ˜¾å­˜ä¼˜åŒ–ï¼šé™åˆ¶ä¼šè¯é•¿åº¦ï¼Œé˜²æ­¢æ˜¾å­˜ç´¯ç§¯
    if len(messages) > 10:  # é™åˆ¶ä¼šè¯å†å²é•¿åº¦
        # ä¿ç•™æœ€æ–°çš„5æ¡æ¶ˆæ¯å’Œç³»ç»Ÿæç¤º
        messages = messages[-5:]
        logger.info("âš ï¸ ä¼šè¯å†å²è¿‡é•¿ï¼Œå·²æˆªæ–­ä»¥èŠ‚çœæ˜¾å­˜")
    
    # åŠ¨æ€è·å–å½“å‰çŸ¥è¯†åº“çš„RAGå·¥å…·
    rag_tool = get_rag_tool(knowledge_base_name)
    available_tools = [rag_tool]
    
    if state.get("web_search_enabled", True):
        global web_search_tool
        if web_search_tool is None:
            web_search_tool = create_search_tool()
        available_tools.append(web_search_tool)
    
    # æ·»åŠ ç”¨æˆ·æ–‡æ¡£å·¥å…·ï¼ˆå¦‚æœæœ‰ï¼‰
    user_document_tools_list = state.get("user_document_tools", [])
    for tool_name in user_document_tools_list:
        tool_info = get_user_document_tool(tool_name)
        if tool_info and "tool" in tool_info:
            available_tools.append(tool_info["tool"])
    
    # å¦‚æœæ˜¯åˆå§‹æŸ¥è¯¢ï¼Œæ·»åŠ ç³»ç»Ÿæç¤º
    if len(messages) == 1 and isinstance(messages[0], HumanMessage):
        # æ„å»ºå·¥å…·åˆ—è¡¨æè¿°
        tools_description = f"""1. rag_knowledge_search: æŸ¥è¯¢å†…éƒ¨çŸ¥è¯†åº“ï¼ˆä¸»è¦åŒ…å«åŒ»å­¦ç›¸å…³å†…å®¹ï¼‰
           - å¿…é¡»å‚æ•°: query (string)
           - å½“å‰çŸ¥è¯†åº“: {knowledge_base_name}
           - è°ƒç”¨ç¤ºä¾‹: {{"name": "rag_knowledge_search", "arguments": {{"query": "NGSé€‚ç”¨äººç¾¤"}}}}
        2. web_search_tool: æŸ¥è¯¢æœ€æ–°äº’è”ç½‘ä¿¡æ¯
           - å¿…é¡»å‚æ•°: query (string)
           - è°ƒç”¨ç¤ºä¾‹: {{"name": "web_search_tool", "arguments": {{"query": "NGSæœ€æ–°ç ”ç©¶"}}}}"""
        
        # æ·»åŠ ç”¨æˆ·æ–‡æ¡£å·¥å…·æè¿°
        if user_document_tools_list:
            tools_description += "\nç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£æœç´¢å·¥å…·:"
            for tool_name in user_document_tools_list:
                tool_info = get_user_document_tool(tool_name)
                if tool_info:
                    tools_description += f"\n{tool_info['tool'].name}: {tool_info['tool'].description}"
                    tools_description += "\n   - å¿…é¡»å‚æ•°: query (string)"
        
        # æ„å»ºç³»ç»Ÿæç¤º
        if not state.get("web_search_enabled", True):
            system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªå¤§å‹ç²’å­åŠ é€Ÿå™¨ä¸“å®¶ï¼Œä¸“é—¨å›ç­”å…³äºç²’å­åŠ é€Ÿå™¨çš„æŠ€æœ¯é—®é¢˜ã€‚

å¯ç”¨å·¥å…·ï¼š
{tools_description}

å·¥ä½œæµç¨‹ï¼š
1. å¯¹äºæ‰€æœ‰ç²’å­åŠ é€Ÿå™¨ç›¸å…³é—®é¢˜ï¼Œå¿…é¡»é¦–å…ˆä½¿ç”¨ rag_knowledge_search æœç´¢çŸ¥è¯†åº“
2. æ£€æŸ¥è¿”å›ç»“æœçš„æœ€é«˜ç›¸ä¼¼åº¦ï¼š
   * å¦‚æœæœ€é«˜ç›¸ä¼¼åº¦ â‰¥ 0.6ï¼Œç»“æœç›¸å…³ï¼ŒåŸºäºæ­¤ç”Ÿæˆå›ç­”
   * å¦‚æœæœ€é«˜ç›¸ä¼¼åº¦ < 0.6ï¼Œç»“æœä¸ç›¸å…³ï¼Œæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·çŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯
3. ä¸¥æ ¼åŸºäºçŸ¥è¯†åº“å†…å®¹å›ç­”ï¼Œä¸å¾—ç¼–é€ æˆ–æ¨æµ‹ä¿¡æ¯

é‡è¦æŒ‡å¯¼åŸåˆ™:
1. å›ç­”å¿…é¡»ä¸¥æ ¼åŸºäºçŸ¥è¯†åº“å†…å®¹ï¼Œä¸å¾—ç¼–é€ ä¿¡æ¯
2. å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·"çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
3. æä¾›ä¿¡æ¯æ—¶è¦æ³¨æ˜æ¥æºï¼ˆæ¥è‡ªçŸ¥è¯†åº“ï¼‰
4. å¯¹äºæŠ€æœ¯å»ºè®®ï¼Œå¿…é¡»åŸºäºçŸ¥è¯†åº“ä¸­çš„æƒå¨èµ„æ–™
5. å¦‚æœçŸ¥è¯†åº“æœç´¢ç»“æœç›¸ä¼¼åº¦ < 0.6ï¼Œä¸å¾—åŸºäºä½ç›¸ä¼¼åº¦ç»“æœç”Ÿæˆå›ç­”
6. ä¸å¾—ä½¿ç”¨ç½‘ç»œæœç´¢æˆ–å…¶ä»–å¤–éƒ¨ä¿¡æ¯æº
7. å›ç­”è¦ä¸“ä¸šã€å‡†ç¡®ã€ä¸¥è°¨
8. å¦‚æœå·²ç»è°ƒç”¨å·¥å…·è¶…è¿‡3æ¬¡ä»æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·çŸ¥è¯†åº“ä¸­æ— ç›¸å…³ä¿¡æ¯
å·¥å…·è°ƒç”¨æ ¼å¼è¦æ±‚:
- ä»…ä½¿ç”¨æŒ‡å®šçš„å·¥å…·åç§°
- ä»…ä¼ é€’å·¥å…·å®šä¹‰ä¸­è¦æ±‚çš„å‚æ•°
- ç»å¯¹ä¸è¦æ·»åŠ é¢å¤–å‚æ•°ï¼ˆå¦‚"using"ã€"reason"ç­‰ï¼‰
- ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºå·¥å…·è°ƒç”¨
- ä¾‹å¦‚: {{"name": "rag_knowledge_search", "arguments": {{"query": "ä½ çš„æŸ¥è¯¢"}}}}
- é‡è¦: ä¸è¦åœ¨å·¥å…·è°ƒç”¨ä¸­åŒ…å«ä»»ä½•é¢å¤–æ–‡æœ¬ã€è§£é‡Šæˆ–<think>æ ‡ç­¾
- å·¥å…·è°ƒç”¨å¿…é¡»æ˜¯çº¯JSONæ ¼å¼ï¼Œä¸èƒ½æœ‰å…¶ä»–å†…å®¹
- é”™è¯¯ç¤ºä¾‹: {{"name": "rag_knowledge_search", "arguments": {{"query": "...", "using": "..."}}}}"""
        else:
            system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªåŒ»ç–—æ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·ï¼š
{tools_description}
å·¥ä½œæµç¨‹ï¼š
- å¦‚æœç”¨æˆ·è¯¢é—®ä¸å…¶ä¸Šä¼ æ–‡æ¡£ç›¸å…³çš„å†…å®¹ï¼Œä¼˜å…ˆä½¿ç”¨å¯¹åº”çš„æ–‡æ¡£æœç´¢å·¥å…·
- å¯¹äºä¸€èˆ¬åŒ»å­¦é—®é¢˜ï¼Œé¦–å…ˆå°è¯•ä½¿ç”¨ rag_knowledge_search
- æ£€æŸ¥è¿”å›ç»“æœçš„æœ€é«˜ç›¸ä¼¼åº¦ï¼š
  * å¦‚æœæœ€é«˜ç›¸ä¼¼åº¦ â‰¥ 0.5ï¼Œç»“æœå¯èƒ½ç›¸å…³ï¼Œå¯åŸºäºæ­¤ç”Ÿæˆå›ç­”
  * å¦‚æœæœ€é«˜ç›¸ä¼¼åº¦ < 0.5ï¼Œç»“æœä¸ç›¸å…³ï¼Œåº”ä½¿ç”¨ web_search_tool
ç¡®ä¿æœ€ç»ˆå›ç­”æ•´åˆæ‰€æœ‰å¯ç”¨ä¿¡æ¯
é‡è¦æŒ‡å¯¼åŸåˆ™:
1. ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£å·¥å…·ï¼ˆå¦‚æœé—®é¢˜ä¸æ–‡æ¡£å†…å®¹ç›¸å…³ï¼‰
2. å…¶æ¬¡ä½¿ç”¨æœ¬åœ°çŸ¥è¯†åº“å·¥å…·(rag_knowledge_search)
3. å½“æœ¬åœ°çŸ¥è¯†åº“æœç´¢ç»“æœçš„æœ€é«˜ç›¸ä¼¼åº¦ < 0.5 æ—¶ï¼Œå¿…é¡»ä½¿ç”¨ç½‘ç»œæœç´¢å·¥å…·(web_search_tool),ä¸å¾—ç›´æ¥ç”Ÿæˆå›ç­”
4. å›ç­”å¿…é¡»åŸºäºè¯æ®ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
5. æä¾›ä¿¡æ¯æ—¶è¦æ³¨æ˜æ¥æºï¼ˆæ¥è‡ªç”¨æˆ·æ–‡æ¡£ã€æœ¬åœ°çŸ¥è¯†åº“æˆ–ç½‘ç»œæœç´¢ï¼‰
6. å¯¹äºåŒ»å­¦å»ºè®®ï¼Œå¿…é¡»æé†’ç”¨æˆ·å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿ
7. å¦‚æœç½‘ç»œæœç´¢è¿”å›äº†æ˜ç¡®çš„ç­”æ¡ˆï¼Œåˆ™ç›´æ¥ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
8. å¦‚æœå·²ç»è°ƒç”¨å·¥å…·è¶…è¿‡5æ¬¡ä»æœªè§£å†³é—®é¢˜ï¼Œè¯·åŸºäºå·²æœ‰ä¿¡æ¯æä¾›æœ€ä½³ç­”æ¡ˆ
9. å½“ç”¨æˆ·æåˆ°"æˆ‘çš„æŠ¥å‘Š"ã€"æˆ‘çš„åŸºå› æ£€æµ‹"ç­‰ç±»ä¼¼è¡¨è¿°æ—¶ï¼Œä¼˜å…ˆä½¿ç”¨ç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£å·¥å…·
10. å¦‚æœæœ€åå·¥å…·è°ƒç”¨æ²¡æœ‰ç»“æœè¿”å›ï¼Œåˆ™ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜
å·¥å…·è°ƒç”¨æ ¼å¼è¦æ±‚:
- ä»…ä½¿ç”¨æŒ‡å®šçš„å·¥å…·åç§°
- ä»…ä¼ é€’å·¥å…·å®šä¹‰ä¸­è¦æ±‚çš„å‚æ•°
- ç»å¯¹ä¸è¦æ·»åŠ é¢å¤–å‚æ•°ï¼ˆå¦‚"using"ã€"reason"ç­‰ï¼‰
- ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºå·¥å…·è°ƒç”¨
- ä¾‹å¦‚: {{"name": "web_search_tool", "arguments": {{"query": "ä½ çš„æŸ¥è¯¢"}}}}
- é‡è¦: ä¸è¦åœ¨å·¥å…·è°ƒç”¨ä¸­åŒ…å«ä»»ä½•é¢å¤–æ–‡æœ¬ã€è§£é‡Šæˆ–<think>æ ‡ç­¾
- å·¥å…·è°ƒç”¨å¿…é¡»æ˜¯çº¯JSONæ ¼å¼ï¼Œä¸èƒ½æœ‰å…¶ä»–å†…å®¹
- é”™è¯¯ç¤ºä¾‹: {{"name": "web_search_tool", "arguments": {{"query": "...", "using": "..."}}}}"""
        
        messages = [SystemMessage(content=system_prompt)] + messages
    
    # æ£€æŸ¥å·¥å…·è°ƒç”¨æ¬¡æ•° - å¦‚æœè¶…è¿‡é™åˆ¶ï¼Œå¼ºåˆ¶æ¨¡å‹æä¾›ç­”æ¡ˆ
    if state.get("tool_call_count", 0) >= 5:
        messages.append(SystemMessage(
            content="âš ï¸ é‡è¦æç¤ºï¼šæ‚¨å·²ç»è°ƒç”¨äº†å¤šæ¬¡å·¥å…·ä½†ä»æœªèƒ½æä¾›æœ€ç»ˆç­”æ¡ˆã€‚"
                    "è¯·åŸºäºå·²æœ‰ä¿¡æ¯ç«‹å³æä¾›å®Œæ•´å›ç­”ï¼Œä¸è¦å†è°ƒç”¨å·¥å…·ã€‚"
        ))
    
    # è®¡æ•°web_searchè°ƒç”¨æ¬¡æ•°
    web_search_count = sum(1 for m in messages
                           if isinstance(m, AIMessage) and
                           m.tool_calls and
                           any(tc["name"] == "web_search_tool" for tc in m.tool_calls))
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»è°ƒç”¨è¿‡web_searchä½†ç»“æœä¸ç†æƒ³
    if web_search_count >= 5:
        messages.append(SystemMessage(
            content="âš ï¸ é‡è¦æç¤ºï¼šæ‚¨å·²ç»å¤šæ¬¡ä½¿ç”¨ç½‘ç»œæœç´¢ä½†ä»æœªæä¾›æœ€ç»ˆç­”æ¡ˆã€‚"
                    "è¯·åŸºäºå·²æœ‰ä¿¡æ¯ç«‹å³æä¾›å®Œæ•´å›ç­”ï¼Œä¸è¦å†è°ƒç”¨å·¥å…·ã€‚"
        ))
    
    # === å…³é”®æ–°å¢ï¼šæ£€æŸ¥ä¸Šä¸€æ¬¡å·¥å…·è°ƒç”¨ç»“æœçš„ç›¸ä¼¼åº¦ ===
    if len(messages) >= 3:
        last_tool_response = messages[-2]  # ä¸Šä¸€æ¡æ˜¯å·¥å…·å“åº”
        if isinstance(last_tool_response, ToolMessage) and last_tool_response.content:
            # ä»å·¥å…·å“åº”ä¸­æå–æœ€é«˜ç›¸ä¼¼åº¦
            highest_similarity = extract_highest_similarity(last_tool_response.content)
            # å¦‚æœæœ€é«˜ç›¸ä¼¼åº¦ä½äºé˜ˆå€¼ï¼Œæ·»åŠ ç³»ç»Ÿæç¤ºå¼ºåˆ¶ä½¿ç”¨web_search_tool
            if highest_similarity < 0.5:
                logger.warning(f"æ£€æµ‹åˆ°ä½ç›¸å…³æ€§ç»“æœ (ç›¸ä¼¼åº¦: {highest_similarity:.4f})ï¼Œå¼ºåˆ¶ä½¿ç”¨ç½‘ç»œæœç´¢")
                messages.append(SystemMessage(
                    content="âš ï¸ é‡è¦æç¤ºï¼šæœ¬åœ°çŸ¥è¯†åº“æœç´¢ç»“æœç›¸å…³æ€§è¾ƒä½ï¼ˆæœ€é«˜ç›¸ä¼¼åº¦: {:.4f}ï¼‰ã€‚"
                            "è¯·ä½¿ç”¨web_search_toolè·å–æœ€æ–°äº’è”ç½‘ä¿¡æ¯ã€‚".format(highest_similarity)
                ))
    
    # å§‹ç»ˆç»‘å®šæ‰€æœ‰å¯ç”¨å·¥å…·
    model_with_tools = model.bind_tools(available_tools)
    
    # è°ƒç”¨æ¨¡å‹
    response = await model_with_tools.ainvoke(messages)
    
    # å…³é”®ä¿®å¤ï¼šéªŒè¯å¹¶æ¸…ç†å·¥å…·è°ƒç”¨å‚æ•°
    if hasattr(response, "tool_calls") and response.tool_calls:
        cleaned_tool_calls = []
        for tool_call in response.tool_calls:
            # åªä¿ç•™æœ‰æ•ˆçš„å‚æ•°
            valid_args = {}
            # æ ¹æ®å·¥å…·åç§°å¤„ç†å‚æ•°
            if tool_call["name"] == "rag_knowledge_search":
                # ä»…ä¿ç•™queryå‚æ•°
                if "query" in tool_call["args"]:
                    valid_args["query"] = tool_call["args"]["query"]
                else:
                    # å¦‚æœæ²¡æœ‰queryå‚æ•°ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå‚æ•°æˆ–æ•´ä¸ªå†…å®¹ä½œä¸ºæŸ¥è¯¢
                    first_arg = next(iter(tool_call["args"].values()), "æœªçŸ¥æŸ¥è¯¢")
                    valid_args["query"] = str(first_arg)
                    logger.warning(f"rag_knowledge_searchç¼ºå°‘queryå‚æ•°ï¼Œä½¿ç”¨å¤‡ç”¨å‚æ•°: {first_arg}")
            elif tool_call["name"] == "web_search_tool":
                # ä»…ä¿ç•™queryå‚æ•°
                if "query" in tool_call["args"]:
                    valid_args["query"] = tool_call["args"]["query"]
                else:
                    first_arg = next(iter(tool_call["args"].values()), "æœªçŸ¥æŸ¥è¯¢")
                    valid_args["query"] = str(first_arg)
                    logger.warning(f"web_search_toolç¼ºå°‘queryå‚æ•°ï¼Œä½¿ç”¨å¤‡ç”¨å‚æ•°: {first_arg}")
            # æ·»åŠ ç”¨æˆ·æ–‡æ¡£å·¥å…·å‚æ•°å¤„ç†
            elif tool_call["name"].startswith("search_"):
                # ä»…ä¿ç•™queryå‚æ•°
                if "query" in tool_call["args"]:
                    valid_args["query"] = tool_call["args"]["query"]
                else:
                    first_arg = next(iter(tool_call["args"].values()), "æœªçŸ¥æŸ¥è¯¢")
                    valid_args["query"] = str(first_arg)
                    logger.warning(f"{tool_call['name']}ç¼ºå°‘queryå‚æ•°ï¼Œä½¿ç”¨å¤‡ç”¨å‚æ•°: {first_arg}")
            
            # åˆ›å»ºæ¸…ç†åçš„å·¥å…·è°ƒç”¨
            cleaned_tool_call = {
                "name": tool_call["name"],
                "args": valid_args,
                "id": tool_call["id"]
            }
            cleaned_tool_calls.append(cleaned_tool_call)
        
        # æ›¿æ¢åŸå§‹çš„tool_calls
        response.tool_calls = cleaned_tool_calls
        logger.info(f"å·²æ¸…ç†å·¥å…·è°ƒç”¨å‚æ•°ï¼Œç§»é™¤æ— æ•ˆå‚æ•°")
    
    # è®¡ç®—å·¥å…·è°ƒç”¨å¢é‡
    tool_call_increment = 1 if (hasattr(response, "tool_calls") and response.tool_calls) else 0
    return {
        "messages": [response],
        "tool_call_count": tool_call_increment,
        "knowledge_base_name": knowledge_base_name,  # ç¡®ä¿ä¼ é€’çŸ¥è¯†åº“åç§°
        "user_document_tools": user_document_tools_list,  # ç¡®ä¿ä¼ é€’ç”¨æˆ·æ–‡æ¡£å·¥å…·åˆ—è¡¨
        "web_search_enabled": state.get("web_search_enabled", True)
    }

# 5. ä¿®æ”¹æ¡ä»¶å‡½æ•°
def should_continue(state: AgentState):
    """å†³å®šæ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·æˆ–ç»“æŸ"""
    messages = state["messages"]
    last_message = messages[-1]
    
    # æ£€æŸ¥å·¥å…·è°ƒç”¨æ¬¡æ•° - è¶…è¿‡3æ¬¡å¼ºåˆ¶ç»“æŸ
    if state.get("tool_call_count", 0) >= 5:
        return END
    
    # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œåˆ™ç»§ç»­
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"
    
    # å¦åˆ™ç»“æŸ
    return END

# 6. å…¨å±€å˜é‡å­˜å‚¨ç¼–è¯‘åçš„å›¾
graph = None

def tool_node(state: AgentState):
    """è‡ªå®šä¹‰å·¥å…·èŠ‚ç‚¹ï¼Œèƒ½æ ¹æ®çŸ¥è¯†åº“åç§°åŠ¨æ€è·å–å·¥å…·"""
    messages = state["messages"]
    last_message = messages[-1]
    knowledge_base_name = state.get("knowledge_base_name", "test")
    
    # åŠ¨æ€è·å–å½“å‰çŸ¥è¯†åº“çš„RAGå·¥å…·
    rag_tool = get_rag_tool(knowledge_base_name)
    
    # åˆ›å»ºå·¥å…·æ˜ å°„
    tools = {
        "rag_knowledge_search": rag_tool
    }
    
    # ä»…å½“å¯ç”¨æ—¶æ‰æ·»åŠ webæœç´¢å·¥å…·
    if state.get("web_search_enabled", True):
        global web_search_tool
        if web_search_tool is None:
            web_search_tool = create_search_tool()
        tools["web_search_tool"] = web_search_tool
    
    # æ·»åŠ ç”¨æˆ·æ–‡æ¡£å·¥å…·
    user_document_tools_list = state.get("user_document_tools", [])
    for tool_name in user_document_tools_list:
        tool_info = get_user_document_tool(tool_name)
        if tool_info and "tool" in tool_info:
            tools[tool_info["tool"].name] = tool_info["tool"]
    
    # æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
    outputs = []
    for tool_call in last_message.tool_calls:
        tool_name = tool_call["name"]
        if tool_name in tools:
            tool = tools[tool_name]
            try:
                # è°ƒç”¨å·¥å…·
                response = tool.invoke(tool_call["args"])
                outputs.append(
                    ToolMessage(
                        content=str(response),
                        name=tool_name,
                        tool_call_id=tool_call["id"]
                    )
                )
            except Exception as e:
                outputs.append(
                    ToolMessage(
                        content=f"å·¥å…·è°ƒç”¨é”™è¯¯: {str(e)}",
                        name=tool_name,
                        status="error",
                        tool_call_id=tool_call["id"]
                    )
                )
        else:
            outputs.append(
                ToolMessage(
                    content=f"Error: {tool_name} is not a valid tool, try one of [{', '.join(tools.keys())}]",
                    name=tool_name,
                    status="error"
                )
            )
    
    return {"messages": outputs}

# 8. æ ¸å¿ƒAPIç«¯ç‚¹
class ChatRequest(BaseModel):
    message: str
    session_id: Optional[str] = None
    stream: Optional[bool] = False  # æ˜¯å¦å¯ç”¨æµå¼å“åº”
    knowledge_base_name: Optional[str] = "test"  # æ–°å¢å‚æ•°ï¼Œé»˜è®¤ä¸º"test"
    url: Optional[str] = None
    enable_web_search: Optional[bool] = True
    message_id: Optional[str] = None

class ChatResponse(BaseModel):
    response: str
    session_id: str
    conversation_history: List[Dict[str, str]]
    tool_calls: Optional[List[Dict]] = None

class ErrorResponse(BaseModel):
    detail: str

@agent_router.post(
    "/chat",
    response_model=ChatResponse,
    responses={
        400: {"model": ErrorResponse, "description": "æ— æ•ˆçš„ä¼šè¯ID"},
        500: {"model": ErrorResponse, "description": "å†…éƒ¨æœåŠ¡é”™è¯¯"}
    }
)
async def chat_endpoint(request: ChatRequest):
    """
    å¤„ç†èŠå¤©è¯·æ±‚
    - æ–°ä¼šè¯: ä¸æä¾›session_id
    - ç»­ä¼šè¯: æä¾›æœ‰æ•ˆçš„session_id
    - url: å¯é€‰ï¼Œç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£URL
    """
    global graph
    if not graph:
        raise HTTPException(
            status_code=503,
            detail="æœåŠ¡å°šæœªåˆå§‹åŒ–å®Œæˆ"
        )
    
    # ç”Ÿæˆ/éªŒè¯ä¼šè¯ID
    session_id = request.session_id or f"session_{uuid.uuid4()}"
    config = {"configurable": {"thread_id": session_id}}
    
    try:
        # è·å–å½“å‰ä¼šè¯çŠ¶æ€
        state = await graph.aget_state(config)
        
        # æ„å»ºæ–°çŠ¶æ€
        if state is None or not isinstance(state.values, dict) or "messages" not in state.values:
            # æ–°ä¼šè¯ï¼ˆåŒ…æ‹¬çŠ¶æ€ä¸å®Œæ•´çš„æƒ…å†µï¼‰
            user_document_tools_list = []
            # =============== æ–°å¢ï¼šå¤„ç†æ–‡æ¡£URL ===============
            if request.url:
                try:
                    logger.info(f"å¤„ç†ç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£URL: {request.url}")
                    # ä½¿ç”¨session_idä½œä¸ºdocument_id
                    tool_name = register_user_document_tool(
                        url=request.url,
                        document_id=session_id,
                        document_name="ç”¨æˆ·ä¸Šä¼ çš„åŸºå› æ£€æµ‹æŠ¥å‘Š"
                    )
                    user_document_tools_list.append(tool_name)
                    logger.info(f"æˆåŠŸæ³¨å†Œç”¨æˆ·æ–‡æ¡£å·¥å…·: {tool_name}")
                except Exception as e:
                    logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")
                    # å³ä½¿æ–‡æ¡£å¤„ç†å¤±è´¥ï¼Œä¹Ÿè¦ç»§ç»­å¯¹è¯
            # =============== æ–°å¢ç»“æŸ ===============
            
            initial_state = {
                "messages": [HumanMessage(content=request.message)],
                "knowledge_base_name": request.knowledge_base_name,
                "tool_call_count": 0,
                "user_document_tools": user_document_tools_list,
                "web_search_enabled": request.enable_web_search  # ä¿å­˜webæœç´¢å¼€å…³çŠ¶æ€
            }
        else:
            # ç»­ä¼šè¯ - å¤åˆ¶ç°æœ‰çŠ¶æ€å¹¶æ·»åŠ æ–°æ¶ˆæ¯
            # ä¿ç•™ä¹‹å‰çš„çŸ¥è¯†åº“åç§°ï¼Œå³ä½¿è¯·æ±‚ä¸­æä¾›äº†æ–°å€¼ï¼ˆé¿å…ä¸­é€”åˆ‡æ¢çŸ¥è¯†åº“å¯¼è‡´æ··æ·†ï¼‰
            knowledge_base_name = state.values.get("knowledge_base_name", request.knowledge_base_name)
            
            # =============== æ–°å¢ï¼šå¤„ç†æ–‡æ¡£URLï¼ˆå¦‚æœæ˜¯æ–°ä¸Šä¼ çš„æ–‡æ¡£ï¼‰ ===============
            user_document_tools_list = state.values.get("user_document_tools", [])
            # å¦‚æœæœ‰æ–°çš„URLä¸”è¿˜æ²¡æœ‰æ³¨å†Œè¿‡å¯¹åº”çš„å·¥å…·
            if request.url and not any(
                    tool_name.startswith("search_" + session_id) for tool_name in user_document_tools_list):
                try:
                    logger.info(f"å¤„ç†ç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£URL: {request.url}")
                    # ä½¿ç”¨session_idä½œä¸ºdocument_id
                    tool_name = register_user_document_tool(
                        url=request.url,
                        document_id=session_id,
                        document_name="ç”¨æˆ·ä¸Šä¼ çš„åŸºå› æ£€æµ‹æŠ¥å‘Š"
                    )
                    user_document_tools_list.append(tool_name)
                    logger.info(f"æˆåŠŸæ³¨å†Œç”¨æˆ·æ–‡æ¡£å·¥å…·: {tool_name}")
                except Exception as e:
                    logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")
            # =============== æ–°å¢ç»“æŸ ===============
            
            # ä¿ç•™ä¹‹å‰çš„webæœç´¢è®¾ç½®ï¼Œé™¤éè¯·æ±‚ä¸­æä¾›äº†æ–°å€¼
            web_search_enabled = state.values.get("web_search_enabled", request.enable_web_search)
            
            initial_state = {
                "messages": state.values["messages"] + [HumanMessage(content=request.message)],
                "knowledge_base_name": knowledge_base_name,
                "tool_call_count": state.values.get("tool_call_count", 0),
                "user_document_tools": user_document_tools_list,
                "web_search_enabled": web_search_enabled
            }
        
        # æ‰§è¡Œå¯¹è¯æµ
        final_state = None
        async for step in graph.astream(initial_state, config=config, stream_mode="values"):
            final_state = step
        
        if not final_state:
            raise HTTPException(
                status_code=500,
                detail="å¯¹è¯æµç¨‹æœªäº§ç”Ÿæœ‰æ•ˆå“åº”"
            )
        
        # æå–æœ€æ–°å›å¤
        last_msg = final_state["messages"][-1]
        if not isinstance(last_msg, AIMessage):
            raise HTTPException(
                status_code=500,
                detail="æ— æ•ˆçš„æ¨¡å‹å“åº”ç±»å‹"
            )
        
        # æ”¶é›†å·¥å…·è°ƒç”¨ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        tool_calls = []
        if hasattr(last_msg, "tool_calls") and last_msg.tool_calls:
            for tool_call in last_msg.tool_calls:
                tool_calls.append({
                    "name": tool_call["name"],
                    "args": tool_call["args"],
                    "id": tool_call["id"]
                })
        
        # æ„å»ºå†å²è®°å½•
        history = []
        for msg in final_state["messages"]:
            if isinstance(msg, HumanMessage):
                history.append({"role": "user", "content": msg.content})
            elif isinstance(msg, AIMessage):
                history.append({"role": "assistant", "content": msg.content})
            elif isinstance(msg, SystemMessage):
                history.append({"role": "system", "content": "ç³»ç»Ÿæç¤º"})
            elif isinstance(msg, ToolMessage):
                history.append({"role": "tool", "content": msg.content})
        
        return ChatResponse(
            response=last_msg.content,
            session_id=session_id,
            conversation_history=history,
            tool_calls=tool_calls if tool_calls else None
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ å¤„ç†è¯·æ±‚å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"è¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}"
        )

# 9. æµå¼å“åº”APIï¼ˆå¯é€‰ï¼‰
@agent_router.post("/chat/stream")
async def chat_stream_endpoint(request: ChatRequest):
    """
    æµå¼å“åº”èŠå¤©è¯·æ±‚ï¼ˆå›ºå®šåˆ†å—å¤§å°ï¼‰
    - ä½¿ç”¨æ ‡å‡†SSEæ ¼å¼
    - ä¿®å¤æµå¼å“åº”é—®é¢˜
    """
    global graph
    if not graph:
        raise HTTPException(
            status_code=503,
            detail="æœåŠ¡å°šæœªåˆå§‹åŒ–å®Œæˆ"
        )

    # ç”Ÿæˆ/éªŒè¯ä¼šè¯ID
    session_id = request.session_id or f"session_{uuid.uuid4()}"
    message_id = request.message_id
    config = {"configurable": {"thread_id": session_id}}

    async def event_generator():
        try:
            # è·å–å½“å‰ä¼šè¯çŠ¶æ€å’Œæ„å»ºåˆå§‹çŠ¶æ€
            state = await graph.aget_state(config)
            if state is None or not isinstance(state.values, dict) or "messages" not in state.values:
                # æ–°ä¼šè¯å¤„ç†...
                user_document_tools_list = []
                if request.url:
                    try:
                        logger.info(f"å¤„ç†ç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£URL: {request.url}")
                        document_id = f"{session_id}_{uuid.uuid4().hex[:8]}"
                        tool_name = register_user_document_tool(
                            url=request.url,
                            document_id=document_id,
                            document_name="ç”¨æˆ·ä¸Šä¼ çš„åŸºå› æ£€æµ‹æŠ¥å‘Š"
                        )
                        user_document_tools_list.append(tool_name)
                        logger.info(f"æˆåŠŸæ³¨å†Œç”¨æˆ·æ–‡æ¡£å·¥å…·: {tool_name}")
                    except Exception as e:
                        logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")

                initial_state = {
                    "messages": [HumanMessage(content=request.message)],
                    "knowledge_base_name": request.knowledge_base_name,
                    "tool_call_count": 0,
                    "user_document_tools": user_document_tools_list,
                    "web_search_enabled": request.enable_web_search
                }
            else:
                # ç»­ä¼šè¯å¤„ç†...
                knowledge_base_name = state.values.get("knowledge_base_name", request.knowledge_base_name)
                user_document_tools_list = state.values.get("user_document_tools", [])
                web_search_enabled = state.values.get("web_search_enabled", request.enable_web_search)

                if request.url and not any(
                        tool_name.startswith("search_" + session_id) for tool_name in user_document_tools_list):
                    try:
                        logger.info(f"å¤„ç†ç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£URL: {request.url}")
                        document_id = f"{session_id}_{uuid.uuid4().hex[:8]}"
                        tool_name = register_user_document_tool(
                            url=request.url,
                            document_id=document_id,
                            document_name="ç”¨æˆ·ä¸Šä¼ çš„åŸºå› æ£€æµ‹æŠ¥å‘Š"
                        )
                        user_document_tools_list.append(tool_name)
                        logger.info(f"æˆåŠŸæ³¨å†Œç”¨æˆ·æ–‡æ¡£å·¥å…·: {tool_name}")
                    except Exception as e:
                        logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")

                initial_state = {
                    "messages": state.values["messages"] + [HumanMessage(content=request.message)],
                    "knowledge_base_name": knowledge_base_name,
                    "tool_call_count": state.values.get("tool_call_count", 0),
                    "user_document_tools": user_document_tools_list,
                    "web_search_enabled": web_search_enabled
                }

            # å›ºå®šåˆ†å—å¤§å°
            CHUNK_SIZE = 10
            full_text = ""
            
            try:
                async for step in graph.astream_log(initial_state, config=config):
                    if isinstance(step, dict) and "ops" in step:
                        ops = step["ops"]
                    elif hasattr(step, "ops"):
                        ops = step.ops
                    else:
                        continue

                    for op in ops:
                        path = op.get("path", "") if isinstance(op, dict) else getattr(op, "path", "")
                        value = op.get("value") if isinstance(op, dict) else getattr(op, "value", None)

                        if path.startswith("/logs/call_model/") and value is not None:
                            if isinstance(value, dict) and "messages" in value:
                                for msg in value["messages"]:
                                    if isinstance(msg, AIMessage) and hasattr(msg, "content"):
                                        full_text += msg.content
                                        # å›ºå®šåˆ†å—å¤„ç†
                                        while len(full_text) >= CHUNK_SIZE:
                                            chunk = full_text[:CHUNK_SIZE]
                                            full_text = full_text[CHUNK_SIZE:]
                                            logger.info(f"å‘é€æµå¼å†…å®¹: {chunk}")  # ä¿®å¤ï¼šè®°å½•chunkè€Œéfull_text

                                            # æ„å»ºç¬¦åˆSSEè§„èŒƒçš„æ•°æ®å—
                                            data = {
                                                "text": chunk,
                                                "finish_reason": None,
                                                "session_id": session_id,
                                                "message_id": message_id
                                            }
                                            # âœ… ä¿®å¤1ï¼šä½¿ç”¨æ­£ç¡®çš„SSEæ ¼å¼
                                            yield f"data: {json.dumps(data)}\n\n"
                                            await asyncio.sleep(0.01)
                                            
                                            # æ˜¾å­˜ä¼˜åŒ–ï¼šå®šæœŸæ¸…ç†
                                            if len(full_text) % 50 == 0:  # æ¯50ä¸ªå­—ç¬¦æ¸…ç†ä¸€æ¬¡
                                                import gc
                                                gc.collect()
                                                logger.info("ğŸ§¹ æ‰§è¡Œæ˜¾å­˜æ¸…ç†")
            except Exception as e:
                logger.error(f"âŒ æµå¼å“åº”å¤±è´¥: {e}")
                # æ˜¾å­˜æ¸…ç†
                import gc
                gc.collect()
                raise e
            
            # å‘é€å‰©ä½™å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
            if full_text:
                data = {
                    "text": full_text,
                    "finish_reason": None,
                    "session_id": session_id,
                    "message_id": message_id
                }
                yield f"data: {json.dumps(data)}\n\n"

            # å‘é€ç»“æŸæ ‡è®°
            end_data = {
                "text": "",
                "finish_reason": "stop",
                "session_id": session_id,
                "message_id": message_id
            }
            yield f"data: {json.dumps(end_data)}\n\n"
            
            # æœ€ç»ˆæ˜¾å­˜æ¸…ç†
            import gc
            gc.collect()
            logger.info("ğŸ§¹ æµå¼å“åº”å®Œæˆï¼Œæ‰§è¡Œæœ€ç»ˆæ˜¾å­˜æ¸…ç†")
        except Exception as e:
            logger.error(f"âŒ event_generator å¤±è´¥: {e}")
            # å‘é€é”™è¯¯ä¿¡æ¯
            error_data = {
                "text": f"é”™è¯¯: {str(e)}",
                "finish_reason": "error",
                "session_id": session_id,
                "message_id": message_id
            }
            yield f"data: {json.dumps(error_data)}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream"  # ç¡®ä¿è¿™æ˜¯æ­£ç¡®çš„åª’ä½“ç±»å‹
    )

# 10. ä¼šè¯ç®¡ç†API
@agent_router.get("/sessions/{session_id}")
async def get_session(session_id: str):
    """è·å–ç‰¹å®šä¼šè¯çš„å®Œæ•´å†å²"""
    global graph
    if not graph:
        raise HTTPException(
            status_code=503,
            detail="æœåŠ¡å°šæœªåˆå§‹åŒ–å®Œæˆ"
        )
    
    config = {"configurable": {"thread_id": session_id}}
    
    try:
        state = await graph.aget_state(config)
        if not state or not isinstance(state.values, dict) or "messages" not in state.values:
            raise HTTPException(
                status_code=404,
                detail="ä¼šè¯ä¸å­˜åœ¨"
            )
        
        history = []
        for msg in state.values["messages"]:
            if isinstance(msg, HumanMessage):
                history.append({"role": "user", "content": msg.content})
            elif isinstance(msg, AIMessage):
                history.append({"role": "assistant", "content": msg.content})
            elif isinstance(msg, SystemMessage):
                history.append({"role": "system", "content": "ç³»ç»Ÿæç¤º"})
            elif isinstance(msg, ToolMessage):
                history.append({"role": "tool", "content": msg.content})
        
        return {
            "session_id": session_id,
            "conversation_history": history,
            "last_updated": state.config["configurable"].get("checkpoint_id")
        }
    except Exception as e:
        logger.error(f"âŒ è·å–ä¼šè¯å¤±è´¥: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"è·å–ä¼šè¯å¤±è´¥: {str(e)}"
        )

@agent_router.delete("/sessions/{session_id}")
async def delete_session(session_id: str):
    """åˆ é™¤ç‰¹å®šä¼šè¯"""
    global checkpointer
    if not checkpointer:
        raise HTTPException(
            status_code=503,
            detail="æœåŠ¡å°šæœªåˆå§‹åŒ–å®Œæˆ"
        )
    
    try:
        # åˆ é™¤ä¼šè¯
        await checkpointer.aput(
            {"configurable": {"thread_id": session_id}},
            None,  # ä¼ é€’Noneè¡¨ç¤ºåˆ é™¤
            None  # ä¼ é€’Noneè¡¨ç¤ºåˆ é™¤
        )
        return {"status": "success", "message": f"ä¼šè¯ {session_id} å·²åˆ é™¤"}
    except Exception as e:
        logger.error(f"âŒ åˆ é™¤ä¼šè¯å¤±è´¥: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"åˆ é™¤ä¼šè¯å¤±è´¥: {str(e)}"
        )

# ======================
# ç»Ÿä¸€å¥åº·æ£€æŸ¥å’Œä¸»åº”ç”¨
# ======================
@asynccontextmanager
async def lifespan(app: FastAPI):
    global qdrant_client, checkpointer, web_search_tool, graph
    
    try:
        # 1. åˆå§‹åŒ–Qdrantå®¢æˆ·ç«¯ï¼ˆçŸ¥è¯†åº“ç®¡ç†éœ€è¦ï¼‰
        logger.info("åˆå§‹åŒ–Qdrantå®¢æˆ·ç«¯...")
        qdrant_client = QdrantClient(host="localhost", port=6333)
        logger.info("âœ… Qdrantå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        
        # 2. åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± ï¼ˆAgentéœ€è¦ï¼‰
        logger.info("åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± ...")
        DB_URI = os.getenv(
            "DB_URI",
            "postgresql://postgres:postgres@localhost:5432/langgraph_db?sslmode=disable"
        )
        connection_kwargs = {
            "autocommit": True,
            "prepare_threshold": 0,
        }
        pool = AsyncConnectionPool(
            conninfo=DB_URI,
            max_size=20,
            kwargs=connection_kwargs
        )
        await pool.open()
        
        logger.info("åˆå§‹åŒ–æ•°æ®åº“æ£€æŸ¥ç‚¹å­˜å‚¨...")
        checkpointer = AsyncPostgresSaver(pool)
        await checkpointer.setup()
        logger.info("âœ… æ•°æ®åº“æ£€æŸ¥ç‚¹å­˜å‚¨åˆå§‹åŒ–æˆåŠŸ")
        
        # 3. åˆå§‹åŒ–webæœç´¢å·¥å…·
        try:
            logger.info("åˆå§‹åŒ–webæœç´¢å·¥å…·...")
            web_search_tool = create_search_tool()
            logger.info("âœ… å·¥å…·åŠ è½½æˆåŠŸ: web_search")
        except Exception as e:
            logger.error(f"âŒ å·¥å…·åŠ è½½å¤±è´¥: {str(e)}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œç»§ç»­åˆå§‹åŒ–å…¶ä»–ç»„ä»¶
        
        # 4. ç¼–è¯‘Agentå›¾
        logger.info("ç¼–è¯‘Agentå›¾...")
        builder = StateGraph(AgentState)
        builder.add_node("call_model", call_model)
        builder.add_node("tools", tool_node)
        builder.add_edge(START, "call_model")
        builder.add_conditional_edges(
            "call_model",
            should_continue,
            {
                "tools": "tools",
                END: END
            }
        )
        builder.add_edge("tools", "call_model")
        
        # ç¼–è¯‘å›¾
        graph = builder.compile(checkpointer=checkpointer)
        logger.info("âœ… Agentå›¾ç¼–è¯‘æˆåŠŸ")
        
        yield  # åº”ç”¨è¿è¡Œä¸­
        
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–å¤±è´¥: {str(e)}", exc_info=True)
        raise
    finally:
        # æ¸…ç†èµ„æº
        if 'pool' in locals():
            await pool.close()
            logger.info("âœ… æ•°æ®åº“è¿æ¥æ± å·²å…³é—­")

# åˆ›å»ºç»Ÿä¸€çš„FastAPIåº”ç”¨
app = FastAPI(
    title="ç»Ÿä¸€çŸ¥è¯†åº“ä¸å¯¹è¯AgentæœåŠ¡",
    description="æ•´åˆçŸ¥è¯†åº“ç®¡ç†å’Œå¯¹è¯AgentåŠŸèƒ½",
    version="1.0.0",
    lifespan=lifespan
)

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ³¨å†Œå­åº”ç”¨
app.include_router(kb_router)
app.include_router(agent_router)

# å¥åº·æ£€æŸ¥ç«¯ç‚¹ï¼ˆç»Ÿä¸€ï¼‰
@app.get("/health")
async def health_check():
    """ç»Ÿä¸€çš„å¥åº·æ£€æŸ¥"""
    try:
        # æ£€æŸ¥Qdrant
        global qdrant_client
        kb_status = "disconnected"
        total_kb = 0
        if qdrant_client:
            try:
                collections = qdrant_client.get_collections().collections
                kb_status = "connected"
                total_kb = len(collections)
            except Exception as e:
                kb_status = f"error: {str(e)}"
        
        # æ£€æŸ¥æ•°æ®åº“
        db_status = "connected" if checkpointer else "disconnected"
        
        # æ£€æŸ¥Agentå›¾
        agent_status = "initialized" if graph else "not_initialized"
        
        return {
            "status": "healthy",
            "knowledge_base": kb_status,
            "agent_database": db_status,
            "agent_status": agent_status,
            "total_knowledge_bases": total_kb,
            "service_info": {
                "port": 8000,
                "kb_api_prefix": "/kb",
                "agent_api_prefix": "/agent"
            }
        }
    except Exception as e:
        logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}", exc_info=True)
        return {
            "status": "unhealthy",
            "error": str(e),
            "service_info": {
                "port": 8000,
                "kb_api_prefix": "/kb",
                "agent_api_prefix": "/agent"
            }
        }

if __name__ == "__main__":
    import uvicorn
    logger.info("å¯åŠ¨ç»Ÿä¸€æœåŠ¡...")
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")