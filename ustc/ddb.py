from enum import Enum
from fastapi import FastAPI, HTTPException, status, APIRouter
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Dict, List, Optional, Any, AsyncGenerator, TypedDict, Annotated
import uuid
import logging
from contextlib import asynccontextmanager, contextmanager
import os
import requests
import tempfile
import shutil
import operator
import re
import json
import asyncio
import gc
import torch
from email._header_value_parser import parse_message_id
from operator import add
from psycopg_pool import AsyncConnectionPool
from langchain_core.messages import ToolMessage, HumanMessage, SystemMessage, AIMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver
from qdrant_client import QdrantClient
from qdrant_client.http.models import VectorParams, Distance, FieldCondition, MatchValue
from oss2 import Auth, Bucket
from starlette.responses import StreamingResponse
from chunks2embedding import (
    embedding_init,
    upsert_md_file,
    upsert_qa_pair,
    delete_by_source,
    list_all_collections,
    get_collection_info
)
from pdf2md import pdf2md
from pdf import (
    register_user_document_tool,
    get_user_document_tool,
    get_user_document_tool_by_session,
    list_user_document_tools
)
from searxng_server import create_search_tool
import datetime

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.StreamHandler()
    ]
)

# åˆ›å»ºä¸“é—¨çš„å¯¹è¯æ—¥å¿—è®°å½•å™¨
chat_logger = logging.getLogger("chat-flow")
chat_logger.setLevel(logging.INFO)
# é˜²æ­¢æ—¥å¿—ä¼ æ’­åˆ°çˆ¶çº§loggerï¼Œé¿å…é‡å¤è¾“å‡º
chat_logger.propagate = False

# åˆ›å»ºå¯¹è¯æ—¥å¿—æ–‡ä»¶å¤„ç†å™¨
import os
log_dir = "logs"
os.makedirs(log_dir, exist_ok=True)
chat_file_handler = logging.FileHandler(os.path.join(log_dir, "chat_flow.log"), encoding='utf-8')
chat_file_handler.setFormatter(logging.Formatter("%(asctime)s [CHAT] %(message)s"))
chat_logger.addHandler(chat_file_handler)

# å¯é€‰ï¼šåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°ï¼ˆå¦‚æœä½ æƒ³è¦å®æ—¶çœ‹åˆ°å¯¹è¯æ—¥å¿—ï¼‰
# chat_console_handler = logging.StreamHandler()
# chat_console_handler.setFormatter(logging.Formatter("%(asctime)s [CHAT] %(message)s"))
# chat_logger.addHandler(chat_console_handler)

# åˆ›å»ºç»Ÿä¸€çš„æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger("unified-service")

# ======================
# æ˜¾å­˜ç®¡ç†å·¥å…·
# ======================
class GPUResourceManager:
    """ç®¡ç†GPUèµ„æºï¼Œç¡®ä¿åŒä¸€æ—¶é—´åªæœ‰ä¸€ä¸ªæ¨¡å‹åœ¨ä½¿ç”¨"""
    
    def __init__(self):
        self.lock = asyncio.Lock()
        self.current_model = None
        self.model_instances = {}
        
    async def acquire(self, model_type):
        """è·å–GPUèµ„æºå¹¶ç¡®ä¿æŒ‡å®šç±»å‹çš„æ¨¡å‹å·²åŠ è½½"""
        logger.info(f"å°è¯•è·å–GPUèµ„æºä»¥ä½¿ç”¨ {model_type} æ¨¡å‹...")
        await self.lock.acquire()
        logger.info(f"å·²è·å–GPUèµ„æºï¼Œå‡†å¤‡ä½¿ç”¨ {model_type} æ¨¡å‹")
        
        try:
            # å¦‚æœå·²ç»æœ‰å…¶ä»–æ¨¡å‹åŠ è½½ï¼Œå…ˆæ¸…ç†
            if self.current_model and self.current_model != model_type:
                await self.release()
            
            # è®°å½•å½“å‰ä½¿ç”¨çš„æ¨¡å‹ç±»å‹
            self.current_model = model_type
            return self
        except Exception as e:
            logger.error(f"è·å–GPUèµ„æºå¤±è´¥: {str(e)}")
            self.lock.release()
            raise
            
    async def release(self):
        """é‡Šæ”¾GPUèµ„æºï¼Œæ¸…ç†å½“å‰æ¨¡å‹"""
        if self.current_model:
            logger.info(f"æ­£åœ¨æ¸…ç† {self.current_model} æ¨¡å‹å ç”¨çš„èµ„æº...")
            # ä»å®ä¾‹ç¼“å­˜ä¸­ç§»é™¤
            if self.current_model in self.model_instances:
                del self.model_instances[self.current_model]
            self.current_model = None
            
            # æ˜¾å¼æ¸…ç†GPUå†…å­˜
            self.clear_gpu_memory()
            
        logger.info("GPUèµ„æºå·²é‡Šæ”¾")
        self.lock.release()
        
    def clear_gpu_memory(self):
        """æ¸…ç†GPUå†…å­˜"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
            logger.info(f"GPUå†…å­˜å·²æ¸…ç† - å·²é‡Šæ”¾ {torch.cuda.memory_allocated()/1024**2:.2f} MB")
    
    def get_ollama_model(self):
        """è·å–Ollamaæ¨¡å‹å®ä¾‹ï¼ˆæŒ‰éœ€åŠ è½½ï¼‰"""
        if "ollama" not in self.model_instances:
            inference_server_url = "http://localhost:11434/v1"
            self.model_instances["ollama"] = ChatOpenAI(
                model="qwen3:4b",
                openai_api_key="none",
                openai_api_base=inference_server_url,
                max_tokens=300,
                temperature=0.7,
                timeout=30.0,
                request_timeout=30.0,
                max_retries=2
            )
        return self.model_instances["ollama"]
    
    def get_embedding_model(self):
        """è·å–å‘é‡åŒ–æ¨¡å‹ï¼ˆæŒ‰éœ€åŠ è½½ï¼‰"""
        if "embedding" not in self.model_instances:
            # è¿™é‡Œå¯ä»¥æ ¹æ®éœ€è¦åˆå§‹åŒ–å‘é‡åŒ–æ¨¡å‹
            # ç”±äºåŸå§‹ä»£ç ä¸­æ²¡æœ‰æ˜¾ç¤ºå…·ä½“å®ç°ï¼Œæˆ‘ä»¬åªè¿”å›åˆå§‹åŒ–å‡½æ•°
            from chunks2embedding import embedding_init
            self.model_instances["embedding"] = embedding_init
        return self.model_instances["embedding"]
    
    def get_ocr_model(self):
        """è·å–OCRæ¨¡å‹ï¼ˆæŒ‰éœ€åŠ è½½ï¼‰"""
        if "ocr" not in self.model_instances:
            # ç”±äºåŸå§‹ä»£ç ä¸­æ²¡æœ‰æ˜¾ç¤ºå…·ä½“å®ç°ï¼Œæˆ‘ä»¬åªè¿”å›åˆå§‹åŒ–å‡½æ•°
            from pdf2md import pdf2md
            self.model_instances["ocr"] = pdf2md
        return self.model_instances["ocr"]

# åˆ›å»ºå…¨å±€GPUèµ„æºç®¡ç†å™¨
gpu_resource_manager = GPUResourceManager()

# ======================
# å…¨å±€å…±äº«èµ„æº
# ======================
qdrant_client = None
checkpointer = None
rag_tool_cache = {}
web_search_tool = None

# OSS é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
OSS_ACCESS_KEY_ID = os.getenv("OSS_ACCESS_KEY_ID", "")
OSS_ACCESS_KEY_SECRET = os.getenv("OSS_ACCESS_KEY_SECRET", "")
OSS_ENDPOINT = os.getenv("OSS_ENDPOINT", "https://oss-cn-hangzhou.aliyuncs.com")
OSS_BUCKET = os.getenv("OSS_BUCKET", "")

# æœ¬åœ°ä¸´æ—¶è·¯å¾„
LOCAL_DIR = "~/autodl-tmp/pdf"
os.makedirs(LOCAL_DIR, exist_ok=True)

# ======================
# åˆ›å»ºè·¯ç”±
# ======================
kb_router = APIRouter(prefix="/kb", tags=["çŸ¥è¯†åº“ç®¡ç†"])
agent_router = APIRouter(prefix="/agent", tags=["å¯¹è¯Agent"])

# ======================
# å…±äº«å·¥å…·å‡½æ•°
# ======================
def get_current_knowledge_base_info(kb_name: str):
    """åªè·å–æŒ‡å®šçŸ¥è¯†åº“çš„æ–‡æ¡£ä¿¡æ¯ï¼ˆä¸åŒ…å«å…¶ä»–çŸ¥è¯†åº“ï¼‰"""
    global qdrant_client
    try:
        # ä½¿ç”¨å…¨å±€Qdrantå®¢æˆ·ç«¯
        if qdrant_client is None:
            qdrant_client = QdrantClient(host="localhost", port=6333)
        # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
        try:
            qdrant_client.get_collection(kb_name)
        except:
            return {
                "name": kb_name,
                "exists": False,
                "points_count": 0,
                "documents": [],
                "document_count": 0
            }
        # è·å–çŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰æ–‡æ¡£å—ï¼ˆåˆ†é¡µè·å–æ‰€æœ‰æ•°æ®ï¼‰
        all_points = []
        offset = None
        while True:
            # ä½¿ç”¨scroll APIè·å–æ•°æ®
            points, next_offset = qdrant_client.scroll(
                collection_name=kb_name,
                limit=100,
                with_payload=True,
                with_vectors=False,
                offset=offset
            )
            all_points.extend(points)
            if not next_offset:
                break
            offset = next_offset
        # æå–å”¯ä¸€æ–‡æ¡£å
        document_names = set()
        for point in all_points:
            try:
                # å°è¯•è®¿é—®sourceå­—æ®µ
                if "metadata" in point.payload and "source" in point.payload["metadata"]:
                    document_names.add(point.payload["metadata"]["source"])
            except Exception as e:
                logger.warning(f"å¤„ç†ç‚¹æ—¶å‡ºé”™: {str(e)}")
        return {
            "name": kb_name,
            "exists": True,
            "points_count": len(all_points),
            "documents": list(document_names),
            "document_count": len(document_names)
        }
    except Exception as e:
        logger.error(f"è·å–çŸ¥è¯†åº“ä¿¡æ¯å¤±è´¥: {str(e)}", exc_info=True)
        return {
            "name": kb_name,
            "exists": False,
            "points_count": 0,
            "documents": [],
            "document_count": 0
        }

def download_pdf_from_oss(file_key, local_path):
    """ä»OSSä¸‹è½½æ–‡ä»¶"""
    try:
        # åˆå§‹åŒ–OSSå®¢æˆ·ç«¯
        auth = Auth(OSS_ACCESS_KEY_ID, OSS_ACCESS_KEY_SECRET)
        bucket = Bucket(auth, OSS_ENDPOINT, OSS_BUCKET)
        bucket.get_object_to_file(file_key, local_path)
        logger.info(f"æ–‡ä»¶ä¸‹è½½æˆåŠŸ: {local_path}")
        return True
    except Exception as e:
        logger.error(f"æ–‡ä»¶ä¸‹è½½å¤±è´¥: {str(e)}")
        return False

def get_rag_tool(knowledge_base_name: str):
    """æ ¹æ®çŸ¥è¯†åº“åç§°è·å–æˆ–åˆ›å»ºRAGå·¥å…·"""
    if knowledge_base_name in rag_tool_cache:
        return rag_tool_cache[knowledge_base_name]
    # åˆ›å»ºæ–°çš„RAGå·¥å…·å®ä¾‹
    from rag_tool import create_rag_tool
    rag_tool = create_rag_tool(
        host="localhost",
        port=6333,
        collection_name=knowledge_base_name
    )
    rag_tool_cache[knowledge_base_name] = rag_tool
    return rag_tool

def extract_highest_similarity(tool_response: str) -> float:
    """ä»å·¥å…·å“åº”ä¸­æå–æœ€é«˜ç›¸ä¼¼åº¦"""
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾æ‰€æœ‰ç›¸ä¼¼åº¦å€¼
    similarity_values = re.findall(r"ç›¸ä¼¼åº¦: ([\d.]+)", tool_response)
    if not similarity_values:
        logger.warning("æœªåœ¨å·¥å…·å“åº”ä¸­æ‰¾åˆ°ç›¸ä¼¼åº¦ä¿¡æ¯")
        return 0.0
    # è½¬æ¢ä¸ºæµ®ç‚¹æ•°å¹¶è¿”å›æœ€å¤§å€¼
    try:
        similarities = [float(val) for val in similarity_values]
        highest = max(similarities)
        logger.info(f"æ£€æµ‹åˆ°æœ€é«˜ç›¸ä¼¼åº¦: {highest:.4f}")
        return highest
    except ValueError:
        logger.error("æ— æ³•è§£æç›¸ä¼¼åº¦å€¼")
        return 0.0

# ======================
# çŸ¥è¯†åº“ç®¡ç†APIè·¯ç”±
# ======================
# 1. å®šä¹‰APIè¯·æ±‚æ¨¡å‹
class KnowledgeBaseAction(str, Enum):
    CREATE = "create"
    UPLOAD = "upload"
    DELETE_DOCUMENT = "delete_document"
    DELETE = "delete"

class KnowledgeBaseRequest(BaseModel):
    action: KnowledgeBaseAction
    name: str
    document_name: Optional[str] = None

# 2. ä¿®å¤KnowledgeBaseResponseå®šä¹‰
class KnowledgeBaseResponse(BaseModel):
    success: bool
    message: str
    data: Optional[Dict[str, Any]] = None

# æ–°å¢ï¼šé—®ç­”å¯¹ä¸Šä¼ è¯·æ±‚æ¨¡å‹
class QAPairRequest(BaseModel):
    knowledge_base_name: str
    question: str
    answer: str
    document_name: Optional[str] = None  # æ–°å¢ï¼šæ–‡æ¡£åï¼Œå¦‚æœä¸æä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆ
    metadata: Optional[Dict[str, Any]] = None  # å¯é€‰çš„å…ƒæ•°æ®ï¼Œå¦‚æ¥æºã€æ ‡ç­¾ç­‰

# 6. ç®€åŒ–åçš„APIç«¯ç‚¹
@kb_router.post("/api/knowledge-base", response_model=KnowledgeBaseResponse)
async def manage_knowledge_base(request: KnowledgeBaseRequest):
    """ç»Ÿä¸€çš„çŸ¥è¯†åº“ç®¡ç†ç«¯ç‚¹ï¼Œåªè¿”å›å½“å‰çŸ¥è¯†åº“ä¿¡æ¯"""
    global qdrant_client
    try:
        if request.action == KnowledgeBaseAction.CREATE:
            # æ£€æŸ¥é›†åˆæ˜¯å¦å·²å­˜åœ¨
            if qdrant_client is None:
                qdrant_client = QdrantClient(host="localhost", port=6333)
            try:
                # å°è¯•è·å–é›†åˆï¼Œå¦‚æœå­˜åœ¨åˆ™è¿”å›exists
                qdrant_client.get_collection(request.name)
                status = "exists"
                message = f"çŸ¥è¯†åº“ '{request.name}' å·²å­˜åœ¨"
            except:
                # é›†åˆä¸å­˜åœ¨ï¼Œåˆ›å»ºå®ƒ
                try:
                    qdrant_client.create_collection(
                        collection_name=request.name,
                        vectors_config={
                            "title": VectorParams(size=1024, distance=Distance.COSINE),
                            "content": VectorParams(size=1024, distance=Distance.COSINE)
                        }
                    )
                    status = "created"
                    message = f"çŸ¥è¯†åº“ '{request.name}' åˆ›å»ºæˆåŠŸ"
                except Exception as e:
                    logger.error(f"åˆ›å»ºçŸ¥è¯†åº“å¤±è´¥: {str(e)}")
                    return KnowledgeBaseResponse(
                        success=False,
                        message=f"åˆ›å»ºçŸ¥è¯†åº“å¤±è´¥: {str(e)}",
                        data={"name": request.name}
                    )
            # ä»…è·å–å½“å‰çŸ¥è¯†åº“çš„ä¿¡æ¯
            current_kb = get_current_knowledge_base_info(request.name)
            return KnowledgeBaseResponse(
                success=True,
                message=message,
                data={
                    "name": request.name,
                    "status": status,
                    "document_count": current_kb["document_count"],
                    "points_count": current_kb["points_count"],
                    "documents": current_kb["documents"]
                }
            )
        elif request.action == KnowledgeBaseAction.DELETE:
            try:
                if qdrant_client is None:
                    qdrant_client = QdrantClient(host="localhost", port=6333)
                qdrant_client.delete_collection(request.name)
                message = f"çŸ¥è¯†åº“ '{request.name}' å·²åˆ é™¤"
                return KnowledgeBaseResponse(
                    success=True,
                    message=message,
                    data={
                        "name": request.name,
                        "document_count": 0,
                        "points_count": 0,
                        "documents": []
                    }
                )
            except Exception as e:
                logger.error(f"åˆ é™¤çŸ¥è¯†åº“å¤±è´¥: {str(e)}")
                return KnowledgeBaseResponse(
                    success=False,
                    message=f"åˆ é™¤çŸ¥è¯†åº“å¤±è´¥: {str(e)}",
                    data={"name": request.name}
                )
        elif request.action == KnowledgeBaseAction.UPLOAD:
            if not request.document_name:
                return KnowledgeBaseResponse(
                    success=False,
                    message="ä¸Šä¼ æ–‡æ¡£éœ€è¦æä¾› document_name",
                    data={"name": request.name}
                )
            # ä¸‹è½½æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
            oss_path = f"knowledge-documents/{request.name}/{request.document_name}.pdf"
            try:
                # ç”Ÿæˆä¸´æ—¶æ–‡ä»¶å
                orginal_file_name = f"{request.document_name}.pdf"
                local_input = os.path.join(LOCAL_DIR, orginal_file_name)
                if not download_pdf_from_oss(oss_path, local_input):
                    return KnowledgeBaseResponse(
                        success=False,
                        message="ä¸‹è½½æ–‡ä»¶å¤±è´¥",
                        data={"name": request.name}
                    )
                
                # =============== æ˜¾å­˜ä¼˜åŒ–ï¼šä½¿ç”¨OCRæ¨¡å‹ ===============
                logger.info("å¼€å§‹ä½¿ç”¨OCRæ¨¡å‹å¤„ç†PDFæ–‡ä»¶...")
                await gpu_resource_manager.acquire("ocr")
                try:
                    # ä¸‹è½½æ–‡ä»¶
                    pdf2md_func = gpu_resource_manager.get_ocr_model()
                    pdf2md_func(local_input)
                finally:
                    await gpu_resource_manager.release()
                logger.info("OCRæ¨¡å‹å¤„ç†å®Œæˆï¼Œèµ„æºå·²é‡Šæ”¾")
                
                tempfile = f'/home/easyai/OCRFlux/localworkspace/markdowns/{request.document_name}/{request.document_name}.md'
                
                # =============== æ˜¾å­˜ä¼˜åŒ–ï¼šä½¿ç”¨å‘é‡åŒ–æ¨¡å‹ ===============
                logger.info("å¼€å§‹ä½¿ç”¨å‘é‡åŒ–æ¨¡å‹å¤„ç†æ–‡æ¡£...")
                await gpu_resource_manager.acquire("embedding")
                try:
                    vector_store = gpu_resource_manager.get_embedding_model()(collection_name=request.name)
                    operation_info = upsert_md_file(tempfile, vector_store)
                finally:
                    await gpu_resource_manager.release()
                logger.info("å‘é‡åŒ–æ¨¡å‹å¤„ç†å®Œæˆï¼Œèµ„æºå·²é‡Šæ”¾")
                
                # ä»…è·å–å½“å‰çŸ¥è¯†åº“çš„ä¿¡æ¯
                current_kb = get_current_knowledge_base_info(request.name)
                return KnowledgeBaseResponse(
                    success=True,
                    message=f"æ–‡æ¡£ '{request.document_name}' å·²ä¸Šä¼ åˆ°çŸ¥è¯†åº“ '{request.name}'",
                    data={
                        "name": request.name,
                        "document": request.document_name,
                        "details": str(operation_info),
                        "document_count": current_kb["document_count"],
                        "points_count": current_kb["points_count"],
                        "documents": current_kb["documents"]
                    }
                )
            finally:
                if os.path.exists(local_input):
                    os.remove(local_input)
                # é¢å¤–æ¸…ç†ç¡®ä¿é‡Šæ”¾æ‰€æœ‰èµ„æº
                gpu_resource_manager.clear_gpu_memory()
        elif request.action == KnowledgeBaseAction.DELETE_DOCUMENT:
            if not request.document_name:
                return KnowledgeBaseResponse(
                    success=False,
                    message="åˆ é™¤æ–‡æ¡£éœ€è¦æä¾› document_name",
                    data={"name": request.name}
                )
            
            # ç›´æ¥è°ƒç”¨æ‚¨å·²æœ‰çš„å‡½æ•°
            # ä¸å†è‡ªåŠ¨æ·»åŠ .mdåç¼€ï¼Œå› ä¸ºQAå¯¹å·²ç»åŒ…å«äº†.mdåç¼€
            deletename = request.document_name
            
            # =============== æ˜¾å­˜ä¼˜åŒ–ï¼šä½¿ç”¨å‘é‡åŒ–æ¨¡å‹ ===============
            logger.info("å¼€å§‹ä½¿ç”¨å‘é‡åŒ–æ¨¡å‹åˆ é™¤æ–‡æ¡£...")
            await gpu_resource_manager.acquire("embedding")
            try:
                vector_store = gpu_resource_manager.get_embedding_model()(collection_name=request.name)
                operation_info = delete_by_source(deletename, vector_store)
            finally:
                await gpu_resource_manager.release()
            logger.info("å‘é‡åŒ–æ¨¡å‹æ“ä½œå®Œæˆï¼Œèµ„æºå·²é‡Šæ”¾")
            
            # ä»…è·å–å½“å‰çŸ¥è¯†åº“çš„ä¿¡æ¯
            current_kb = get_current_knowledge_base_info(request.name)
            return KnowledgeBaseResponse(
                success=True,
                message=f"æ–‡æ¡£ '{request.document_name}' å·²ä»çŸ¥è¯†åº“ '{request.name}' ä¸­åˆ é™¤",
                data={
                    "name": request.name,
                    "document": request.document_name,
                    "details": str(operation_info),
                    "document_count": current_kb["document_count"],
                    "points_count": current_kb["points_count"],
                    "documents": current_kb["documents"]
                }
            )
    except Exception as e:
        logger.error(f"å¤„ç†è¯·æ±‚å¤±è´¥: {str(e)}", exc_info=True)
        return KnowledgeBaseResponse(
            success=False,
            message=f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}",
            data={"action": request.action, "name": request.name}
        )

# 7. ç®€åŒ–å…¶ä»–ç«¯ç‚¹
@kb_router.get("/api/knowledge-bases", response_model=KnowledgeBaseResponse)
async def list_knowledge_bases():
    """åˆ—å‡ºæ‰€æœ‰çŸ¥è¯†åº“ - ä¿æŒä¸å˜ï¼Œå› ä¸ºè¿™æ˜¯å¦ä¸€ä¸ªåŠŸèƒ½"""
    try:
        global qdrant_client
        if qdrant_client is None:
            qdrant_client = QdrantClient(host="localhost", port=6333)
        collections = qdrant_client.get_collections().collections
        kb_list = []
        for collection in collections:
            kb_info = get_current_knowledge_base_info(collection.name)
            kb_list.append({
                "name": collection.name,
                "points_count": kb_info["points_count"],
                "document_count": kb_info["document_count"]
            })
        return KnowledgeBaseResponse(
            success=True,
            message="çŸ¥è¯†åº“åˆ—è¡¨è·å–æˆåŠŸ",
            data={
                "knowledge_bases": kb_list,
                "total": len(kb_list)
            }
        )
    except Exception as e:
        logger.error(f"è·å–çŸ¥è¯†åº“åˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)
        return KnowledgeBaseResponse(
            success=False,
            message=f"è·å–çŸ¥è¯†åº“åˆ—è¡¨å¤±è´¥: {str(e)}",
            data={}
        )

@kb_router.get("/api/knowledge-base/{kb_name}/documents", response_model=KnowledgeBaseResponse)
async def list_documents(kb_name: str):
    """åˆ—å‡ºçŸ¥è¯†åº“ä¸­çš„æ–‡æ¡£ - ä¿æŒä¸å˜"""
    try:
        # ä»…è·å–å½“å‰çŸ¥è¯†åº“çš„ä¿¡æ¯
        current_kb = get_current_knowledge_base_info(kb_name)
        if not current_kb["exists"]:
            return KnowledgeBaseResponse(
                success=False,
                message=f"çŸ¥è¯†åº“ '{kb_name}' ä¸å­˜åœ¨",
                data={"name": kb_name}
            )
        return KnowledgeBaseResponse(
            success=True,
            message=f"çŸ¥è¯†åº“ '{kb_name}' ä¸­çš„æ–‡æ¡£åˆ—è¡¨",
            data={
                "knowledge_base": kb_name,
                "documents": current_kb["documents"],
                "total": current_kb["document_count"],
                "points_count": current_kb["points_count"]
            }
        )
    except Exception as e:
        logger.error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)
        return KnowledgeBaseResponse(
            success=False,
            message=f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}",
            data={"name": kb_name}
        )

# æ–°å¢ï¼šé—®ç­”å¯¹ä¸Šä¼ APIç«¯ç‚¹
@kb_router.post("/api/qa-pair", response_model=KnowledgeBaseResponse)
async def upload_qa_pair(request: QAPairRequest):
    """ä¸Šä¼ é—®ç­”å¯¹åˆ°æŒ‡å®šçŸ¥è¯†åº“"""
    try:
        global qdrant_client
        # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
        if qdrant_client is None:
            qdrant_client = QdrantClient(host="localhost", port=6333)
        
        try:
            qdrant_client.get_collection(request.knowledge_base_name)
        except:
            return KnowledgeBaseResponse(
                success=False,
                message=f"çŸ¥è¯†åº“ '{request.knowledge_base_name}' ä¸å­˜åœ¨",
                data={"name": request.knowledge_base_name}
            )
        
        # =============== æ˜¾å­˜ä¼˜åŒ–ï¼šä½¿ç”¨å‘é‡åŒ–æ¨¡å‹ ===============
        logger.info("å¼€å§‹ä½¿ç”¨å‘é‡åŒ–æ¨¡å‹å¤„ç†é—®ç­”å¯¹...")
        await gpu_resource_manager.acquire("embedding")
        try:
            vector_store = gpu_resource_manager.get_embedding_model()(collection_name=request.knowledge_base_name)
            
            # æ„å»ºé—®ç­”å¯¹çš„æ–‡æœ¬å†…å®¹
            qa_content = f"é—®é¢˜ï¼š{request.question}\n\nç­”æ¡ˆï¼š{request.answer}"
            
            # æ„å»ºå…ƒæ•°æ®
            metadata = {
                "source": "qa_pair",  # ä¿ç•™åŸæœ‰æ ‡è¯†ï¼Œä½†ä¼šè¢«upsert_qa_pairå‡½æ•°è¦†ç›–
                "type": "qa",
                "question": request.question,
                "answer": request.answer,
                "created_at": str(datetime.datetime.now())
            }
            # å¦‚æœæä¾›äº†æ–‡æ¡£åï¼Œæ·»åŠ åˆ°metadataä¸­
            if request.document_name:
                metadata["document_name"] = request.document_name
            if request.metadata:
                metadata.update(request.metadata)
            
            # ä½¿ç”¨ç°æœ‰çš„upsert_md_fileå‡½æ•°ï¼Œä½†ä¼ å…¥è‡ªå®šä¹‰å†…å®¹
            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä¿®æ”¹upsert_md_fileå‡½æ•°ä»¥æ”¯æŒè‡ªå®šä¹‰å†…å®¹ï¼Œæˆ–è€…åˆ›å»ºæ–°çš„å‡½æ•°
            operation_info = upsert_qa_pair(qa_content, metadata, vector_store)
            
        finally:
            await gpu_resource_manager.release()
        logger.info("å‘é‡åŒ–æ¨¡å‹å¤„ç†å®Œæˆï¼Œèµ„æºå·²é‡Šæ”¾")
        
        # è·å–æ›´æ–°åçš„çŸ¥è¯†åº“ä¿¡æ¯
        current_kb = get_current_knowledge_base_info(request.knowledge_base_name)
        
        return KnowledgeBaseResponse(
            success=True,
            message=f"é—®ç­”å¯¹å·²æˆåŠŸæ·»åŠ åˆ°çŸ¥è¯†åº“ '{request.knowledge_base_name}'",
            data={
                "name": request.knowledge_base_name,
                "qa_pair": {
                    "question": request.question,
                    "answer": request.answer
                },
                "details": str(operation_info),
                "document_count": current_kb["document_count"],
                "points_count": current_kb["points_count"],
                "documents": current_kb["documents"]
            }
        )
        
    except Exception as e:
        logger.error(f"ä¸Šä¼ é—®ç­”å¯¹å¤±è´¥: {str(e)}", exc_info=True)
        return KnowledgeBaseResponse(
            success=False,
            message=f"ä¸Šä¼ é—®ç­”å¯¹å¤±è´¥: {str(e)}",
            data={"name": request.knowledge_base_name}
        )

# æ–°å¢ï¼šæ‰¹é‡é—®ç­”å¯¹ä¸Šä¼ APIç«¯ç‚¹
@kb_router.post("/api/qa-pairs/batch", response_model=KnowledgeBaseResponse)
async def upload_qa_pairs_batch(request: List[QAPairRequest]):
    """æ‰¹é‡ä¸Šä¼ é—®ç­”å¯¹åˆ°æŒ‡å®šçŸ¥è¯†åº“"""
    try:
        global qdrant_client
        if not request:
            return KnowledgeBaseResponse(
                success=False,
                message="è¯·æ±‚åˆ—è¡¨ä¸ºç©º",
                data={}
            )
        
        # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªè¯·æ±‚çš„çŸ¥è¯†åº“åç§°ï¼‰
        knowledge_base_name = request[0].knowledge_base_name
        if qdrant_client is None:
            qdrant_client = QdrantClient(host="localhost", port=6333)
        
        try:
            qdrant_client.get_collection(knowledge_base_name)
        except:
            return KnowledgeBaseResponse(
                success=False,
                message=f"çŸ¥è¯†åº“ '{knowledge_base_name}' ä¸å­˜åœ¨",
                data={"name": knowledge_base_name}
            )
        
        # =============== æ˜¾å­˜ä¼˜åŒ–ï¼šä½¿ç”¨å‘é‡åŒ–æ¨¡å‹ ===============
        logger.info(f"å¼€å§‹ä½¿ç”¨å‘é‡åŒ–æ¨¡å‹æ‰¹é‡å¤„ç† {len(request)} ä¸ªé—®ç­”å¯¹...")
        await gpu_resource_manager.acquire("embedding")
        try:
            vector_store = gpu_resource_manager.get_embedding_model()(collection_name=knowledge_base_name)
            
            success_count = 0
            failed_count = 0
            failed_items = []
            
            for qa_request in request:
                try:
                    # æ„å»ºé—®ç­”å¯¹çš„æ–‡æœ¬å†…å®¹
                    qa_content = f"é—®é¢˜ï¼š{qa_request.question}\n\nç­”æ¡ˆï¼š{qa_request.answer}"
                    
                    # æ„å»ºå…ƒæ•°æ®
                    metadata = {
                        "source": "qa_pair",  # ä¿ç•™åŸæœ‰æ ‡è¯†ï¼Œä½†ä¼šè¢«upsert_qa_pairå‡½æ•°è¦†ç›–
                        "type": "qa",
                        "question": qa_request.question,
                        "answer": qa_request.answer,
                        "created_at": str(datetime.datetime.now())
                    }
                    # å¦‚æœæä¾›äº†æ–‡æ¡£åï¼Œæ·»åŠ åˆ°metadataä¸­
                    if qa_request.document_name:
                        metadata["document_name"] = qa_request.document_name
                    if qa_request.metadata:
                        metadata.update(qa_request.metadata)
                    
                    # ä¸Šä¼ é—®ç­”å¯¹
                    operation_info = upsert_qa_pair(qa_content, metadata, vector_store)
                    success_count += 1
                    
                except Exception as e:
                    failed_count += 1
                    failed_items.append({
                        "question": qa_request.question[:50] + "..." if len(qa_request.question) > 50 else qa_request.question,
                        "error": str(e)
                    })
                    logger.error(f"å¤„ç†é—®ç­”å¯¹å¤±è´¥: {str(e)}")
            
        finally:
            await gpu_resource_manager.release()
        logger.info("å‘é‡åŒ–æ¨¡å‹æ‰¹é‡å¤„ç†å®Œæˆï¼Œèµ„æºå·²é‡Šæ”¾")
        
        # è·å–æ›´æ–°åçš„çŸ¥è¯†åº“ä¿¡æ¯
        current_kb = get_current_knowledge_base_info(knowledge_base_name)
        
        return KnowledgeBaseResponse(
            success=True,
            message=f"æ‰¹é‡ä¸Šä¼ å®Œæˆï¼šæˆåŠŸ {success_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ª",
            data={
                "name": knowledge_base_name,
                "total_requested": len(request),
                "success_count": success_count,
                "failed_count": failed_count,
                "failed_items": failed_items if failed_items else None,
                "document_count": current_kb["document_count"],
                "points_count": current_kb["points_count"],
                "documents": current_kb["documents"]
            }
        )
        
    except Exception as e:
        logger.error(f"æ‰¹é‡ä¸Šä¼ é—®ç­”å¯¹å¤±è´¥: {str(e)}", exc_info=True)
        return KnowledgeBaseResponse(
            success=False,
            message=f"æ‰¹é‡ä¸Šä¼ é—®ç­”å¯¹å¤±è´¥: {str(e)}",
            data={}
        )

# ======================
# å¯¹è¯Agent APIè·¯ç”±
# ======================
# é‡è¦ä¿®å¤è¯´æ˜ï¼š
# ä¿®å¤äº†åŒé‡ç´¯ç§¯é—®é¢˜ï¼Œç°åœ¨å®Œå…¨ä¾èµ– LangGraph çš„ Annotated[list, add] è‡ªåŠ¨ç´¯ç§¯æ¶ˆæ¯
# é¿å…äº†æ‰‹åŠ¨çŠ¶æ€æ„å»ºæ—¶é‡å¤æ·»åŠ æ¶ˆæ¯å¯¼è‡´çš„å†…å­˜æµªè´¹å’Œæ€§èƒ½ä¸‹é™
# ä¿®å¤å‰ï¼šmessages = state.values["messages"] + [æ–°æ¶ˆæ¯] (æ‰‹åŠ¨ç´¯ç§¯)
# ä¿®å¤åï¼šmessages = [æ–°æ¶ˆæ¯] (è®© LangGraph è‡ªåŠ¨ç´¯ç§¯)
# ======================
# 3. å®šä¹‰çŠ¶æ€
class AgentState(TypedDict):
    messages: Annotated[list, add]  # æ¶ˆæ¯å†å²ï¼Œä½¿ç”¨ LangGraph çš„ add æ“ä½œç¬¦è‡ªåŠ¨ç´¯ç§¯
    tool_call_count: Annotated[int, operator.add]  # æ·»åŠ å·¥å…·è°ƒç”¨è®¡æ•°å™¨
    knowledge_base_name: str  # æ–°å¢å­—æ®µï¼Œç”¨äºå­˜å‚¨å½“å‰ä¼šè¯ä½¿ç”¨çš„çŸ¥è¯†åº“åç§°
    user_document_tools: List[str]  # æ–°å¢å­—æ®µï¼Œç”¨äºå­˜å‚¨å½“å‰ä¼šè¯å¯ç”¨çš„ç”¨æˆ·æ–‡æ¡£å·¥å…·åç§°
    web_search_enabled: bool  # æ–°å¢ï¼šè®°å½•webæœç´¢æ˜¯å¦å¯ç”¨

# 4. ä¿®æ”¹æ¨¡å‹è°ƒç”¨èŠ‚ç‚¹
async def call_model(state: AgentState):
    """æ¨¡å‹è‡ªä¸»å†³ç­–æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·ï¼ŒåŒ…å«å‚æ•°éªŒè¯å’ŒçŠ¶æ€æ›´æ–°"""
    messages = state["messages"]
    knowledge_base_name = state.get("knowledge_base_name", "test")
    
    # =============== æ–°å¢ï¼šè¯¦ç»†æ—¥å¿—è®°å½• ===============
    # ä»æ¶ˆæ¯ä¸­æå–sessionä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    session_info = ""
    if messages and isinstance(messages[-1], HumanMessage):
        # å°è¯•ä»æ¶ˆæ¯å†…å®¹ä¸­æå–sessionä¿¡æ¯
        last_message = messages[-1].content
        if "session_id:" in str(last_message):
            session_info = f" - åŸºäºç”¨æˆ·æ¶ˆæ¯"
        chat_logger.info(f"ğŸ§  æ¨¡å‹å¼€å§‹æ€è€ƒ")
        chat_logger.info(f"  ğŸ“Š æ¶ˆæ¯æ•°é‡: {len(messages)}")
        chat_logger.info(f"  ğŸ”§ çŸ¥è¯†åº“: {knowledge_base_name}")
        chat_logger.info(f"  ğŸ’­ ç”¨æˆ·é—®é¢˜: {last_message[:100]}{'...' if len(last_message) > 100 else ''}")
        if session_info:
            chat_logger.info(f"  â„¹ï¸ {session_info}")
    # =============== æ–°å¢ç»“æŸ ===============
    
    # æ˜¾å­˜ä¼˜åŒ–ï¼šé™åˆ¶ä¼šè¯é•¿åº¦ï¼Œé˜²æ­¢æ˜¾å­˜ç´¯ç§¯
    if len(messages) > 10:  # é™åˆ¶ä¼šè¯å†å²é•¿åº¦
        # ä¿ç•™æœ€æ–°çš„5æ¡æ¶ˆæ¯å’Œç³»ç»Ÿæç¤º
        messages = messages[-5:]
        logger.info("âš ï¸ ä¼šè¯å†å²è¿‡é•¿ï¼Œå·²æˆªæ–­ä»¥èŠ‚çœæ˜¾å­˜")
        chat_logger.info(f"âš ï¸ ä¼šè¯å†å²è¿‡é•¿ï¼Œå·²æˆªæ–­è‡³ {len(messages)} æ¡æ¶ˆæ¯")
    
    # åŠ¨æ€è·å–å½“å‰çŸ¥è¯†åº“çš„RAGå·¥å…·
    rag_tool = get_rag_tool(knowledge_base_name)
    available_tools = [rag_tool]
    
    if state.get("web_search_enabled", True):
        global web_search_tool
        if web_search_tool is None:
            web_search_tool = create_search_tool()
        available_tools.append(web_search_tool)
    
    # æ·»åŠ ç”¨æˆ·æ–‡æ¡£å·¥å…·ï¼ˆå¦‚æœæœ‰ï¼‰
    user_document_tools_list = state.get("user_document_tools", [])
    for tool_name in user_document_tools_list:
        tool_info = get_user_document_tool(tool_name)
        if tool_info and "tool" in tool_info:
            available_tools.append(tool_info["tool"])
    
    chat_logger.info(f"ğŸ”§ å¯ç”¨å·¥å…·: {[tool.name if hasattr(tool, 'name') else str(tool) for tool in available_tools]}")
    
    # =============== æ˜¾å­˜ä¼˜åŒ–ï¼šè·å–Ollamaæ¨¡å‹ ===============
    logger.info("å‡†å¤‡ä½¿ç”¨Ollamaæ¨¡å‹å¤„ç†å¯¹è¯...")
    chat_logger.info(f"ğŸ¤– è·å–Ollamaæ¨¡å‹...")
    await gpu_resource_manager.acquire("ollama")
    try:
        # å¦‚æœæ˜¯åˆå§‹æŸ¥è¯¢ï¼Œæ·»åŠ ç³»ç»Ÿæç¤º
        if len(messages) == 1 and isinstance(messages[0], HumanMessage):
            chat_logger.info(f"ğŸ†• é¦–æ¬¡æŸ¥è¯¢ï¼Œæ·»åŠ ç³»ç»Ÿæç¤º")
            chat_logger.info(f"ğŸ“š å½“å‰çŸ¥è¯†åº“: {knowledge_base_name}")
            chat_logger.info(f"ğŸŒ Webæœç´¢çŠ¶æ€: {'å¯ç”¨' if state.get('web_search_enabled', True) else 'ç¦ç”¨'}")
            chat_logger.info(f"ğŸ“„ ç”¨æˆ·æ–‡æ¡£å·¥å…·æ•°é‡: {len(user_document_tools_list)}")
            
            # æ„å»ºå·¥å…·åˆ—è¡¨æè¿°
            tools_description = f"""1. rag_knowledge_search: æŸ¥è¯¢å†…éƒ¨çŸ¥è¯†åº“ï¼ˆä¸»è¦åŒ…å«åŒ»å­¦ç›¸å…³å†…å®¹ï¼‰
            - å¿…é¡»å‚æ•°: query (string)
            - å½“å‰çŸ¥è¯†åº“: {knowledge_base_name}
            - è°ƒç”¨ç¤ºä¾‹: {{"name": "rag_knowledge_search", "arguments": {{"query": "NGSé€‚ç”¨äººç¾¤"}}}}
            - ç‰¹åˆ«è¯´æ˜: çŸ¥è¯†åº“åŒ…å«ä¸¤ç§ç±»å‹çš„å†…å®¹ï¼š
              * QAå¯¹çŸ¥è¯†åº“: ä»¥"é—®é¢˜ï¼š...ç­”æ¡ˆï¼š..."æ ¼å¼è¿”å›å®Œæ•´é—®ç­”å¯¹ï¼Œé—®é¢˜æƒé‡æ›´é«˜
              * æ–‡æ¡£ç‰‡æ®µ: è¿”å›ç›¸å…³æ–‡æ¡£å†…å®¹ç‰‡æ®µ
            - å½“æ£€ç´¢åˆ°QAå¯¹æ—¶ï¼Œç³»ç»Ÿä¼šä¼˜å…ˆè¿”å›é—®é¢˜åŒ¹é…åº¦é«˜çš„ç»“æœï¼Œå¹¶æ ‡è®°ä¸º"QAå¯¹çŸ¥è¯†åº“"
            2. web_search_tool: æŸ¥è¯¢æœ€æ–°äº’è”ç½‘ä¿¡æ¯
            - å¿…é¡»å‚æ•°: query (string)
            - è°ƒç”¨ç¤ºä¾‹: {{"name": "web_search_tool", "arguments": {{"query": "NGSæœ€æ–°ç ”ç©¶"}}}}"""
            # æ·»åŠ ç”¨æˆ·æ–‡æ¡£å·¥å…·æè¿°
            if user_document_tools_list:
                tools_description += "\nç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£æœç´¢å·¥å…·:"
                for tool_name in user_document_tools_list:
                    tool_info = get_user_document_tool(tool_name)
                    if tool_info:
                        tools_description += f"\n{tool_info['tool'].name}: {tool_info['tool'].description}"
                        tools_description += "\n   - å¿…é¡»å‚æ•°: query (string)"
            
            # æ·»åŠ QAå¯¹è¯´æ˜
            tools_description += f"""
            3. çŸ¥è¯†åº“å†…å®¹è¯´æ˜:
            - çŸ¥è¯†åº“ '{knowledge_base_name}' åŒ…å«PDFæ–‡æ¡£å’Œé—®ç­”å¯¹
            - PDFæ–‡æ¡£: æŒ‰åŸå§‹æ–‡ä»¶å.mdå­˜å‚¨
            - é—®ç­”å¯¹: æŒ‰ç”¨æˆ·æŒ‡å®šçš„æ–‡æ¡£å.mdå­˜å‚¨ï¼ˆå¦‚test.mdï¼‰
            - æ‰€æœ‰å†…å®¹éƒ½å¯é€šè¿‡rag_knowledge_searchç»Ÿä¸€æœç´¢"""
            
            # æ„å»ºç³»ç»Ÿæç¤º
            if not state.get("web_search_enabled", True):
                system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªå¤§å‹ç²’å­åŠ é€Ÿå™¨ä¸“å®¶ï¼Œä¸“é—¨å›ç­”å…³äºç²’å­åŠ é€Ÿå™¨çš„æŠ€æœ¯é—®é¢˜ã€‚

å¯ç”¨å·¥å…·ï¼š
{tools_description}

å·¥ä½œæµç¨‹ï¼š
1. å¯¹äºæ‰€æœ‰ç²’å­åŠ é€Ÿå™¨ç›¸å…³é—®é¢˜ï¼Œå¿…é¡»é¦–å…ˆä½¿ç”¨ rag_knowledge_search æœç´¢çŸ¥è¯†åº“
2. æ£€æŸ¥è¿”å›ç»“æœçš„æœ€é«˜ç›¸ä¼¼åº¦ï¼š
   * å¦‚æœæœ€é«˜ç›¸ä¼¼åº¦ â‰¥ 0.6ï¼Œç»“æœç›¸å…³ï¼ŒåŸºäºæ­¤ç”Ÿæˆå›ç­”
   * å¦‚æœæœ€é«˜ç›¸ä¼¼åº¦ < 0.6ï¼Œç»“æœä¸ç›¸å…³ï¼Œæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·çŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯
3. ä¸¥æ ¼åŸºäºçŸ¥è¯†åº“å†…å®¹å›ç­”ï¼Œä¸å¾—ç¼–é€ æˆ–æ¨æµ‹ä¿¡æ¯

é‡è¦æŒ‡å¯¼åŸåˆ™:
1. å›ç­”å¿…é¡»ä¸¥æ ¼åŸºäºçŸ¥è¯†åº“å†…å®¹ï¼Œä¸å¾—ç¼–é€ ä¿¡æ¯
2. å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·"çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
3. æä¾›ä¿¡æ¯æ—¶è¦æ³¨æ˜æ¥æºï¼ˆæ¥è‡ªçŸ¥è¯†åº“ï¼‰
4. å¯¹äºæŠ€æœ¯å»ºè®®ï¼Œå¿…é¡»åŸºäºçŸ¥è¯†åº“ä¸­çš„æƒå¨èµ„æ–™
5. å¦‚æœçŸ¥è¯†åº“æœç´¢ç»“æœç›¸ä¼¼åº¦ < 0.6ï¼Œä¸å¾—åŸºäºä½ç›¸ä¼¼åº¦ç»“æœç”Ÿæˆå›ç­”
6. ä¸å¾—ä½¿ç”¨ç½‘ç»œæœç´¢æˆ–å…¶ä»–å¤–éƒ¨ä¿¡æ¯æº
7. å›ç­”è¦ä¸“ä¸šã€å‡†ç¡®ã€ä¸¥è°¨
8. å¦‚æœå·²ç»è°ƒç”¨å·¥å…·è¶…è¿‡3æ¬¡ä»æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·çŸ¥è¯†åº“ä¸­æ— ç›¸å…³ä¿¡æ¯
å·¥å…·è°ƒç”¨æ ¼å¼è¦æ±‚:
- ä»…ä½¿ç”¨æŒ‡å®šçš„å·¥å…·åç§°
- ä»…ä¼ é€’å·¥å…·å®šä¹‰ä¸­è¦æ±‚çš„å‚æ•°
- ç»å¯¹ä¸è¦æ·»åŠ é¢å¤–å‚æ•°ï¼ˆå¦‚"using"ã€"reason"ç­‰ï¼‰
- ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºå·¥å…·è°ƒç”¨
- ä¾‹å¦‚: {{"name": "rag_knowledge_search", "arguments": {{"query": "ä½ çš„æŸ¥è¯¢"}}}}
- é‡è¦: ä¸è¦åœ¨å·¥å…·è°ƒç”¨ä¸­åŒ…å«ä»»ä½•é¢å¤–æ–‡æœ¬ã€è§£é‡Šæˆ–<think>æ ‡ç­¾
- å·¥å…·è°ƒç”¨å¿…é¡»æ˜¯çº¯JSONæ ¼å¼ï¼Œä¸èƒ½æœ‰å…¶ä»–å†…å®¹
- é”™è¯¯ç¤ºä¾‹: {{"name": "rag_knowledge_search", "arguments": {{"query": "...", "using": "..."}}}}"""
                
                chat_logger.info(f"ğŸ”§ ç”Ÿæˆç³»ç»Ÿæç¤º (Webæœç´¢ç¦ç”¨):")
                chat_logger.info(f"   ğŸ“ æç¤ºé•¿åº¦: {len(system_prompt)} å­—ç¬¦")
                chat_logger.info(f"   ğŸ“‹ æç¤ºå†…å®¹: {system_prompt}")
            else:
                system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªåŒ»ç–—æ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·ï¼š
{tools_description}
å·¥ä½œæµç¨‹ï¼š
- å¦‚æœç”¨æˆ·è¯¢é—®ä¸å…¶ä¸Šä¼ æ–‡æ¡£ç›¸å…³çš„å†…å®¹ï¼Œä¼˜å…ˆä½¿ç”¨å¯¹åº”çš„æ–‡æ¡£æœç´¢å·¥å…·
- å¯¹äºä¸€èˆ¬åŒ»å­¦é—®é¢˜ï¼Œé¦–å…ˆå°è¯•ä½¿ç”¨ rag_knowledge_search
- æ£€æŸ¥è¿”å›ç»“æœçš„æœ€é«˜ç›¸ä¼¼åº¦ï¼š
  * å¦‚æœæœ€é«˜ç›¸ä¼¼åº¦ â‰¥ 0.5ï¼Œç»“æœå¯èƒ½ç›¸å…³ï¼Œå¯åŸºäºæ­¤ç”Ÿæˆå›ç­”
  * å¦‚æœæœ€é«˜ç›¸ä¼¼åº¦ < 0.5ï¼Œç»“æœä¸ç›¸å…³ï¼Œåº”ä½¿ç”¨ web_search_tool
- ç‰¹åˆ«è¯´æ˜ï¼šå½“æ£€ç´¢åˆ°QAå¯¹çŸ¥è¯†åº“å†…å®¹æ—¶ï¼š
  * QAå¯¹çš„é—®é¢˜éƒ¨åˆ†æƒé‡æ›´é«˜ï¼Œé—®é¢˜åŒ¹é…åº¦é«˜çš„ç»“æœä¼šä¼˜å…ˆè¿”å›
  * ç³»ç»Ÿä¼šè¿”å›å®Œæ•´çš„é—®ç­”å¯¹å†…å®¹ï¼Œæ ¼å¼ä¸º"é—®é¢˜ï¼š...ç­”æ¡ˆï¼š..."
  * è¿™äº›QAå¯¹é€šå¸¸æ›´å‡†ç¡®ã€æ›´ç›´æ¥åœ°å›ç­”ç”¨æˆ·é—®é¢˜
  * å¦‚æœQAå¯¹çŸ¥è¯†åº“æœ‰ç›¸å…³å†…å®¹ï¼Œåº”ä¼˜å…ˆåŸºäºQAå¯¹ç”Ÿæˆå›ç­”
ç¡®ä¿æœ€ç»ˆå›ç­”æ•´åˆæ‰€æœ‰å¯ç”¨ä¿¡æ¯
é‡è¦æŒ‡å¯¼åŸåˆ™:
1. ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£å·¥å…·ï¼ˆå¦‚æœé—®é¢˜ä¸æ–‡æ¡£å†…å®¹ç›¸å…³ï¼‰
2. å…¶æ¬¡ä½¿ç”¨æœ¬åœ°çŸ¥è¯†åº“å·¥å…·(rag_knowledge_search)
3. å½“æœ¬åœ°çŸ¥è¯†åº“æœç´¢ç»“æœçš„æœ€é«˜ç›¸ä¼¼åº¦ < 0.5 æ—¶ï¼Œå¿…é¡»ä½¿ç”¨ç½‘ç»œæœç´¢å·¥å…·(web_search_tool),ä¸å¾—ç›´æ¥ç”Ÿæˆå›ç­”
4. å›ç­”å¿…é¡»åŸºäºè¯æ®ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
5. æä¾›ä¿¡æ¯æ—¶è¦æ³¨æ˜æ¥æºï¼ˆæ¥è‡ªç”¨æˆ·æ–‡æ¡£ã€æœ¬åœ°çŸ¥è¯†åº“æˆ–ç½‘ç»œæœç´¢ï¼‰
6. å¯¹äºåŒ»å­¦å»ºè®®ï¼Œå¿…é¡»æé†’ç”¨æˆ·å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿ
7. å¦‚æœç½‘ç»œæœç´¢è¿”å›äº†æ˜ç¡®çš„ç­”æ¡ˆï¼Œåˆ™ç›´æ¥ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
8. å¦‚æœå·²ç»è°ƒç”¨å·¥å…·è¶…è¿‡5æ¬¡ä»æœªè§£å†³é—®é¢˜ï¼Œè¯·åŸºäºå·²æœ‰ä¿¡æ¯æä¾›æœ€ä½³ç­”æ¡ˆ
9. å½“ç”¨æˆ·æåˆ°"æˆ‘çš„æŠ¥å‘Š"ã€"æˆ‘çš„åŸºå› æ£€æµ‹"ç­‰ç±»ä¼¼è¡¨è¿°æ—¶ï¼Œä¼˜å…ˆä½¿ç”¨ç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£å·¥å…·
10. å¦‚æœæœ€åå·¥å…·è°ƒç”¨æ²¡æœ‰ç»“æœè¿”å›ï¼Œåˆ™ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜
å·¥å…·è°ƒç”¨æ ¼å¼è¦æ±‚:
- ä»…ä½¿ç”¨æŒ‡å®šçš„å·¥å…·åç§°
- ä»…ä¼ é€’å·¥å…·å®šä¹‰ä¸­è¦æ±‚çš„å‚æ•°
- ç»å¯¹ä¸è¦æ·»åŠ é¢å¤–å‚æ•°ï¼ˆå¦‚"using"ã€"reason"ç­‰ï¼‰
- ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºå·¥å…·è°ƒç”¨
- ä¾‹å¦‚: {{"name": "web_search_tool", "arguments": {{"query": "ä½ çš„æŸ¥è¯¢"}}}}
- é‡è¦: ä¸è¦åœ¨å·¥å…·è°ƒç”¨ä¸­åŒ…å«ä»»ä½•é¢å¤–æ–‡æœ¬ã€è§£é‡Šæˆ–<think>æ ‡ç­¾
- å·¥å…·è°ƒç”¨å¿…é¡»æ˜¯çº¯JSONæ ¼å¼ï¼Œä¸èƒ½æœ‰å…¶ä»–å†…å®¹
- é”™è¯¯ç¤ºä¾‹: {{"name": "web_search_tool", "arguments": {{"query": "...", "using": "..."}}}}"""
                
                chat_logger.info(f"ğŸ”§ ç”Ÿæˆç³»ç»Ÿæç¤º (Webæœç´¢å¯ç”¨):")
                chat_logger.info(f"   ğŸ“ æç¤ºé•¿åº¦: {len(system_prompt)} å­—ç¬¦")
                chat_logger.info(f"   ğŸ“‹ æç¤ºå†…å®¹: {system_prompt}")
            
            messages = [SystemMessage(content=system_prompt)] + messages
            chat_logger.info(f"âœ… ç³»ç»Ÿæç¤ºå·²æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨")
            chat_logger.info(f"ğŸ“Š æ·»åŠ ç³»ç»Ÿæç¤ºåæ¶ˆæ¯æ€»æ•°: {len(messages)}")
        
        # æ£€æŸ¥å·¥å…·è°ƒç”¨æ¬¡æ•° - å¦‚æœè¶…è¿‡é™åˆ¶ï¼Œå¼ºåˆ¶æ¨¡å‹æä¾›ç­”æ¡ˆ
        current_tool_count = state.get("tool_call_count", 0)
        chat_logger.info(f"ğŸ” æ£€æŸ¥å·¥å…·è°ƒç”¨æ¬¡æ•°: å½“å‰ {current_tool_count}/5")
        
        if current_tool_count >= 5:
            warning_msg = "âš ï¸ é‡è¦æç¤ºï¼šæ‚¨å·²ç»è°ƒç”¨äº†å¤šæ¬¡å·¥å…·ä½†ä»æœªèƒ½æä¾›æœ€ç»ˆç­”æ¡ˆã€‚è¯·åŸºäºå·²æœ‰ä¿¡æ¯ç«‹å³æä¾›å®Œæ•´å›ç­”ï¼Œä¸è¦å†è°ƒç”¨å·¥å…·ã€‚"
            messages.append(SystemMessage(content=warning_msg))
            chat_logger.warning(f"âš ï¸ å·¥å…·è°ƒç”¨æ¬¡æ•°å·²è¾¾ä¸Šé™ï¼Œæ·»åŠ å¼ºåˆ¶å›ç­”æç¤º")
            chat_logger.info(f"ğŸ“ æ·»åŠ çš„æç¤ºå†…å®¹: {warning_msg}")
        
        # è®¡æ•°web_searchè°ƒç”¨æ¬¡æ•°
        web_search_count = sum(1 for m in messages
                            if isinstance(m, AIMessage) and
                            m.tool_calls and
                            any(tc["name"] == "web_search_tool" for tc in m.tool_calls))
        
        chat_logger.info(f"ğŸ” æ£€æŸ¥Webæœç´¢è°ƒç”¨æ¬¡æ•°: å½“å‰ {web_search_count}/5")
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»è°ƒç”¨è¿‡web_searchä½†ç»“æœä¸ç†æƒ³
        if web_search_count >= 5:
            warning_msg = "âš ï¸ é‡è¦æç¤ºï¼šæ‚¨å·²ç»å¤šæ¬¡ä½¿ç”¨ç½‘ç»œæœç´¢ä½†ä»æœªæä¾›æœ€ç»ˆç­”æ¡ˆã€‚è¯·åŸºäºå·²æœ‰ä¿¡æ¯ç«‹å³æä¾›å®Œæ•´å›ç­”ï¼Œä¸è¦å†è°ƒç”¨å·¥å…·ã€‚"
            messages.append(SystemMessage(content=warning_msg))
            chat_logger.warning(f"âš ï¸ Webæœç´¢è°ƒç”¨æ¬¡æ•°å·²è¾¾ä¸Šé™ï¼Œæ·»åŠ å¼ºåˆ¶å›ç­”æç¤º")
            chat_logger.info(f"ğŸ“ æ·»åŠ çš„æç¤ºå†…å®¹: {warning_msg}")
        
        # === å…³é”®æ–°å¢ï¼šæ£€æŸ¥ä¸Šä¸€æ¬¡å·¥å…·è°ƒç”¨ç»“æœçš„ç›¸ä¼¼åº¦ ===
        if len(messages) >= 3:
            last_tool_response = messages[-2]  # ä¸Šä¸€æ¡æ˜¯å·¥å…·å“åº”
            chat_logger.info(f"ğŸ” æ£€æŸ¥ä¸Šä¸€æ¬¡å·¥å…·è°ƒç”¨ç»“æœç›¸ä¼¼åº¦...")
            
            if isinstance(last_tool_response, ToolMessage) and last_tool_response.content:
                chat_logger.info(f"ğŸ“¤ ä¸Šä¸€æ¬¡å·¥å…·å“åº”: {last_tool_response.name}")
                chat_logger.info(f"ğŸ“„ å·¥å…·å“åº”å†…å®¹é•¿åº¦: {len(last_tool_response.content)} å­—ç¬¦")
                
                # ä»å·¥å…·å“åº”ä¸­æå–æœ€é«˜ç›¸ä¼¼åº¦
                highest_similarity = extract_highest_similarity(last_tool_response.content)
                chat_logger.info(f"ğŸ¯ æå–çš„æœ€é«˜ç›¸ä¼¼åº¦: {highest_similarity:.4f}")
                
                # å¦‚æœæœ€é«˜ç›¸ä¼¼åº¦ä½äºé˜ˆå€¼ï¼Œæ·»åŠ ç³»ç»Ÿæç¤ºå¼ºåˆ¶ä½¿ç”¨web_search_tool
                if highest_similarity < 0.5:
                    warning_msg = f"âš ï¸ é‡è¦æç¤ºï¼šæœ¬åœ°çŸ¥è¯†åº“æœç´¢ç»“æœç›¸å…³æ€§è¾ƒä½ï¼ˆæœ€é«˜ç›¸ä¼¼åº¦: {highest_similarity:.4f}ï¼‰ã€‚è¯·ä½¿ç”¨web_search_toolè·å–æœ€æ–°äº’è”ç½‘ä¿¡æ¯ã€‚"
                    messages.append(SystemMessage(content=warning_msg))
                    logger.warning(f"æ£€æµ‹åˆ°ä½ç›¸å…³æ€§ç»“æœ (ç›¸ä¼¼åº¦: {highest_similarity:.4f})ï¼Œå¼ºåˆ¶ä½¿ç”¨ç½‘ç»œæœç´¢")
                    chat_logger.warning(f"âš ï¸ æ£€æµ‹åˆ°ä½ç›¸å…³æ€§ç»“æœï¼Œæ·»åŠ å¼ºåˆ¶ç½‘ç»œæœç´¢æç¤º")
                    chat_logger.info(f"ğŸ“ æ·»åŠ çš„æç¤ºå†…å®¹: {warning_msg}")
                else:
                    chat_logger.info(f"âœ… ç›¸ä¼¼åº¦ {highest_similarity:.4f} è¾¾åˆ°é˜ˆå€¼ï¼Œæ— éœ€å¼ºåˆ¶ç½‘ç»œæœç´¢")
            else:
                chat_logger.info(f"â„¹ï¸ ä¸Šä¸€æ¬¡æ¶ˆæ¯ä¸æ˜¯å·¥å…·å“åº”æˆ–å†…å®¹ä¸ºç©º")
        else:
            chat_logger.info(f"â„¹ï¸ æ¶ˆæ¯æ•°é‡ä¸è¶³ï¼Œè·³è¿‡ç›¸ä¼¼åº¦æ£€æŸ¥")
        
        # å§‹ç»ˆç»‘å®šæ‰€æœ‰å¯ç”¨å·¥å…·
        chat_logger.info(f"ğŸ”§ å¼€å§‹ç»‘å®šå·¥å…·åˆ°æ¨¡å‹...")
        chat_logger.info(f"ğŸ“‹ å¯ç”¨å·¥å…·åˆ—è¡¨: {[tool.name if hasattr(tool, 'name') else str(tool) for tool in available_tools]}")
        
        model = gpu_resource_manager.get_ollama_model()
        chat_logger.info(f"ğŸ¤– è·å–åˆ°Ollamaæ¨¡å‹: {type(model).__name__}")
        
        model_with_tools = model.bind_tools(available_tools)
        chat_logger.info(f"âœ… å·¥å…·ç»‘å®šå®Œæˆ")
        
        # è°ƒç”¨æ¨¡å‹
        chat_logger.info(f"ğŸ¤– å¼€å§‹è°ƒç”¨Ollamaæ¨¡å‹...")
        chat_logger.info(f"ğŸ“ å‘é€ç»™æ¨¡å‹çš„æ¶ˆæ¯æ•°é‡: {len(messages)}")
        
        # è®°å½•å‘é€ç»™æ¨¡å‹çš„è¯¦ç»†æ¶ˆæ¯å†…å®¹
        for i, msg in enumerate(messages):
            if isinstance(msg, HumanMessage):
                chat_logger.info(f"  ğŸ“¤ æ¶ˆæ¯ {i+1} [ç”¨æˆ·]: {msg.content[:100]}{'...' if len(msg.content) > 100 else ''}")
            elif isinstance(msg, SystemMessage):
                chat_logger.info(f"  ğŸ“¤ æ¶ˆæ¯ {i+1} [ç³»ç»Ÿ]: {msg.content[:100]}{'...' if len(msg.content) > 100 else ''}")
            elif isinstance(msg, AIMessage):
                chat_logger.info(f"  ğŸ“¤ æ¶ˆæ¯ {i+1} [AI]: {msg.content[:100] if msg.content else 'æ— å†…å®¹'}{'...' if msg.content and len(msg.content) > 100 else ''}")
            elif isinstance(msg, ToolMessage):
                chat_logger.info(f"  ğŸ“¤ æ¶ˆæ¯ {i+1} [å·¥å…·]: {msg.name} - {msg.content[:100]}{'...' if len(msg.content) > 100 else ''}")
        
        response = await model_with_tools.ainvoke(messages)
        chat_logger.info(f"âœ… æ¨¡å‹å“åº”å®Œæˆ")
        
        # è®°å½•æ¨¡å‹å“åº”çš„å®Œæ•´å†…å®¹
        if hasattr(response, "content") and response.content:
            chat_logger.info(f"ğŸ’¬ æ¨¡å‹å›ç­”å®Œæ•´å†…å®¹:")
            chat_logger.info(f"   ğŸ“„ å›ç­”é•¿åº¦: {len(response.content)} å­—ç¬¦")
            chat_logger.info(f"   ğŸ“ å›ç­”å†…å®¹: {response.content}")
        else:
            chat_logger.info(f"âš ï¸ æ¨¡å‹å“åº”æ— å†…å®¹")
        
        # è®°å½•å·¥å…·è°ƒç”¨çš„è¯¦ç»†ä¿¡æ¯
        if hasattr(response, "tool_calls") and response.tool_calls:
            chat_logger.info(f"ğŸ”§ æ¨¡å‹å†³å®šè°ƒç”¨å·¥å…·: {len(response.tool_calls)} ä¸ª")
            for i, tool_call in enumerate(response.tool_calls):
                chat_logger.info(f"  ğŸ”§ å·¥å…· {i+1} è¯¦ç»†ä¿¡æ¯:")
                chat_logger.info(f"    ğŸ“› å·¥å…·åç§°: {tool_call['name']}")
                chat_logger.info(f"    ğŸ”‘ å·¥å…·ID: {tool_call['id']}")
                chat_logger.info(f"    ğŸ“‹ å·¥å…·å‚æ•°: {json.dumps(tool_call['args'], ensure_ascii=False, indent=2)}")
                chat_logger.info(f"    ğŸ“Š å‚æ•°æ•°é‡: {len(tool_call['args'])}")
        else:
            chat_logger.info(f"ğŸ’¬ æ¨¡å‹ç›´æ¥å›ç­”ï¼Œæ— å·¥å…·è°ƒç”¨")
        
        # è®°å½•æ¨¡å‹å“åº”çš„å…¶ä»–å±æ€§ï¼ˆé¿å…è®¿é—®Pydanticå·²å¼ƒç”¨çš„å±æ€§ï¼‰
        chat_logger.info(f"ğŸ” æ¨¡å‹å“åº”å¯¹è±¡å±æ€§:")
        # å®šä¹‰å®‰å…¨çš„å±æ€§åˆ—è¡¨ï¼Œé¿å…è®¿é—®å·²å¼ƒç”¨çš„Pydanticå±æ€§
        safe_attrs = ['content', 'tool_calls', 'response_metadata', 'usage_metadata', 'id', 'type']
        for attr in safe_attrs:
            if hasattr(response, attr):
                try:
                    value = getattr(response, attr)
                    if value is not None:
                        chat_logger.info(f"    ğŸ“Œ {attr}: {value}")
                except Exception as e:
                    chat_logger.info(f"    ğŸ“Œ {attr}: æ— æ³•è·å–å€¼ ({str(e)})")
        
        # å…³é”®ä¿®å¤ï¼šéªŒè¯å¹¶æ¸…ç†å·¥å…·è°ƒç”¨å‚æ•°
        if hasattr(response, "tool_calls") and response.tool_calls:
            chat_logger.info(f"ğŸ§¹ å¼€å§‹æ¸…ç†å·¥å…·è°ƒç”¨å‚æ•°...")
            chat_logger.info(f"ğŸ“Š åŸå§‹å·¥å…·è°ƒç”¨æ•°é‡: {len(response.tool_calls)}")
            
            cleaned_tool_calls = []
            for i, tool_call in enumerate(response.tool_calls):
                chat_logger.info(f"  ğŸ” å¤„ç†å·¥å…·è°ƒç”¨ {i+1}:")
                chat_logger.info(f"    ğŸ“› åŸå§‹åç§°: {tool_call['name']}")
                chat_logger.info(f"    ğŸ”‘ åŸå§‹ID: {tool_call['id']}")
                chat_logger.info(f"    ğŸ“‹ åŸå§‹å‚æ•°: {json.dumps(tool_call['args'], ensure_ascii=False, indent=4)}")
                
                # åªä¿ç•™æœ‰æ•ˆçš„å‚æ•°
                valid_args = {}
                # æ ¹æ®å·¥å…·åç§°å¤„ç†å‚æ•°
                if tool_call["name"] == "rag_knowledge_search":
                    # ä»…ä¿ç•™queryå‚æ•°
                    if "query" in tool_call["args"]:
                        valid_args["query"] = tool_call["args"]["query"]
                        chat_logger.info(f"    âœ… ä¿ç•™æœ‰æ•ˆå‚æ•°: query = {tool_call['args']['query']}")
                    else:
                        # å¦‚æœæ²¡æœ‰queryå‚æ•°ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå‚æ•°æˆ–æ•´ä¸ªå†…å®¹ä½œä¸ºæŸ¥è¯¢
                        first_arg = next(iter(tool_call["args"].values()), "æœªçŸ¥æŸ¥è¯¢")
                        valid_args["query"] = str(first_arg)
                        logger.warning(f"rag_knowledge_searchç¼ºå°‘queryå‚æ•°ï¼Œä½¿ç”¨å¤‡ç”¨å‚æ•°: {first_arg}")
                        chat_logger.warning(f"âš ï¸ rag_knowledge_searchç¼ºå°‘queryå‚æ•°ï¼Œä½¿ç”¨å¤‡ç”¨å‚æ•°: {first_arg}")
                elif tool_call["name"] == "web_search_tool":
                    # ä»…ä¿ç•™queryå‚æ•°
                    if "query" in tool_call["args"]:
                        valid_args["query"] = tool_call["args"]["query"]
                        chat_logger.info(f"    âœ… ä¿ç•™æœ‰æ•ˆå‚æ•°: query = {tool_call['args']['query']}")
                    else:
                        first_arg = next(iter(tool_call["args"].values()), "æœªçŸ¥æŸ¥è¯¢")
                        valid_args["query"] = str(first_arg)
                        logger.warning(f"web_search_toolç¼ºå°‘queryå‚æ•°ï¼Œä½¿ç”¨å¤‡ç”¨å‚æ•°: {first_arg}")
                        chat_logger.warning(f"âš ï¸ web_search_toolç¼ºå°‘queryå‚æ•°ï¼Œä½¿ç”¨å¤‡ç”¨å‚æ•°: {first_arg}")
                # æ·»åŠ ç”¨æˆ·æ–‡æ¡£å·¥å…·å‚æ•°å¤„ç†
                elif tool_call["name"].startswith("search_"):
                    # ä»…ä¿ç•™queryå‚æ•°
                    if "query" in tool_call["args"]:
                        valid_args["query"] = tool_call["args"]["query"]
                        chat_logger.info(f"    âœ… ä¿ç•™æœ‰æ•ˆå‚æ•°: query = {tool_call['args']['query']}")
                    else:
                        first_arg = next(iter(tool_call["args"].values()), "æœªçŸ¥æŸ¥è¯¢")
                        valid_args["query"] = str(first_arg)
                        logger.warning(f"{tool_call['name']}ç¼ºå°‘queryå‚æ•°ï¼Œä½¿ç”¨å¤‡ç”¨å‚æ•°: {first_arg}")
                        chat_logger.warning(f"âš ï¸ {tool_call['name']}ç¼ºå°‘queryå‚æ•°ï¼Œä½¿ç”¨å¤‡ç”¨å‚æ•°: {first_arg}")
                
                # åˆ›å»ºæ¸…ç†åçš„å·¥å…·è°ƒç”¨
                cleaned_tool_call = {
                    "name": tool_call["name"],
                    "args": valid_args,
                    "id": tool_call["id"]
                }
                cleaned_tool_calls.append(cleaned_tool_call)
                
                chat_logger.info(f"    ğŸ§¹ æ¸…ç†åå‚æ•°: {json.dumps(valid_args, ensure_ascii=False, indent=4)}")
                chat_logger.info(f"    ğŸ“Š å‚æ•°æ¸…ç†: {len(tool_call['args'])} -> {len(valid_args)}")
            
            # æ›¿æ¢åŸå§‹çš„tool_calls
            response.tool_calls = cleaned_tool_calls
            logger.info(f"å·²æ¸…ç†å·¥å…·è°ƒç”¨å‚æ•°ï¼Œç§»é™¤æ— æ•ˆå‚æ•°")
            chat_logger.info(f"âœ… å·¥å…·è°ƒç”¨å‚æ•°æ¸…ç†å®Œæˆ")
            chat_logger.info(f"ğŸ“Š æ¸…ç†åå·¥å…·è°ƒç”¨æ•°é‡: {len(cleaned_tool_calls)}")
        else:
            chat_logger.info(f"â„¹ï¸ æ— éœ€æ¸…ç†å·¥å…·è°ƒç”¨å‚æ•°ï¼ˆæ— å·¥å…·è°ƒç”¨ï¼‰")
        
        # è®¡ç®—å·¥å…·è°ƒç”¨å¢é‡
        tool_call_increment = 1 if (hasattr(response, "tool_calls") and response.tool_calls) else 0
        
        # =============== æ–°å¢ï¼šè¯¦ç»†æ—¥å¿—è®°å½• ===============
        chat_logger.info(f"ğŸ“Š å·¥å…·è°ƒç”¨ç»Ÿè®¡ - æœ¬æ¬¡å¢é‡: {tool_call_increment}")
        chat_logger.info(f"ğŸ¯ æ¨¡å‹æ€è€ƒå®Œæˆï¼Œå‡†å¤‡è¿”å›ç»“æœ")
        # =============== æ–°å¢ç»“æŸ ===============
        return {
            "messages": [response],
            "tool_call_count": tool_call_increment,
            "knowledge_base_name": knowledge_base_name,  # ç¡®ä¿ä¼ é€’çŸ¥è¯†åº“åç§°
            "user_document_tools": user_document_tools_list,  # ç¡®ä¿ä¼ é€’ç”¨æˆ·æ–‡æ¡£å·¥å…·åˆ—è¡¨
            "web_search_enabled": state.get("web_search_enabled", True)
        }
    finally:
        await gpu_resource_manager.release()
        logger.info("Ollamaæ¨¡å‹å¤„ç†å®Œæˆï¼Œèµ„æºå·²é‡Šæ”¾")
        chat_logger.info(f"ğŸ§¹ GPUèµ„æºå·²é‡Šæ”¾")

# 5. ä¿®æ”¹æ¡ä»¶å‡½æ•°
def should_continue(state: AgentState):
    """å†³å®šæ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·æˆ–ç»“æŸ"""
    messages = state["messages"]
    last_message = messages[-1]
    tool_call_count = state.get("tool_call_count", 0)
    
    # =============== æ–°å¢ï¼šè¯¦ç»†æ—¥å¿—è®°å½• ===============
    chat_logger.info(f"ğŸ¤” å†³ç­–æ˜¯å¦ç»§ç»­ - å·¥å…·è°ƒç”¨æ¬¡æ•°: {tool_call_count}")
    # =============== æ–°å¢ç»“æŸ ===============
    
    # æ£€æŸ¥å·¥å…·è°ƒç”¨æ¬¡æ•° - è¶…è¿‡5æ¬¡å¼ºåˆ¶ç»“æŸ
    if tool_call_count >= 5:
        chat_logger.info(f"ğŸ›‘ å·¥å…·è°ƒç”¨æ¬¡æ•°å·²è¾¾ä¸Šé™({tool_call_count})ï¼Œç»“æŸå¯¹è¯")
        return END
    # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œåˆ™ç»§ç»­
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        chat_logger.info(f"ğŸ”„ æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼Œç»§ç»­æ‰§è¡Œå·¥å…·èŠ‚ç‚¹")
        return "tools"
    # å¦åˆ™ç»“æŸ
    chat_logger.info(f"âœ… æ— å·¥å…·è°ƒç”¨ï¼Œå¯¹è¯ç»“æŸ")
    return END

# 6. å…¨å±€å˜é‡å­˜å‚¨ç¼–è¯‘åçš„å›¾
graph = None

def tool_node(state: AgentState):
    """è‡ªå®šä¹‰å·¥å…·èŠ‚ç‚¹ï¼Œèƒ½æ ¹æ®çŸ¥è¯†åº“åç§°åŠ¨æ€è·å–å·¥å…·"""
    messages = state["messages"]
    last_message = messages[-1]
    knowledge_base_name = state.get("knowledge_base_name", "test")
    
    # =============== æ–°å¢ï¼šè¯¦ç»†æ—¥å¿—è®°å½• ===============
    chat_logger.info(f"ğŸ”§ å¼€å§‹æ‰§è¡Œå·¥å…·èŠ‚ç‚¹")
    chat_logger.info(f"  ğŸ”§ çŸ¥è¯†åº“: {knowledge_base_name}")
    chat_logger.info(f"  ğŸ“ éœ€è¦æ‰§è¡Œçš„å·¥å…·è°ƒç”¨: {len(last_message.tool_calls)} ä¸ª")
    # å°è¯•ä»çŠ¶æ€ä¸­æå–æ›´å¤šä¼šè¯ä¿¡æ¯
    if hasattr(state, 'get') and state.get('session_id'):
        chat_logger.info(f"  ğŸ†” Session ID: {state.get('session_id')}")
    if hasattr(state, 'get') and state.get('message_id'):
        chat_logger.info(f"  ğŸ“¨ Message ID: {state.get('message_id')}")
    
    for i, tool_call in enumerate(last_message.tool_calls):
        chat_logger.info(f"  ğŸ”§ å·¥å…· {i+1}: {tool_call['name']} - å‚æ•°: {tool_call['args']}")
    # =============== æ–°å¢ç»“æŸ ===============
    
    # åŠ¨æ€è·å–å½“å‰çŸ¥è¯†åº“çš„RAGå·¥å…·
    rag_tool = get_rag_tool(knowledge_base_name)
    # åˆ›å»ºå·¥å…·æ˜ å°„
    tools = {
        "rag_knowledge_search": rag_tool
    }
    # ä»…å½“å¯ç”¨æ—¶æ‰æ·»åŠ webæœç´¢å·¥å…·
    if state.get("web_search_enabled", True):
        global web_search_tool
        if web_search_tool is None:
            web_search_tool = create_search_tool()
        tools["web_search_tool"] = web_search_tool
    # æ·»åŠ ç”¨æˆ·æ–‡æ¡£å·¥å…·
    user_document_tools_list = state.get("user_document_tools", [])
    for tool_name in user_document_tools_list:
        tool_info = get_user_document_tool(tool_name)
        if tool_info and "tool" in tool_info:
            tools[tool_info["tool"].name] = tool_info["tool"]
    chat_logger.info(f"ğŸ”§ å¯ç”¨å·¥å…·: {list(tools.keys())}")
    
        # æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
    outputs = []
    for i, tool_call in enumerate(last_message.tool_calls):
        tool_name = tool_call["name"]
        chat_logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œå·¥å…· {i+1}: {tool_name}")
        
        if tool_name in tools:
            tool = tools[tool_name]
            try:
                # è°ƒç”¨å·¥å…·
                chat_logger.info(f"ğŸ”§ è°ƒç”¨å·¥å…· {tool_name}")
                chat_logger.info(f"  ğŸ“‹ å·¥å…·å‚æ•°: {tool_call['args']}")
                chat_logger.info(f"  ğŸ“‹ å·¥å…·å‚æ•°è¯¦æƒ…: {json.dumps(tool_call['args'], ensure_ascii=False, indent=4)}")
                
                # è®°å½•å·¥å…·è°ƒç”¨å¼€å§‹æ—¶é—´
                import time
                start_time = time.time()
                chat_logger.info(f"â±ï¸ å·¥å…· {tool_name} å¼€å§‹æ‰§è¡Œ...")
                
                response = tool.invoke(tool_call["args"])
                
                # è®°å½•å·¥å…·æ‰§è¡Œæ—¶é—´
                end_time = time.time()
                execution_time = end_time - start_time
                chat_logger.info(f"â±ï¸ å·¥å…· {tool_name} æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {execution_time:.3f} ç§’")
                
                # è®°å½•å·¥å…·è¿”å›ç»“æœ
                response_str = str(response)
                chat_logger.info(f"âœ… å·¥å…· {tool_name} æ‰§è¡ŒæˆåŠŸ")
                chat_logger.info(f"ğŸ“¤ å·¥å…·è¿”å›ç»“æœé•¿åº¦: {len(response_str)} å­—ç¬¦")
                chat_logger.info(f"ğŸ“„ å·¥å…·è¿”å›å®Œæ•´å†…å®¹:")
                chat_logger.info(f"   {response_str}")
                
                # æ·»åŠ ä¼šè¯ä¸Šä¸‹æ–‡ä¿¡æ¯
                chat_logger.info(f"  ğŸ“Š å·¥å…·æ‰§è¡Œç»Ÿè®¡:")
                chat_logger.info(f"    â±ï¸ æ‰§è¡Œæ—¶é—´: {execution_time:.3f} ç§’")
                chat_logger.info(f"    ğŸ“ è¿”å›å†…å®¹é•¿åº¦: {len(response_str)} å­—ç¬¦")
                chat_logger.info(f"    ğŸ¯ å·¥å…·åç§°: {tool_name}")
                
                # å¦‚æœæ˜¯RAGå·¥å…·ï¼Œè®°å½•ç›¸ä¼¼åº¦ä¿¡æ¯
                if tool_name == "rag_knowledge_search" and "ç›¸ä¼¼åº¦:" in response_str:
                    similarities = re.findall(r"ç›¸ä¼¼åº¦: ([\d.]+)", response_str)
                    if similarities:
                        similarities_float = [float(s) for s in similarities]
                        max_sim = max(similarities_float)
                        min_sim = min(similarities_float)
                        avg_sim = sum(similarities_float) / len(similarities_float)
                        chat_logger.info(f"ğŸ¯ RAGå·¥å…·ç›¸ä¼¼åº¦ç»Ÿè®¡:")
                        chat_logger.info(f"   ğŸ“Š æœ€é«˜ç›¸ä¼¼åº¦: {max_sim:.4f}")
                        chat_logger.info(f"   ğŸ“Š æœ€ä½ç›¸ä¼¼åº¦: {min_sim:.4f}")
                        chat_logger.info(f"   ğŸ“Š å¹³å‡ç›¸ä¼¼åº¦: {avg_sim:.4f}")
                        chat_logger.info(f"   ğŸ“Š ç›¸ä¼¼åº¦æ•°é‡: {len(similarities)}")
                        chat_logger.info(f"   ğŸ“‹ æ‰€æœ‰ç›¸ä¼¼åº¦: {similarities}")
                
                # è®°å½•å·¥å…·å“åº”çš„å…¶ä»–å±æ€§ï¼ˆé¿å…è®¿é—®Pydanticå·²å¼ƒç”¨çš„å±æ€§ï¼‰
                chat_logger.info(f"ğŸ” å·¥å…·å“åº”å¯¹è±¡å±æ€§:")
                # å®šä¹‰å®‰å…¨çš„å±æ€§åˆ—è¡¨ï¼Œé¿å…è®¿é—®å·²å¼ƒç”¨çš„Pydanticå±æ€§
                safe_attrs = ['content', 'name', 'status', 'tool_call_id']
                for attr in safe_attrs:
                    if hasattr(response, attr):
                        try:
                            value = getattr(response, attr)
                            if value is not None:
                                chat_logger.info(f"    ğŸ“Œ {attr}: {value}")
                        except Exception as e:
                            chat_logger.info(f"    ğŸ“Œ {attr}: æ— æ³•è·å–å€¼ ({str(e)})")
                
                outputs.append(
                    ToolMessage(
                        content=str(response),
                        name=tool_name,
                        tool_call_id=tool_call["id"]
                    )
                )
            except Exception as e:
                chat_logger.error(f"âŒ å·¥å…· {tool_name} æ‰§è¡Œå¤±è´¥: {str(e)}")
                chat_logger.error(f"ğŸ” é”™è¯¯è¯¦æƒ…: {type(e).__name__}: {str(e)}")
                
                # è®°å½•è¯¦ç»†çš„é”™è¯¯å †æ ˆ
                import traceback
                error_traceback = traceback.format_exc()
                chat_logger.error(f"ğŸ“š é”™è¯¯å †æ ˆ: {error_traceback}")
                
                outputs.append(
                    ToolMessage(
                        content=f"å·¥å…·è°ƒç”¨é”™è¯¯: {str(e)}",
                        name=tool_name,
                        status="error",
                        tool_call_id=tool_call["id"]
                    )
                )
        else:
            chat_logger.error(f"âŒ å·¥å…· {tool_name} ä¸å­˜åœ¨ï¼Œå¯ç”¨å·¥å…·: {list(tools.keys())}")
            outputs.append(
                ToolMessage(
                    content=f"Error: {tool_name} is not a valid tool, try one of [{', '.join(tools.keys())}]",
                    name=tool_name,
                    status="error"
                )
            )
    
    chat_logger.info(f"âœ… å·¥å…·èŠ‚ç‚¹æ‰§è¡Œå®Œæˆï¼Œè¿”å› {len(outputs)} ä¸ªç»“æœ")
    return {"messages": outputs}

# 8. æ ¸å¿ƒAPIç«¯ç‚¹
class ChatRequest(BaseModel):
    message: str
    session_id: Optional[str] = None
    stream: Optional[bool] = False  # æ˜¯å¦å¯ç”¨æµå¼å“åº”
    knowledge_base_name: Optional[str] = "test"  # æ–°å¢å‚æ•°ï¼Œé»˜è®¤ä¸º"test"
    url: Optional[str] = None
    enable_web_search: Optional[bool] = True
    message_id: Optional[str] = None

class ChatResponse(BaseModel):
    response: str
    session_id: str
    conversation_history: List[Dict[str, str]]
    tool_calls: Optional[List[Dict]] = None

class ErrorResponse(BaseModel):
    detail: str

@agent_router.post(
    "/chat",
    response_model=ChatResponse,
    responses={
        400: {"model": ErrorResponse, "description": "æ— æ•ˆçš„ä¼šè¯ID"},
        500: {"model": ErrorResponse, "description": "å†…éƒ¨æœåŠ¡é”™è¯¯"}
    }
)
async def chat_endpoint(request: ChatRequest):
    """
    å¤„ç†èŠå¤©è¯·æ±‚
    - æ–°ä¼šè¯: ä¸æä¾›session_id
    - ç»­ä¼šè¯: æä¾›æœ‰æ•ˆçš„session_id
    - url: å¯é€‰ï¼Œç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£URL
    """
    global graph
    if not graph:
        raise HTTPException(
            status_code=503,
            detail="æœåŠ¡å°šæœªåˆå§‹åŒ–å®Œæˆ"
        )
    # ç”Ÿæˆ/éªŒè¯ä¼šè¯ID
    session_id = request.session_id or f"session_{uuid.uuid4()}"
    config = {"configurable": {"thread_id": session_id}}
    
    # =============== æ–°å¢ï¼šè¯¦ç»†æ—¥å¿—è®°å½• ===============
    chat_logger.info(f"ğŸš€ å¼€å§‹å¤„ç†èŠå¤©è¯·æ±‚")
    chat_logger.info(f"  ğŸ†” Session ID: {session_id}")
    if request.message_id:
        chat_logger.info(f"  ğŸ“¨ Message ID: {request.message_id}")
    chat_logger.info(f"  ğŸ“ ç”¨æˆ·è¾“å…¥: {request.message}")
    chat_logger.info(f"  ğŸ”§ çŸ¥è¯†åº“: {request.knowledge_base_name}")
    chat_logger.info(f"  ğŸŒ Webæœç´¢: {'å¯ç”¨' if request.enable_web_search else 'ç¦ç”¨'}")
    if request.url:
        chat_logger.info(f"  ğŸ“„ æ–‡æ¡£URL: {request.url}")
    chat_logger.info(f"  â° è¯·æ±‚æ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    # =============== æ–°å¢ç»“æŸ ===============
    
    try:
        # è·å–å½“å‰ä¼šè¯çŠ¶æ€
        state = await graph.aget_state(config)
        # æ„å»ºæ–°çŠ¶æ€
        if state is None or not isinstance(state.values, dict) or "messages" not in state.values:
            # æ–°ä¼šè¯ï¼ˆåŒ…æ‹¬çŠ¶æ€ä¸å®Œæ•´çš„æƒ…å†µï¼‰
            chat_logger.info(f"ğŸ†• åˆ›å»ºæ–°ä¼šè¯")
            user_document_tools_list = []
            # =============== æ–°å¢ï¼šå¤„ç†æ–‡æ¡£URL ===============
            if request.url:
                try:
                    logger.info(f"å¤„ç†ç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£URL: {request.url}")
                    # ä½¿ç”¨session_idä½œä¸ºdocument_id
                    tool_name = register_user_document_tool(
                        url=request.url,
                        document_id=session_id,
                        document_name="ç”¨æˆ·ä¸Šä¼ çš„åŸºå› æ£€æµ‹æŠ¥å‘Š"
                    )
                    user_document_tools_list.append(tool_name)
                    logger.info(f"æˆåŠŸæ³¨å†Œç”¨æˆ·æ–‡æ¡£å·¥å…·: {tool_name}")
                    chat_logger.info(f"âœ… æ³¨å†Œæ–‡æ¡£å·¥å…·: {tool_name}")
                except Exception as e:
                    logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")
                    chat_logger.error(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")
                    # å³ä½¿æ–‡æ¡£å¤„ç†å¤±è´¥ï¼Œä¹Ÿè¦ç»§ç»­å¯¹è¯
            # =============== æ–°å¢ç»“æŸ ===============
            initial_state = {
                "messages": [HumanMessage(content=request.message)],
                "knowledge_base_name": request.knowledge_base_name,
                "tool_call_count": 0,
                "user_document_tools": user_document_tools_list,
                "web_search_enabled": request.enable_web_search  # ä¿å­˜webæœç´¢å¼€å…³çŠ¶æ€
            }
        else:
            # ç»­ä¼šè¯ - åªæ·»åŠ æ–°æ¶ˆæ¯ï¼Œè®© LangGraph è‡ªåŠ¨ç®¡ç†çŠ¶æ€ç´¯ç§¯
            # ä¿®å¤è¯´æ˜ï¼šé¿å…åŒé‡ç´¯ç§¯é—®é¢˜ï¼Œè®© LangGraph çš„ Annotated[list, add] è‡ªåŠ¨å¤„ç†æ¶ˆæ¯å†å²
            chat_logger.info(f"ğŸ”„ ç»§ç»­ç°æœ‰ä¼šè¯ï¼Œå†å²æ¶ˆæ¯æ•°: {len(state.values.get('messages', []))}")
            chat_logger.info(f"ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ LangGraph è‡ªåŠ¨ç´¯ç§¯ï¼Œé¿å…æ‰‹åŠ¨é‡å¤æ·»åŠ æ¶ˆæ¯")
            # ä¿ç•™ä¹‹å‰çš„çŸ¥è¯†åº“åç§°ï¼Œå³ä½¿è¯·æ±‚ä¸­æä¾›äº†æ–°å€¼ï¼ˆé¿å…ä¸­é€”åˆ‡æ¢çŸ¥è¯†åº“å¯¼è‡´æ··æ·†ï¼‰
            knowledge_base_name = state.values.get("knowledge_base_name", request.knowledge_base_name)
            # =============== æ–°å¢ï¼šå¤„ç†æ–‡æ¡£URLï¼ˆå¦‚æœæ˜¯æ–°ä¸Šä¼ çš„æ–‡æ¡£ï¼‰ ===============
            user_document_tools_list = state.values.get("user_document_tools", [])
            chat_logger.info(f"ğŸ“š ç°æœ‰æ–‡æ¡£å·¥å…·: {user_document_tools_list}")
            # å¦‚æœæœ‰æ–°çš„URLä¸”è¿˜æ²¡æœ‰æ³¨å†Œè¿‡å¯¹åº”çš„å·¥å…·
            if request.url and not any(
                    tool_name.startswith("search_" + session_id) for tool_name in user_document_tools_list):
                try:
                    logger.info(f"å¤„ç†ç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£URL: {request.url}")
                    # ä½¿ç”¨session_idä½œä¸ºdocument_id
                    tool_name = register_user_document_tool(
                        url=request.url,
                        document_id=session_id,
                        document_name="ç”¨æˆ·ä¸Šä¼ çš„åŸºå› æ£€æµ‹æŠ¥å‘Š"
                    )
                    user_document_tools_list.append(tool_name)
                    logger.info(f"æˆåŠŸæ³¨å†Œç”¨æˆ·æ–‡æ¡£å·¥å…·: {tool_name}")
                    chat_logger.info(f"âœ… æ–°å¢æ–‡æ¡£å·¥å…·: {tool_name}")
                except Exception as e:
                    logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")
                    chat_logger.error(f"âŒ æ–°å¢æ–‡æ¡£å¤±è´¥: {str(e)}")
            # =============== æ–°å¢ç»“æŸ ===============
            # ä¿ç•™ä¹‹å‰çš„webæœç´¢è®¾ç½®ï¼Œé™¤éè¯·æ±‚ä¸­æä¾›äº†æ–°å€¼
            web_search_enabled = state.values.get("web_search_enabled", request.enable_web_search)
            initial_state = {
                "messages": [HumanMessage(content=request.message)],  # ä¿®å¤ï¼šåªæ·»åŠ æ–°æ¶ˆæ¯ï¼Œè®© LangGraph è‡ªåŠ¨ç´¯ç§¯
                # é‡è¦è¯´æ˜ï¼šè¿™é‡Œåªæ·»åŠ æ–°æ¶ˆæ¯ï¼ŒLangGraph çš„ Annotated[list, add] ä¼šè‡ªåŠ¨å°†æ–°æ¶ˆæ¯
                # ä¸ä¹‹å‰ä¿å­˜çš„çŠ¶æ€ä¸­çš„æ¶ˆæ¯åˆ—è¡¨åˆå¹¶ï¼Œé¿å…æ‰‹åŠ¨é‡å¤ç´¯ç§¯å¯¼è‡´çš„å†…å­˜æµªè´¹
                "knowledge_base_name": knowledge_base_name,
                "tool_call_count": state.values.get("tool_call_count", 0),
                "user_document_tools": user_document_tools_list,
                "web_search_enabled": web_search_enabled
            }
        chat_logger.info(f"ğŸ¯ åˆå§‹çŠ¶æ€æ„å»ºå®Œæˆï¼Œå·¥å…·æ•°é‡: {len(initial_state.get('user_document_tools', []))}")
        
        # æ‰§è¡Œå¯¹è¯æµ
        chat_logger.info(f"ğŸ”„ å¼€å§‹æ‰§è¡Œå¯¹è¯æµç¨‹...")
        final_state = None
        async for step in graph.astream(initial_state, config=config, stream_mode="values"):
            final_state = step
        if not final_state:
            chat_logger.error(f"âŒ å¯¹è¯æµç¨‹æœªäº§ç”Ÿæœ‰æ•ˆå“åº”")
            raise HTTPException(
                status_code=500,
                detail="å¯¹è¯æµç¨‹æœªäº§ç”Ÿæœ‰æ•ˆå“åº”"
            )
        
        chat_logger.info(f"âœ… å¯¹è¯æµç¨‹æ‰§è¡Œå®Œæˆ")
        # æå–æœ€æ–°å›å¤
        last_msg = final_state["messages"][-1]
        if not isinstance(last_msg, AIMessage):
            chat_logger.error(f"âŒ æ— æ•ˆçš„æ¨¡å‹å“åº”ç±»å‹: {type(last_msg)}")
            raise HTTPException(
                status_code=500,
                detail="æ— æ•ˆçš„æ¨¡å‹å“åº”ç±»å‹"
            )
        
        # è®°å½•æœ€ç»ˆçŠ¶æ€çš„è¯¦ç»†ä¿¡æ¯
        chat_logger.info(f"ğŸ“Š æœ€ç»ˆçŠ¶æ€è¯¦ç»†ä¿¡æ¯:")
        chat_logger.info(f"  ğŸ†” Session ID: {session_id}")
        if request.message_id:
            chat_logger.info(f"  ğŸ“¨ Message ID: {request.message_id}")
        chat_logger.info(f"  ğŸ“ æ€»æ¶ˆæ¯æ•°é‡: {len(final_state['messages'])}")
        chat_logger.info(f"  ğŸ”§ å·¥å…·è°ƒç”¨è®¡æ•°: {final_state.get('tool_call_count', 0)}")
        chat_logger.info(f"  ğŸ“š çŸ¥è¯†åº“åç§°: {final_state.get('knowledge_base_name', 'unknown')}")
        chat_logger.info(f"  ğŸŒ Webæœç´¢å¯ç”¨: {final_state.get('web_search_enabled', False)}")
        chat_logger.info(f"  ğŸ“„ ç”¨æˆ·æ–‡æ¡£å·¥å…·: {final_state.get('user_document_tools', [])}")
        if request.url:
            chat_logger.info(f"  ğŸ“„ æ–‡æ¡£URL: {request.url}")
        
        # è®°å½•æ‰€æœ‰æ¶ˆæ¯çš„è¯¦ç»†ä¿¡æ¯
        chat_logger.info(f"ğŸ“‹ å®Œæ•´å¯¹è¯å†å²:")
        for i, msg in enumerate(final_state["messages"]):
            if isinstance(msg, HumanMessage):
                chat_logger.info(f"  ğŸ“¤ æ¶ˆæ¯ {i+1} [ç”¨æˆ·]: {msg.content}")
            elif isinstance(msg, AIMessage):
                chat_logger.info(f"  ğŸ“¤ æ¶ˆæ¯ {i+1} [AI]: {msg.content}")
            elif isinstance(msg, SystemMessage):
                chat_logger.info(f"  ğŸ“¤ æ¶ˆæ¯ {i+1} [ç³»ç»Ÿ]: {msg.content}")
            elif isinstance(msg, ToolMessage):
                chat_logger.info(f"  ğŸ“¤ æ¶ˆæ¯ {i+1} [å·¥å…· {msg.name}]: {msg.content}")
        
        # æ”¶é›†å·¥å…·è°ƒç”¨ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        tool_calls = []
        if hasattr(last_msg, "tool_calls") and last_msg.tool_calls:
            chat_logger.info(f"ğŸ”§ æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨: {len(last_msg.tool_calls)} ä¸ª")
            for i, tool_call in enumerate(last_msg.tool_calls):
                tool_calls.append({
                    "name": tool_call["name"],
                    "args": tool_call["args"],
                    "id": tool_call["id"]
                })
                chat_logger.info(f"  ğŸ”§ å·¥å…· {i+1}: {tool_call['name']} - å‚æ•°: {tool_call['args']}")
        else:
            chat_logger.info(f"ğŸ’¬ æ— å·¥å…·è°ƒç”¨ï¼Œç›´æ¥å›ç­”")
        
        # æ„å»ºå†å²è®°å½•
        history = []
        for msg in final_state["messages"]:
            if isinstance(msg, HumanMessage):
                history.append({"role": "user", "content": msg.content})
            elif isinstance(msg, AIMessage):
                history.append({"role": "assistant", "content": msg.content})
            elif isinstance(msg, SystemMessage):
                history.append({"role": "system", "content": "ç³»ç»Ÿæç¤º"})
            elif isinstance(msg, ToolMessage):
                history.append({"role": "tool", "content": msg.content})
        
        # è®°å½•æœ€ç»ˆè¿”å›çš„è¯¦ç»†ä¿¡æ¯
        chat_logger.info(f"ğŸ“¤ è¿”å›æœ€ç»ˆå›ç­”:")
        chat_logger.info(f"  ğŸ†” Session ID: {session_id}")
        if request.message_id:
            chat_logger.info(f"  ğŸ“¨ Message ID: {request.message_id}")
        chat_logger.info(f"  ğŸ“„ å›ç­”é•¿åº¦: {len(last_msg.content)} å­—ç¬¦")
        chat_logger.info(f"  ğŸ“ å›ç­”å†…å®¹: {last_msg.content}")
        chat_logger.info(f"  ğŸ“Š å†å²è®°å½•æ•°é‡: {len(history)}")
        chat_logger.info(f"  ğŸ”§ å·¥å…·è°ƒç”¨æ•°é‡: {len(tool_calls) if tool_calls else 0}")
        if request.url:
            chat_logger.info(f"  ğŸ“„ æ–‡æ¡£URL: {request.url}")
        
        chat_logger.info(f"ğŸ¯ å¯¹è¯å®Œæˆ")
        chat_logger.info(f"  ğŸ†” Session ID: {session_id}")
        if request.message_id:
            chat_logger.info(f"  ğŸ“¨ Message ID: {request.message_id}")
        chat_logger.info(f"  â° å®Œæˆæ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        return ChatResponse(
            response=last_msg.content,
            session_id=session_id,
            conversation_history=history,
            tool_calls=tool_calls if tool_calls else None
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ å¤„ç†è¯·æ±‚å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"è¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}"
        )

# 9. æµå¼å“åº”APIï¼ˆå¯é€‰ï¼‰
@agent_router.post("/chat/stream")
async def chat_stream_endpoint(request: ChatRequest):
    """
    æµå¼å“åº”èŠå¤©è¯·æ±‚ï¼ˆå›ºå®šåˆ†å—å¤§å°ï¼‰
    - ä½¿ç”¨æ ‡å‡†SSEæ ¼å¼
    - ä¿®å¤æµå¼å“åº”é—®é¢˜
    """
    global graph
    if not graph:
        raise HTTPException(
            status_code=503,
            detail="æœåŠ¡å°šæœªåˆå§‹åŒ–å®Œæˆ"
        )
    # ç”Ÿæˆ/éªŒè¯ä¼šè¯ID
    session_id = request.session_id or f"session_{uuid.uuid4()}"
    message_id = request.message_id
    config = {"configurable": {"thread_id": session_id}}
    
    async def event_generator():
        try:
            # è·å–å½“å‰ä¼šè¯çŠ¶æ€å’Œæ„å»ºåˆå§‹çŠ¶æ€
            state = await graph.aget_state(config)
            if state is None or not isinstance(state.values, dict) or "messages" not in state.values:
                # æ–°ä¼šè¯å¤„ç†...
                user_document_tools_list = []
                if request.url:
                    try:
                        logger.info(f"å¤„ç†ç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£URL: {request.url}")
                        document_id = f"{session_id}_{uuid.uuid4().hex[:8]}"
                        tool_name = register_user_document_tool(
                            url=request.url,
                            document_id=document_id,
                            document_name="ç”¨æˆ·ä¸Šä¼ çš„åŸºå› æ£€æµ‹æŠ¥å‘Š"
                        )
                        user_document_tools_list.append(tool_name)
                        logger.info(f"æˆåŠŸæ³¨å†Œç”¨æˆ·æ–‡æ¡£å·¥å…·: {tool_name}")
                    except Exception as e:
                        logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")
                initial_state = {
                    "messages": [HumanMessage(content=request.message)],
                    "knowledge_base_name": request.knowledge_base_name,
                    "tool_call_count": 0,
                    "user_document_tools": user_document_tools_list,
                    "web_search_enabled": request.enable_web_search
                }
            else:
                # ç»­ä¼šè¯å¤„ç†...
                knowledge_base_name = state.values.get("knowledge_base_name", request.knowledge_base_name)
                user_document_tools_list = state.values.get("user_document_tools", [])
                web_search_enabled = state.values.get("web_search_enabled", request.enable_web_search)
                if request.url and not any(
                        tool_name.startswith("search_" + session_id) for tool_name in user_document_tools_list):
                    try:
                        logger.info(f"å¤„ç†ç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£URL: {request.url}")
                        document_id = f"{session_id}_{uuid.uuid4().hex[:8]}"
                        tool_name = register_user_document_tool(
                            url=request.url,
                            document_id=document_id,
                            document_name="ç”¨æˆ·ä¸Šä¼ çš„åŸºå› æ£€æµ‹æŠ¥å‘Š"
                        )
                        user_document_tools_list.append(tool_name)
                        logger.info(f"æˆåŠŸæ³¨å†Œç”¨æˆ·æ–‡æ¡£å·¥å…·: {tool_name}")
                    except Exception as e:
                        logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")
                initial_state = {
                    "messages": [HumanMessage(content=request.message)],  # ä¿®å¤ï¼šåªæ·»åŠ æ–°æ¶ˆæ¯ï¼Œè®© LangGraph è‡ªåŠ¨ç´¯ç§¯
                    # é‡è¦è¯´æ˜ï¼šè¿™é‡Œåªæ·»åŠ æ–°æ¶ˆæ¯ï¼ŒLangGraph çš„ Annotated[list, add] ä¼šè‡ªåŠ¨å°†æ–°æ¶ˆæ¯
                    # ä¸ä¹‹å‰ä¿å­˜çš„çŠ¶æ€ä¸­çš„æ¶ˆæ¯åˆ—è¡¨åˆå¹¶ï¼Œé¿å…æ‰‹åŠ¨é‡å¤ç´¯ç§¯å¯¼è‡´çš„å†…å­˜æµªè´¹
                    "knowledge_base_name": knowledge_base_name,
                    "tool_call_count": state.values.get("tool_call_count", 0),
                    "user_document_tools": user_document_tools_list,
                    "web_search_enabled": web_search_enabled
                }
            
            # ä¼˜åŒ–åˆ†å—å¤§å°å’Œæµå¼å¤„ç†é€»è¾‘
            CHUNK_SIZE = 50  # å¢å¤§åˆ†å—å¤§å°ï¼Œå‡å°‘è¿‡åº¦åˆ†å‰²
            full_text = ""
            last_sent_length = 0  # è®°å½•å·²å‘é€çš„æ–‡æœ¬é•¿åº¦
            
            logger.info(f"ïš€ å¼€å§‹æµå¼å“åº”ï¼Œåˆ†å—å¤§å°: {CHUNK_SIZE}")
            
            # å‘é€å¼€å§‹æ ‡è®°
            start_data = {
                "text": "",
                "finish_reason": "start",
                "session_id": session_id,
                "message_id": message_id
            }
            yield f"data: {json.dumps(start_data)}\n"
            await asyncio.sleep(0.01)
            
            # è®°å½•æµå¼å“åº”å¼€å§‹
            logger.info(f"ğŸ”„ æµå¼å“åº”å¼€å§‹")
            logger.info(f"  ğŸ†” Session ID: {session_id}")
            if message_id:
                logger.info(f"  ğŸ“¨ Message ID: {message_id}")
            logger.info(f"  ğŸ“ ç”¨æˆ·æ¶ˆæ¯: {request.message[:100]}{'...' if len(request.message) > 100 else ''}")
            if request.url:
                logger.info(f"  ğŸ“„ æ–‡æ¡£URL: {request.url}")
            logger.info(f"  â° å¼€å§‹æ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            
            try:
                async for step in graph.astream_log(initial_state, config=config):
                    if isinstance(step, dict) and "ops" in step:
                        ops = step["ops"]
                    elif hasattr(step, "ops"):
                        ops = step.ops
                    else:
                        continue
                    
                    for op in ops:
                        path = op.get("path", "") if isinstance(op, dict) else getattr(op, "path", "")
                        value = op.get("value") if isinstance(op, dict) else getattr(op, "value", None)
                        
                        # æ£€æŸ¥æ˜¯å¦æ˜¯æ¨¡å‹è°ƒç”¨çš„æ—¥å¿—
                        if path.startswith("/logs/call_model/") and value is not None:
                            if isinstance(value, dict) and "messages" in value:
                                for msg in value["messages"]:
                                    if isinstance(msg, AIMessage) and hasattr(msg, "content") and msg.content:
                                        # ç´¯ç§¯æ–°å†…å®¹
                                        new_content = msg.content
                                        if new_content:
                                            full_text += new_content
                                            logger.debug(f"ï“ ç´¯ç§¯å†…å®¹ï¼Œå½“å‰é•¿åº¦: {len(full_text)}")
                                            
                                            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°å†…å®¹éœ€è¦å‘é€
                                            if len(full_text) > last_sent_length:
                                                # å‘é€æ–°å¢çš„å†…å®¹
                                                new_chunk = full_text[last_sent_length:]
                                                logger.debug(f"ï“¤ å‡†å¤‡å‘é€æ–°å†…å®¹ï¼Œé•¿åº¦: {len(new_chunk)}")
                                                
                                                # å¦‚æœæ–°å†…å®¹è¶…è¿‡åˆ†å—å¤§å°ï¼Œè¿›è¡Œåˆ†å—
                                                while len(new_chunk) > CHUNK_SIZE:
                                                    chunk = new_chunk[:CHUNK_SIZE]
                                                    new_chunk = new_chunk[CHUNK_SIZE:]
                                                    
                                                    # å‘é€åˆ†å—å†…å®¹
                                                    data = {
                                                        "text": chunk,
                                                        "finish_reason": None,
                                                        "session_id": session_id,
                                                        "message_id": message_id
                                                    }
                                                    yield f"data: {json.dumps(data)}\n"
                                                    await asyncio.sleep(0.01)
                                                    logger.debug(f"ï“¤ å‘é€åˆ†å—å†…å®¹ï¼Œé•¿åº¦: {len(chunk)}")
                                                
                                                # å‘é€å‰©ä½™çš„æ–°å†…å®¹
                                                if new_chunk:
                                                    data = {
                                                        "text": new_chunk,
                                                        "finish_reason": None,
                                                        "session_id": session_id,
                                                        "message_id": message_id
                                                    }
                                                    yield f"data: {json.dumps(data)}\n"
                                                    await asyncio.sleep(0.01)
                                                    logger.debug(f"ï“¤ å‘é€å‰©ä½™å†…å®¹ï¼Œé•¿åº¦: {len(new_chunk)}")
                                                
                                                # æ›´æ–°å·²å‘é€é•¿åº¦
                                                last_sent_length = len(full_text)
                                                logger.debug(f"âœ… å·²å‘é€é•¿åº¦æ›´æ–°ä¸º: {last_sent_length}")
                                                
                                                # å®šæœŸæ¸…ç†æ˜¾å­˜
                                                if len(full_text) % 200 == 0:
                                                    import gc
                                                    gc.collect()
                                                    logger.info("ï§¹ æ‰§è¡Œæ˜¾å­˜æ¸…ç†")
                                                
                                                # å‘é€è¿›åº¦æŒ‡ç¤ºå™¨
                                                if len(full_text) % 100 == 0:
                                                    progress_data = {
                                                        "text": "",
                                                        "finish_reason": "progress",
                                                        "session_id": session_id,
                                                        "message_id": message_id,
                                                        "progress": {
                                                            "total_length": len(full_text),
                                                            "sent_length": last_sent_length
                                                        }
                                                    }
                                                    yield f"data: {json.dumps(progress_data)}\n"
                                                    await asyncio.sleep(0.01)
                
                # ç„¶åè·å–æœ€ç»ˆçŠ¶æ€ï¼Œç¡®ä¿å¯¹è¯æµç¨‹å®Œæˆ
                logger.info("ï”„ æ—¥å¿—æµå®Œæˆï¼Œè·å–æœ€ç»ˆçŠ¶æ€...")
                final_state = None
                async for step in graph.astream(initial_state, config=config, stream_mode="values"):
                    final_state = step
                
                # å¦‚æœæœ‰æœ€ç»ˆçŠ¶æ€ï¼Œå‘é€æœ€ç»ˆå›ç­”
                if final_state and "messages" in final_state:
                    last_msg = final_state["messages"][-1]
                    if isinstance(last_msg, AIMessage) and hasattr(last_msg, "content") and last_msg.content:
                        final_content = last_msg.content
                        # æ£€æŸ¥æ˜¯å¦ä¸å·²å‘é€å†…å®¹ä¸åŒ
                        if final_content != full_text:
                            logger.info(f"ï“¤ å‘é€æœ€ç»ˆå›ç­”ï¼Œé•¿åº¦: {len(final_content)}")
                            # å‘é€æœ€ç»ˆå›ç­”
                            data = {
                                "text": final_content,
                                "finish_reason": "final_answer",
                                "session_id": session_id,
                                "message_id": message_id
                            }
                            yield f"data: {json.dumps(data)}\n"
                            await asyncio.sleep(0.01)
            except Exception as e:
                logger.error(f"âŒ æµå¼å“åº”å¤±è´¥: {e}")
                # æ˜¾å­˜æ¸…ç†
                import gc
                gc.collect()
                raise e
            
            # å‘é€å‰©ä½™å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
            if full_text and len(full_text) > last_sent_length:
                remaining_text = full_text[last_sent_length:]
                if remaining_text:
                    data = {
                        "text": remaining_text,
                        "finish_reason": None,
                        "session_id": session_id,
                        "message_id": message_id
                    }
                    yield f"data: {json.dumps(data)}\n"
                    await asyncio.sleep(0.01)
                    logger.debug(f"ï“¤ å‘é€æœ€ç»ˆå‰©ä½™å†…å®¹ï¼Œé•¿åº¦: {len(remaining_text)}")
            
            # å‘é€ç»“æŸæ ‡è®°
            end_data = {
                "text": "",
                "finish_reason": "stop",
                "session_id": session_id,
                "message_id": message_id
            }
            yield f"data: {json.dumps(end_data)}\n"
            
            # è®°å½•æµå¼å“åº”å®Œæˆ
            logger.info(f"âœ… æµå¼å“åº”å®Œæˆ")
            logger.info(f"  ğŸ†” Session ID: {session_id}")
            if message_id:
                logger.info(f"  ğŸ“¨ Message ID: {message_id}")
            logger.info(f"  ğŸ“Š æ€»å†…å®¹é•¿åº¦: {len(full_text)}")
            logger.info(f"  ğŸ“¤ å·²å‘é€é•¿åº¦: {last_sent_length}")
            logger.info(f"  â° å®Œæˆæ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            
            # æœ€ç»ˆæ˜¾å­˜æ¸…ç†
            import gc
            gc.collect()
            logger.info(f"ï§¹ æµå¼å“åº”å®Œæˆï¼Œæ‰§è¡Œæœ€ç»ˆæ˜¾å­˜æ¸…ç†ã€‚æ€»å†…å®¹é•¿åº¦: {len(full_text)}, å·²å‘é€é•¿åº¦: {last_sent_length}")
        except Exception as e:
            logger.error(f"âŒ event_generator å¤±è´¥: {e}")
            # å‘é€é”™è¯¯ä¿¡æ¯
            error_data = {
                "text": f"é”™è¯¯: {str(e)}",
                "finish_reason": "error",
                "session_id": session_id,
                "message_id": message_id
            }
            yield f"data: {json.dumps(error_data)}\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream"  # ç¡®ä¿è¿™æ˜¯æ­£ç¡®çš„åª’ä½“ç±»å‹
    )


# 10. ä¼šè¯ç®¡ç†API
@agent_router.get("/sessions/{session_id}")
async def get_session(session_id: str):
    """è·å–ç‰¹å®šä¼šè¯çš„å®Œæ•´å†å²"""
    global graph
    if not graph:
        raise HTTPException(
            status_code=503,
            detail="æœåŠ¡å°šæœªåˆå§‹åŒ–å®Œæˆ"
        )
    config = {"configurable": {"thread_id": session_id}}
    try:
        state = await graph.aget_state(config)
        if not state or not isinstance(state.values, dict) or "messages" not in state.values:
            raise HTTPException(
                status_code=404,
                detail="ä¼šè¯ä¸å­˜åœ¨"
            )
        history = []
        for msg in state.values["messages"]:
            if isinstance(msg, HumanMessage):
                history.append({"role": "user", "content": msg.content})
            elif isinstance(msg, AIMessage):
                history.append({"role": "assistant", "content": msg.content})
            elif isinstance(msg, SystemMessage):
                history.append({"role": "system", "content": "ç³»ç»Ÿæç¤º"})
            elif isinstance(msg, ToolMessage):
                history.append({"role": "tool", "content": msg.content})
        return {
            "session_id": session_id,
            "conversation_history": history,
            "last_updated": state.config["configurable"].get("checkpoint_id")
        }
    except Exception as e:
        logger.error(f"âŒ è·å–ä¼šè¯å¤±è´¥: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"è·å–ä¼šè¯å¤±è´¥: {str(e)}"
        )

@agent_router.delete("/sessions/{session_id}")
async def delete_session(session_id: str):
    """åˆ é™¤ç‰¹å®šä¼šè¯"""
    global checkpointer
    if not checkpointer:
        raise HTTPException(
            status_code=503,
            detail="æœåŠ¡å°šæœªåˆå§‹åŒ–å®Œæˆ"
        )
    try:
        # åˆ é™¤ä¼šè¯
        await checkpointer.aput(
            {"configurable": {"thread_id": session_id}},
            None,  # ä¼ é€’Noneè¡¨ç¤ºåˆ é™¤
            None  # ä¼ é€’Noneè¡¨ç¤ºåˆ é™¤
        )
        return {"status": "success", "message": f"ä¼šè¯ {session_id} å·²åˆ é™¤"}
    except Exception as e:
        logger.error(f"âŒ åˆ é™¤ä¼šè¯å¤±è´¥: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"åˆ é™¤ä¼šè¯å¤±è´¥: {str(e)}"
        )

# ======================
# ç»Ÿä¸€å¥åº·æ£€æŸ¥å’Œä¸»åº”ç”¨
# ======================
@asynccontextmanager
async def lifespan(app: FastAPI):
    global qdrant_client, checkpointer, web_search_tool, graph
    try:
        # 1. åˆå§‹åŒ–Qdrantå®¢æˆ·ç«¯ï¼ˆçŸ¥è¯†åº“ç®¡ç†éœ€è¦ï¼‰
        logger.info("åˆå§‹åŒ–Qdrantå®¢æˆ·ç«¯...")
        qdrant_client = QdrantClient(host="localhost", port=6333)
        logger.info("âœ… Qdrantå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        
        # 2. åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± ï¼ˆAgentéœ€è¦ï¼‰
        logger.info("åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± ...")
        DB_URI = os.getenv(
            "DB_URI",
            "postgresql://postgres:postgres@localhost:5432/langgraph_db?sslmode=disable"
        )
        connection_kwargs = {
            "autocommit": True,
            "prepare_threshold": 0,
        }
        pool = AsyncConnectionPool(
            conninfo=DB_URI,
            max_size=20,
            kwargs=connection_kwargs
        )
        await pool.open()
        logger.info("åˆå§‹åŒ–æ•°æ®åº“æ£€æŸ¥ç‚¹å­˜å‚¨...")
        checkpointer = AsyncPostgresSaver(pool)
        await checkpointer.setup()
        logger.info("âœ… æ•°æ®åº“æ£€æŸ¥ç‚¹å­˜å‚¨åˆå§‹åŒ–æˆåŠŸ")
        
        # 3. åˆå§‹åŒ–webæœç´¢å·¥å…·
        try:
            logger.info("åˆå§‹åŒ–webæœç´¢å·¥å…·...")
            web_search_tool = create_search_tool()
            logger.info("âœ… å·¥å…·åŠ è½½æˆåŠŸ: web_search")
        except Exception as e:
            logger.error(f"âŒ å·¥å…·åŠ è½½å¤±è´¥: {str(e)}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œç»§ç»­åˆå§‹åŒ–å…¶ä»–ç»„ä»¶
        
        # 4. ç¼–è¯‘Agentå›¾
        logger.info("ç¼–è¯‘Agentå›¾...")
        builder = StateGraph(AgentState)
        builder.add_node("call_model", call_model)
        builder.add_node("tools", tool_node)
        builder.add_edge(START, "call_model")
        builder.add_conditional_edges(
            "call_model",
            should_continue,
            {
                "tools": "tools",
                END: END
            }
        )
        builder.add_edge("tools", "call_model")
        # ç¼–è¯‘å›¾
        graph = builder.compile(checkpointer=checkpointer)
        logger.info("âœ… Agentå›¾ç¼–è¯‘æˆåŠŸ")
        
        yield  # åº”ç”¨è¿è¡Œä¸­
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–å¤±è´¥: {str(e)}", exc_info=True)
        raise
    finally:
        # æ¸…ç†èµ„æº
        if 'pool' in locals():
            await pool.close()
            logger.info("âœ… æ•°æ®åº“è¿æ¥æ± å·²å…³é—­")

# åˆ›å»ºç»Ÿä¸€çš„FastAPIåº”ç”¨
app = FastAPI(
    title="ç»Ÿä¸€çŸ¥è¯†åº“ä¸å¯¹è¯AgentæœåŠ¡",
    description="æ•´åˆçŸ¥è¯†åº“ç®¡ç†å’Œå¯¹è¯AgentåŠŸèƒ½",
    version="1.0.0",
    lifespan=lifespan
)

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ³¨å†Œå­åº”ç”¨
app.include_router(kb_router)
app.include_router(agent_router)

# å¥åº·æ£€æŸ¥ç«¯ç‚¹ï¼ˆç»Ÿä¸€ï¼‰
@app.get("/health")
async def health_check():
    """ç»Ÿä¸€çš„å¥åº·æ£€æŸ¥"""
    try:
        # æ£€æŸ¥Qdrant
        global qdrant_client
        kb_status = "disconnected"
        total_kb = 0
        if qdrant_client:
            try:
                collections = qdrant_client.get_collections().collections
                kb_status = "connected"
                total_kb = len(collections)
            except Exception as e:
                kb_status = f"error: {str(e)}"
        
        # æ£€æŸ¥æ•°æ®åº“
        db_status = "connected" if checkpointer else "disconnected"
        
        # æ£€æŸ¥Agentå›¾
        agent_status = "initialized" if graph else "not_initialized"
        
        return {
            "status": "healthy",
            "knowledge_base": kb_status,
            "agent_database": db_status,
            "agent_status": agent_status,
            "total_knowledge_bases": total_kb,
            "service_info": {
                "port": 8000,
                "kb_api_prefix": "/kb",
                "agent_api_prefix": "/agent"
            }
        }
    except Exception as e:
        logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}", exc_info=True)
        return {
            "status": "unhealthy",
            "error": str(e),
            "service_info": {
                "port": 8000,
                "kb_api_prefix": "/kb",
                "agent_api_prefix": "/agent"
            }
        }

if __name__ == "__main__":
    import uvicorn
    logger.info("å¯åŠ¨ç»Ÿä¸€æœåŠ¡...")
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")