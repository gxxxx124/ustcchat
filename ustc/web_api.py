from enum import Enum
from fastapi import FastAPI, HTTPException, status, APIRouter, UploadFile, File, Form, Depends, Request
from fastapi.middleware.cors import CORSMiddleware
from starlette.middleware.base import BaseHTTPMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pydantic import BaseModel
from typing import Dict, List, Optional, Any, AsyncGenerator, TypedDict, Annotated
import uuid
import logging
from contextlib import asynccontextmanager, contextmanager
import os
import requests
import tempfile
import shutil
import operator
import re
import json
import asyncio
import gc
import torch
import time
from email._header_value_parser import parse_message_id
from operator import add
from psycopg_pool import AsyncConnectionPool

# å¯¼å…¥è®¤è¯ç›¸å…³æ¨¡å—
from auth import create_users_table, set_global_pool, get_current_admin_user, get_current_user, get_current_contributor_user, UserResponse, UserRole
from fastapi import Request
from auth_routes import auth_router, set_user_manager as set_auth_user_manager, init_ustc_oauth
from auth_middleware import create_auth_middleware
from langchain_core.messages import ToolMessage, HumanMessage, SystemMessage, AIMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver
from qdrant_client import QdrantClient
from qdrant_client.http import models as qdrant_models
from qdrant_client.http.models import VectorParams, Distance, FieldCondition, MatchValue
from oss2 import Auth, Bucket
from starlette.responses import StreamingResponse
from chunks2embedding import (
    embedding_init,
    upsert_md_file,
    upsert_md_file_with_source,
    upsert_md_file_with_original,
    upsert_qa_pair,
    delete_by_source,
    list_all_collections,
    get_collection_info
)

# å°è¯•å¯¼å…¥markerè½¬æ¢å™¨ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨è½»é‡çº§è½¬æ¢å™¨
try:
    from marker_pdf_converter import convert_pdf_to_markdown_with_marker
    MARKER_AVAILABLE = True
    print("âœ… Markerè½¬æ¢å™¨å¯ç”¨")
except ImportError as e:
    MARKER_AVAILABLE = False
    print(f"âš ï¸ Markerè½¬æ¢å™¨ä¸å¯ç”¨: {str(e)}ï¼Œå°†ä½¿ç”¨è½»é‡çº§è½¬æ¢å™¨")

from pdf import (
    register_user_document_tool,
    get_user_document_tool,
    get_user_document_tool_by_session,
    list_user_document_tools,
    set_db_pool,
    load_user_document_tools_from_db
)
from docx import Document
from smart_search import create_search_tool
import datetime

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(os.path.join('logs', "app.log"), encoding='utf-8')
    ]
)

# åˆ›å»ºä¸“é—¨çš„å¯¹è¯æ—¥å¿—è®°å½•å™¨
chat_logger = logging.getLogger("chat-flow")
chat_logger.setLevel(logging.INFO)
# é˜²æ­¢æ—¥å¿—ä¼ æ’­åˆ°çˆ¶çº§loggerï¼Œé¿å…é‡å¤è¾“å‡º
chat_logger.propagate = False

# åˆ›å»ºå¯¹è¯æ—¥å¿—æ–‡ä»¶å¤„ç†å™¨
import os
log_dir = "logs"
os.makedirs(log_dir, exist_ok=True)
chat_file_handler = logging.FileHandler(os.path.join(log_dir, "chat_flow.log"), encoding='utf-8')
chat_file_handler.setFormatter(logging.Formatter("%(asctime)s [CHAT] %(message)s"))
chat_logger.addHandler(chat_file_handler)

# åªè¾“å‡ºåˆ°æ–‡ä»¶ï¼Œä¸è¾“å‡ºåˆ°æ§åˆ¶å°ï¼ˆé¿å…æ—¥å¿—è¿‡å¤šï¼‰
# chat_console_handler = logging.StreamHandler()
# chat_console_handler.setFormatter(logging.Formatter("%(asctime)s [CHAT] %(message)s"))
# chat_logger.addHandler(chat_console_handler)

# åˆ›å»ºç»Ÿä¸€çš„æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger("unified-service")
logger.setLevel(logging.INFO)
# é˜²æ­¢æ—¥å¿—ä¼ æ’­åˆ°çˆ¶çº§loggerï¼Œé¿å…é‡å¤è¾“å‡º
logger.propagate = False

# åˆ›å»ºç»Ÿä¸€æœåŠ¡æ—¥å¿—æ–‡ä»¶å¤„ç†å™¨
unified_file_handler = logging.FileHandler(os.path.join(log_dir, "unified_service.log"), encoding='utf-8')
unified_file_handler.setFormatter(logging.Formatter("%(asctime)s [SERVICE] %(message)s"))
logger.addHandler(unified_file_handler)

# åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°ï¼ˆå®æ—¶çœ‹åˆ°æœåŠ¡æ—¥å¿—ï¼‰
unified_console_handler = logging.StreamHandler()
unified_console_handler.setFormatter(logging.Formatter("%(asctime)s [SERVICE] %(message)s"))
logger.addHandler(unified_console_handler)

# ======================
# æ˜¾å­˜ç®¡ç†å·¥å…·
# ======================
class GPUResourceManager:
    """ç®¡ç†GPUèµ„æºï¼Œç¡®ä¿åŒä¸€æ—¶é—´åªæœ‰ä¸€ä¸ªæ¨¡å‹åœ¨ä½¿ç”¨"""
    
    def __init__(self):
        self.lock = asyncio.Lock()
        self.current_model = None
        self.model_instances = {}
        
    async def acquire(self, model_type):
        """è·å–GPUèµ„æºå¹¶ç¡®ä¿æŒ‡å®šç±»å‹çš„æ¨¡å‹å·²åŠ è½½"""
        logger.info(f"å°è¯•è·å–GPUèµ„æºä»¥ä½¿ç”¨ {model_type} æ¨¡å‹...")
        logger.info(f"å½“å‰æ˜¾å­˜çŠ¶æ€: {self.get_gpu_memory_info()}")
        
        await self.lock.acquire()
        
        try:
            # å¦‚æœå·²ç»æœ‰å…¶ä»–æ¨¡å‹åŠ è½½ï¼Œå…ˆæ¸…ç†
            if self.current_model and self.current_model != model_type:
                logger.info(f"æ£€æµ‹åˆ°ä¸åŒæ¨¡å‹ç±»å‹ï¼Œå…ˆæ¸…ç† {self.current_model} æ¨¡å‹...")
                await self.release()
            
            # æ£€æŸ¥æ˜¾å­˜æ˜¯å¦è¶³å¤Ÿ
            required_memory = 2048 if model_type == "ocr" else 512  # OCRéœ€è¦æ›´å¤šæ˜¾å­˜ï¼Œembeddingç°åœ¨åªéœ€è¦512MB
            if not self.check_gpu_memory_available(required_memory):
                logger.warning(f"GPUæ˜¾å­˜ä¸è¶³ï¼Œå°è¯•å¼ºåˆ¶æ¸…ç†...")
                self.clear_gpu_memory()
                # å†æ¬¡æ£€æŸ¥
                if not self.check_gpu_memory_available(required_memory):
                    raise RuntimeError(f"GPUæ˜¾å­˜ä¸è¶³ï¼Œéœ€è¦è‡³å°‘ {required_memory}MB æ˜¾å­˜")
            
            # è®°å½•å½“å‰ä½¿ç”¨çš„æ¨¡å‹ç±»å‹
            self.current_model = model_type
            logger.info(f"å·²è·å–GPUèµ„æºï¼Œå‡†å¤‡ä½¿ç”¨ {model_type} æ¨¡å‹")
            return self
        except Exception as e:
            logger.error(f"è·å–GPUèµ„æºå¤±è´¥: {str(e)}")
            self.lock.release()
            raise
            
    async def release(self):
        """é‡Šæ”¾GPUèµ„æºï¼Œæ¸…ç†å½“å‰æ¨¡å‹"""
        if self.current_model:
            logger.info(f"æ­£åœ¨æ¸…ç† {self.current_model} æ¨¡å‹å ç”¨çš„èµ„æº...")
            # ä»å®ä¾‹ç¼“å­˜ä¸­ç§»é™¤
            if self.current_model in self.model_instances:
                del self.model_instances[self.current_model]
            self.current_model = None
            
            # æ˜¾å¼æ¸…ç†GPUå†…å­˜
            self.clear_gpu_memory()
            
        logger.info("GPUèµ„æºå·²é‡Šæ”¾")
        self.lock.release()
        
    def get_gpu_memory_info(self):
        """è·å–GPUå†…å­˜ä¿¡æ¯"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**2  # MB
            cached = torch.cuda.memory_reserved() / 1024**2  # MB
            total = torch.cuda.get_device_properties(0).total_memory / 1024**2  # MB
            return f"å·²åˆ†é…: {allocated:.2f}MB, å·²ç¼“å­˜: {cached:.2f}MB, æ€»è®¡: {total:.2f}MB"
        else:
            return "GPUä¸å¯ç”¨"
    
    def check_gpu_memory_available(self, required_mb):
        """æ£€æŸ¥GPUæ˜¯å¦æœ‰è¶³å¤Ÿçš„å†…å­˜"""
        if not torch.cuda.is_available():
            return False
        
        allocated = torch.cuda.memory_allocated() / 1024**2  # MB
        total = torch.cuda.get_device_properties(0).total_memory / 1024**2  # MB
        available = total - allocated
        
        return available >= required_mb
    
    def clear_gpu_memory(self):
        """æ¸…ç†GPUå†…å­˜"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
            logger.info(f"GPUå†…å­˜å·²æ¸…ç† - å·²é‡Šæ”¾ {torch.cuda.memory_allocated()/1024**2:.2f} MB")
    
def get_ollama_model(self):
        """è·å–DeepSeek APIæ¨¡å‹å®ä¾‹ï¼ˆæŒ‰éœ€åŠ è½½ï¼‰"""
        if "ollama" not in self.model_instances:
            # ä½¿ç”¨DeepSeek API
            # =============== DeepSeek API é…ç½® ===============
            # è·å–API Key: https://platform.deepseek.com/
            # æ”¯æŒçš„æ¨¡å‹: deepseek-chat, deepseek-coder, deepseek-r3
            api_key = os.getenv("DEEPSEEK_API_KEY", "")  # DeepSeek API Keyï¼ˆå¿…é¡»ä»ç¯å¢ƒå˜é‡è®¾ç½®ï¼‰
            api_base = os.getenv("DEEPSEEK_API_BASE", "https://api.deepseek.com")  # DeepSeek API åœ°å€
            model_name = os.getenv("DEEPSEEK_MODEL", "deepseek-chat")  # DeepSeek æ¨¡å‹åç§°
            # ================================================
            
            self.model_instances["ollama"] = ChatOpenAI(
                model=model_name,
                openai_api_key=api_key,
                openai_api_base=api_base,
                max_tokens=10000,  # APIæ¨¡å‹å¯ä»¥å¤„ç†æ›´å¤štoken
                temperature=0.1,
                request_timeout=120.0,  # å¢åŠ è¶…æ—¶æ—¶é—´
                max_retries=5,  # å¢åŠ é‡è¯•æ¬¡æ•°
                streaming=False  # ç¦ç”¨æµå¼å“åº”é¿å…å¡ä½
            )
            logger.info(f"ğŸš€ ä½¿ç”¨DeepSeek APIæ¨¡å‹: {model_name}")
        return self.model_instances["ollama"]
    
    def get_embedding_model(self):
        """è·å–å‘é‡åŒ–æ¨¡å‹ï¼ˆæŒ‰éœ€åŠ è½½ï¼‰"""
        if "embedding" not in self.model_instances:
            # è¿™é‡Œå¯ä»¥æ ¹æ®éœ€è¦åˆå§‹åŒ–å‘é‡åŒ–æ¨¡å‹
            # ç”±äºåŸå§‹ä»£ç ä¸­æ²¡æœ‰æ˜¾ç¤ºå…·ä½“å®ç°ï¼Œæˆ‘ä»¬åªè¿”å›åˆå§‹åŒ–å‡½æ•°
            from chunks2embedding import embedding_init
            self.model_instances["embedding"] = embedding_init
        return self.model_instances["embedding"]
    
    def get_ocr_model(self):
        """è·å–OCRæ¨¡å‹ï¼ˆæŒ‰éœ€åŠ è½½ï¼‰"""
        if "ocr" not in self.model_instances:
            # ç”±äºåŸå§‹ä»£ç ä¸­æ²¡æœ‰æ˜¾ç¤ºå…·ä½“å®ç°ï¼Œæˆ‘ä»¬åªè¿”å›åˆå§‹åŒ–å‡½æ•°
            from marker_pdf_converter import convert_pdf_to_markdown_with_marker
            self.model_instances["ocr"] = convert_pdf_to_markdown_with_marker
        return self.model_instances["ocr"]

# åˆ›å»ºå…¨å±€GPUèµ„æºç®¡ç†å™¨
gpu_resource_manager = GPUResourceManager()

# ======================
# å…¨å±€å…±äº«èµ„æº
# ======================
qdrant_client = None
checkpointer = None
rag_tool_cache = {}
web_search_tool = None

def get_current_user_from_token(request):
    """ä»è¯·æ±‚ä¸­è·å–å½“å‰ç”¨æˆ·ä¿¡æ¯"""
    try:
        # æ£€æŸ¥requestæ˜¯å¦æœ‰headerså±æ€§ï¼ˆFastAPI Requestå¯¹è±¡ï¼‰
        if hasattr(request, 'headers'):
            # ä»è¯·æ±‚å¤´ä¸­è·å–Authorization token (ä¸åŒºåˆ†å¤§å°å†™)
            auth_header = request.headers.get("Authorization") or request.headers.get("authorization")
            logger.info(f"ğŸ” è°ƒè¯•è®¤è¯: auth_header = {auth_header}")
            
            if not auth_header or not auth_header.startswith("Bearer "):
                logger.info("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„Authorizationå¤´")
                return None
            
            token = auth_header.split(" ")[1]
            logger.info(f"ğŸ” æå–çš„token: {token[:20]}...")
            
            # éªŒè¯tokenå¹¶è·å–ç”¨æˆ·ä¿¡æ¯
            from auth import verify_token
            token_data = verify_token(token)
            logger.info(f"ğŸ” tokenéªŒè¯ç»“æœ: {token_data}")
            
            # æš‚æ—¶å…è®¸æµ‹è¯•token
            if not token_data and token == "test":
                token_data = {"username": "test_user"}
                logger.info("ğŸ” ä½¿ç”¨æµ‹è¯•token")
            
            if not token_data:
                logger.info("âŒ tokenéªŒè¯å¤±è´¥")
                return None
            
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„ç”¨æˆ·ç®¡ç†é€»è¾‘æ¥è·å–ç”¨æˆ·ä¿¡æ¯
            # æš‚æ—¶è¿”å›ä¸€ä¸ªç®€å•çš„ç”¨æˆ·å¯¹è±¡
            class SimpleUser:
                def __init__(self, username):
                    self.id = username  # ä½¿ç”¨ç”¨æˆ·åä½œä¸ºID
                    self.username = username
            
            username = token_data.get("username") if isinstance(token_data, dict) else token_data.username
            user = SimpleUser(username)
            logger.info(f"âœ… æˆåŠŸè·å–ç”¨æˆ·: {user.username}")
            return user
        else:
            # å¦‚æœæ˜¯ChatRequestå¯¹è±¡ï¼Œæš‚æ—¶è¿”å›Noneï¼ˆéœ€è¦ä»å…¶ä»–åœ°æ–¹è·å–è®¤è¯ä¿¡æ¯ï¼‰
            logger.info("âŒ è¯·æ±‚å¯¹è±¡æ²¡æœ‰headerså±æ€§ï¼Œæ— æ³•è·å–è®¤è¯ä¿¡æ¯")
            return None
    except Exception as e:
        logger.error(f"è·å–ç”¨æˆ·ä¿¡æ¯å¤±è´¥: {str(e)}", exc_info=True)
        return None

# OSS é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
OSS_ACCESS_KEY_ID = os.getenv("OSS_ACCESS_KEY_ID", "")
OSS_ACCESS_KEY_SECRET = os.getenv("OSS_ACCESS_KEY_SECRET", "")
OSS_ENDPOINT = os.getenv("OSS_ENDPOINT", "https://oss-cn-hangzhou.aliyuncs.com")
OSS_BUCKET = os.getenv("OSS_BUCKET", "")

# æœ¬åœ°ä¸´æ—¶è·¯å¾„
LOCAL_DIR = "/home/user/ustcchat/oss"
os.makedirs(LOCAL_DIR, exist_ok=True)

# ======================
# åˆ›å»ºè·¯ç”±
# ======================
kb_router = APIRouter(prefix="/kb", tags=["çŸ¥è¯†åº“ç®¡ç†"])
agent_router = APIRouter(prefix="/agent", tags=["å¯¹è¯Agent"])

# ======================
# çŸ¥è¯†åº“åç§°æ˜ å°„é…ç½®
# ======================
KNOWLEDGE_BASE_NAME_MAPPING = {
    "nsrl_tech_docs": "NSRLæŠ€æœ¯æ–‡æ¡£åº“",
    "test": "æµ‹è¯•çŸ¥è¯†åº“",
    "default": "é»˜è®¤çŸ¥è¯†åº“"
}

def get_display_name(technical_name: str) -> str:
    """è·å–çŸ¥è¯†åº“çš„æ˜¾ç¤ºåç§°"""
    return KNOWLEDGE_BASE_NAME_MAPPING.get(technical_name, technical_name)

def get_technical_name(display_name: str) -> str:
    """æ ¹æ®æ˜¾ç¤ºåç§°è·å–æŠ€æœ¯åç§°"""
    for tech_name, disp_name in KNOWLEDGE_BASE_NAME_MAPPING.items():
        if disp_name == display_name:
            return tech_name
    return display_name

# ======================
# å…±äº«å·¥å…·å‡½æ•°
# ======================
def get_current_knowledge_base_info(kb_name: str):
    """åªè·å–æŒ‡å®šçŸ¥è¯†åº“çš„æ–‡æ¡£ä¿¡æ¯ï¼ˆä¸åŒ…å«å…¶ä»–çŸ¥è¯†åº“ï¼‰"""
    global qdrant_client
    try:
        # ä½¿ç”¨å…¨å±€Qdrantå®¢æˆ·ç«¯
        if qdrant_client is None:
            qdrant_client = QdrantClient(host="localhost", port=6333)
        # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
        try:
            qdrant_client.get_collection(kb_name)
        except:
            return {
                "name": kb_name,
                "exists": False,
                "points_count": 0,
                "documents": [],
                "document_count": 0
            }
        # è·å–çŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰æ–‡æ¡£å—ï¼ˆåˆ†é¡µè·å–æ‰€æœ‰æ•°æ®ï¼‰
        all_points = []
        offset = None
        while True:
            # ä½¿ç”¨scroll APIè·å–æ•°æ®
            points, next_offset = qdrant_client.scroll(
                collection_name=kb_name,
                limit=100,
                with_payload=True,
                with_vectors=False,
                offset=offset
            )
            all_points.extend(points)
            if not next_offset:
                break
            offset = next_offset
        # æå–å”¯ä¸€æ–‡æ¡£å
        document_names = set()
        for point in all_points:
            try:
                # å°è¯•è®¿é—®sourceå­—æ®µ
                if "metadata" in point.payload and "source" in point.payload["metadata"]:
                    document_names.add(point.payload["metadata"]["source"])
            except Exception as e:
                logger.warning(f"å¤„ç†ç‚¹æ—¶å‡ºé”™: {str(e)}")
        return {
            "name": kb_name,
            "display_name": get_display_name(kb_name),
            "exists": True,
            "points_count": len(all_points),
            "documents": list(document_names),
            "document_count": len(document_names)
        }
    except Exception as e:
        logger.error(f"è·å–çŸ¥è¯†åº“ä¿¡æ¯å¤±è´¥: {str(e)}", exc_info=True)
        return {
            "name": kb_name,
            "display_name": get_display_name(kb_name),
            "exists": False,
            "points_count": 0,
            "documents": [],
            "document_count": 0
        }

def download_pdf_from_oss(file_key, local_path):
    """ä»OSSä¸‹è½½æ–‡ä»¶"""
    try:
        # åˆå§‹åŒ–OSSå®¢æˆ·ç«¯
        auth = Auth(OSS_ACCESS_KEY_ID, OSS_ACCESS_KEY_SECRET)
        bucket = Bucket(auth, OSS_ENDPOINT, OSS_BUCKET)
        bucket.get_object_to_file(file_key, local_path)
        logger.info(f"æ–‡ä»¶ä¸‹è½½æˆåŠŸ: {local_path}")
        return True
    except Exception as e:
        logger.error(f"æ–‡ä»¶ä¸‹è½½å¤±è´¥: {str(e)}")
        return False

def get_rag_tool(knowledge_base_name: str):
    """æ ¹æ®çŸ¥è¯†åº“åç§°è·å–æˆ–åˆ›å»ºRAGå·¥å…·"""
    if knowledge_base_name in rag_tool_cache:
        return rag_tool_cache[knowledge_base_name]
    # åˆ›å»ºæ–°çš„RAGå·¥å…·å®ä¾‹
    from rag_tool import create_rag_tool
    rag_tool = create_rag_tool(
        host="localhost",
        port=6333,
        collection_name=knowledge_base_name
    )
    rag_tool_cache[knowledge_base_name] = rag_tool
    return rag_tool

class SecurityHeadersMiddleware(BaseHTTPMiddleware):
    """å®‰å…¨å“åº”å¤´ä¸­é—´ä»¶ï¼Œé˜²æ­¢HTTPå“åº”å¤´æ³¨å…¥æ”»å‡»"""
    
    async def dispatch(self, request: Request, call_next):
        response = await call_next(request)
        
        # æ¸…ç†æ‰€æœ‰å“åº”å¤´ï¼Œç§»é™¤å¯èƒ½çš„CRLFæ³¨å…¥å­—ç¬¦
        cleaned_headers = {}
        for key, value in response.headers.items():
            # æ¸…ç†é”®å
            clean_key = re.sub(r'[\r\n]', '', key)
            # æ¸…ç†å€¼
            clean_value = re.sub(r'[\r\n]', '', str(value))
            cleaned_headers[clean_key] = clean_value
        
        # é‡æ–°è®¾ç½®å“åº”å¤´
        # æ¸…é™¤ç°æœ‰å¤´éƒ¨
        for key in list(response.headers.keys()):
            del response.headers[key]
        # è®¾ç½®æ¸…ç†åçš„å¤´éƒ¨
        for key, value in cleaned_headers.items():
            response.headers[key] = value
            
        return response

def sanitize_filename(filename: str) -> str:
    """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤æ‰€æœ‰å¯èƒ½å¯¼è‡´HTTPå“åº”å¤´æ³¨å…¥çš„å­—ç¬¦"""
    if not filename:
        return f"uploaded_file_{int(time.time())}"
    
    # ç§»é™¤æ§åˆ¶å­—ç¬¦ï¼ˆåŒ…æ‹¬\rã€\nã€\tç­‰ï¼‰
    safe_filename = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', filename)
    # ç§»é™¤å¯èƒ½å¯¼è‡´è·¯å¾„éå†çš„å­—ç¬¦
    safe_filename = safe_filename.replace('..', '_')
    safe_filename = safe_filename.replace('/', '_')
    safe_filename = safe_filename.replace('\\', '_')
    # ç§»é™¤å…¶ä»–ç‰¹æ®Šå­—ç¬¦
    safe_filename = re.sub(r'[<>:"|?*]', '_', safe_filename)
    # ç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œç‚¹
    safe_filename = safe_filename.strip('. ')
    # ç¡®ä¿æ–‡ä»¶åä¸ä¸ºç©º
    if not safe_filename.strip():
        safe_filename = f"uploaded_file_{int(time.time())}"
    
    return safe_filename

def extract_highest_similarity(tool_response: str) -> float:
    """ä»å·¥å…·å“åº”ä¸­æå–æœ€é«˜ç›¸ä¼¼åº¦"""
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾æ‰€æœ‰ç›¸ä¼¼åº¦å€¼
    similarity_values = re.findall(r"ç›¸ä¼¼åº¦: ([\d.]+)", tool_response)
    if not similarity_values:
        logger.warning("æœªåœ¨å·¥å…·å“åº”ä¸­æ‰¾åˆ°ç›¸ä¼¼åº¦ä¿¡æ¯")
        return 0.0
    # è½¬æ¢ä¸ºæµ®ç‚¹æ•°å¹¶è¿”å›æœ€å¤§å€¼
    try:
        similarities = [float(val) for val in similarity_values]
        highest = max(similarities)
        logger.info(f"æ£€æµ‹åˆ°æœ€é«˜ç›¸ä¼¼åº¦: {highest:.4f}")
        return highest
    except ValueError:
        logger.error("æ— æ³•è§£æç›¸ä¼¼åº¦å€¼")
        return 0.0


def process_uploaded_file(file: UploadFile, knowledge_base: str) -> Dict[str, Any]:
    """å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶å¹¶æ·»åŠ åˆ°çŸ¥è¯†åº“"""
    try:
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        temp_dir = tempfile.mkdtemp()
        file_extension = os.path.splitext(file.filename)[1].lower()
        # æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤æ‰€æœ‰å¯èƒ½å¯¼è‡´HTTPå“åº”å¤´æ³¨å…¥çš„å­—ç¬¦
        safe_filename = sanitize_filename(file.filename)
        temp_file_path = os.path.join(temp_dir, safe_filename)
        
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        with open(temp_file_path, "wb") as buffer:
            content = file.file.read()
            buffer.write(content)
        
        logger.info(f"æ–‡ä»¶å·²ä¿å­˜åˆ°ä¸´æ—¶è·¯å¾„: {temp_file_path}")
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹å¤„ç†
        if file_extension in ['.pdf', '.docx', '.ppt', '.pptx', '.xls', '.xlsx']:
            # å…¶ä»–æ ¼å¼ä¼˜å…ˆä½¿ç”¨markerè½¬æ¢å™¨ï¼Œè½»é‡çº§è½¬æ¢å™¨ä½œä¸ºå¤‡ç”¨
            try:
                logger.info(f"ğŸ”„ ä½¿ç”¨markerè½¬æ¢å™¨å¤„ç†æ–‡æ¡£: {file.filename}")
                return process_document_with_marker(temp_file_path, knowledge_base, file.filename)
            except Exception as marker_error:
                logger.warning(f"âš ï¸ markerè½¬æ¢å¤±è´¥ï¼Œå°è¯•è½»é‡çº§è½¬æ¢å™¨: {str(marker_error)}")
                try:
                    logger.info(f"ğŸ”„ å°è¯•ä½¿ç”¨è½»é‡çº§è½¬æ¢å™¨å¤„ç†æ–‡æ¡£: {file.filename}")
                    return process_document_file(temp_file_path, knowledge_base, file.filename)
                except Exception as lightweight_error:
                    logger.error(f"âŒ è½»é‡çº§è½¬æ¢ä¹Ÿå¤±è´¥: {str(lightweight_error)}")
                    return {
                        "success": False,
                        "message": f"æ–‡æ¡£è½¬æ¢å¤±è´¥ã€‚Markeré”™è¯¯: {str(marker_error)}ã€‚è½»é‡çº§è½¬æ¢å™¨é”™è¯¯: {str(lightweight_error)}",
                        "data": {"filename": file.filename}
                    }
        elif file_extension in ['.md', '.markdown']:
            return process_markdown_file(temp_file_path, knowledge_base, file.filename)
        elif file_extension == '.txt':
            return process_text_file(temp_file_path, knowledge_base, file.filename)
        else:
            return {
                "success": False,
                "message": f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_extension}ã€‚æ”¯æŒçš„æ ¼å¼: PDF, Word(.docx), PowerPoint, Excel, Markdown, TXT",
                "data": {"filename": file.filename}
            }
    
    except Exception as e:
        logger.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        return {
            "success": False,
            "message": f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}",
            "data": {"filename": file.filename}
        }
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            if 'temp_file_path' in locals() and os.path.exists(temp_file_path):
                os.remove(temp_file_path)
            if 'temp_dir' in locals() and os.path.exists(temp_dir):
                os.rmdir(temp_dir)
        except Exception as e:
            logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {str(e)}")


def process_document_with_marker(file_path: str, knowledge_base: str, filename: str) -> Dict[str, Any]:
    """ä½¿ç”¨ç‹¬ç«‹markerè¿›ç¨‹å¤„ç†å¤šç§æ–‡æ¡£æ ¼å¼ï¼ˆPDFã€Wordã€PowerPointã€Excelï¼‰"""
    try:
        # è·å–æ–‡ä»¶åï¼ˆå»æ‰è·¯å¾„å’Œæ‰©å±•åï¼‰
        base_name = os.path.splitext(os.path.basename(file_path))[0]
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        output_dir = f'/home/user/ustcchat/ustc/marker_outputs/{base_name}'
        os.makedirs(output_dir, exist_ok=True)
        
        # ä½¿ç”¨ç‹¬ç«‹markerè¿›ç¨‹è½¬æ¢
        logger.info(f"ğŸ”„ ä½¿ç”¨ç‹¬ç«‹markerè¿›ç¨‹è½¬æ¢æ–‡æ¡£: {file_path}")
        
        # ç›´æ¥è°ƒç”¨ç‹¬ç«‹markerè„šæœ¬ï¼Œé¿å…å¯¼å…¥é—®é¢˜
        import subprocess
        import json
        
        result = subprocess.run(
            ['/home/user/miniconda3/envs/langchain/bin/python', 'marker_standalone.py', file_path, output_dir, base_name],
            capture_output=True,
            text=True,
            timeout=300,
            cwd=os.path.dirname(os.path.abspath(__file__))
        )
        
        if result.returncode != 0:
            raise Exception(f"ç‹¬ç«‹markerè¿›ç¨‹æ‰§è¡Œå¤±è´¥: {result.stderr}")
        
        # è§£æç»“æœ
        try:
            result_data = json.loads(result.stdout.strip())
        except json.JSONDecodeError:
            # å¦‚æœæ²¡æœ‰JSONè¾“å‡ºï¼Œæ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†æ–‡ä»¶
            expected_md_file = os.path.join(output_dir, f"{base_name}.md")
            if os.path.exists(expected_md_file):
                with open(expected_md_file, 'r', encoding='utf-8') as f:
                    text = f.read()
                result_data = {
                    "success": True,
                    "text_length": len(text),
                    "md_file": expected_md_file,
                    "method": "marker_standalone"
                }
            else:
                raise Exception(f"ç‹¬ç«‹markerè¿›ç¨‹æ‰§è¡Œå¤±è´¥ï¼Œæ— JSONè¾“å‡ºä¸”æœªç”Ÿæˆæ–‡ä»¶")
        
        if not result_data.get("success"):
            raise Exception(f"Markerè½¬æ¢å¤±è´¥: {result_data.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
        # ç”Ÿæˆçš„markdownæ–‡ä»¶è·¯å¾„
        md_file_path = result_data.get("md_file", os.path.join(output_dir, f"{base_name}.md"))
        
        if not os.path.exists(md_file_path):
            raise Exception(f"markerè½¬æ¢å¤±è´¥ï¼Œæœªç”Ÿæˆmarkdownæ–‡ä»¶ã€‚æœŸæœ›è·¯å¾„: {md_file_path}")
        
        # æ·»åŠ åˆ°çŸ¥è¯†åº“ - ä½¿ç”¨æ–°å‡½æ•°å­˜å‚¨åŸæ–‡ä»¶å†…å®¹
        vector_store = embedding_init(collection_name=knowledge_base)
        operation_info = upsert_md_file_with_original(md_file_path, vector_store)
        
        return {
            "success": True,
            "message": f"æ–‡æ¡£æ–‡ä»¶ {filename} å¤„ç†æˆåŠŸï¼ˆmarkerç‹¬ç«‹è¿›ç¨‹ï¼‰",
            "data": {
                "filename": filename,
                "operation_info": operation_info,
                "converter_result": {
                    "file_path": file_path,
                    "output_dir": output_dir,
                    "text_length": result_data.get("text_length", 0),
                    "method": "marker_standalone"
                },
                "method": "marker_standalone"
            }
        }
        
    except Exception as e:
        logger.error(f"markerç‹¬ç«‹è¿›ç¨‹å¤„ç†æ–‡æ¡£æ–‡ä»¶å¤±è´¥: {str(e)}")
        return {
            "success": False,
            "message": f"markerç‹¬ç«‹è¿›ç¨‹å¤„ç†æ–‡æ¡£æ–‡ä»¶å¤±è´¥: {str(e)}",
            "data": {"filename": filename}
        }


def process_document_file(file_path: str, knowledge_base: str, filename: str) -> Dict[str, Any]:
    """å¤„ç†å¤šç§æ–‡æ¡£æ ¼å¼ï¼ˆPDFã€Wordã€PowerPointã€Excelï¼‰"""
    try:
        # è·å–æ–‡ä»¶åï¼ˆå»æ‰è·¯å¾„å’Œæ‰©å±•åï¼‰
        base_name = os.path.splitext(os.path.basename(file_path))[0]
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        output_dir = f'/home/user/ustcchat/ustc/marker_outputs/{base_name}'
        os.makedirs(output_dir, exist_ok=True)
        
        # ä½¿ç”¨è½»é‡çº§å¤šæ ¼å¼è½¬æ¢å™¨
        logger.info(f"ğŸ”„ ä½¿ç”¨è½»é‡çº§è½¬æ¢å™¨å¤„ç†æ–‡æ¡£: {file_path}")
        from lightweight_marker_converter import convert_with_lightweight_marker
        result = convert_with_lightweight_marker(
            file_path=file_path,
            output_dir=output_dir,
            base_name=base_name
        )
        
        if not result["success"]:
            return {
                "success": False,
                "message": f"æ–‡æ¡£è½¬æ¢å¤±è´¥: {result['message']}",
                "data": {"filename": filename}
            }
        
        # ç”Ÿæˆçš„markdownæ–‡ä»¶è·¯å¾„
        md_file_path = os.path.join(output_dir, f"{base_name}.md")
        
        if not os.path.exists(md_file_path):
            return {
                "success": False,
                "message": f"æ–‡æ¡£è½¬æ¢å¤±è´¥ï¼Œæœªç”Ÿæˆmarkdownæ–‡ä»¶ã€‚æœŸæœ›è·¯å¾„: {md_file_path}",
                "data": {"filename": filename}
            }
        
        # æ·»åŠ åˆ°çŸ¥è¯†åº“ - ä½¿ç”¨æ–°å‡½æ•°å­˜å‚¨åŸæ–‡ä»¶å†…å®¹
        vector_store = embedding_init(collection_name=knowledge_base)
        operation_info = upsert_md_file_with_original(md_file_path, vector_store)
        
        return {
            "success": True,
            "message": f"æ–‡æ¡£æ–‡ä»¶ {filename} å¤„ç†æˆåŠŸ",
            "data": {
                "filename": filename,
                "operation_info": operation_info,
                "converter_result": result["data"]
            }
        }
    
    except Exception as e:
        logger.error(f"å¤„ç†æ–‡æ¡£æ–‡ä»¶å¤±è´¥: {str(e)}")
        return {
            "success": False,
            "message": f"å¤„ç†æ–‡æ¡£æ–‡ä»¶å¤±è´¥: {str(e)}",
            "data": {"filename": filename}
        }


def process_pdf_file(file_path: str, knowledge_base: str, filename: str) -> Dict[str, Any]:
    """å¤„ç†PDFæ–‡ä»¶"""
    try:
        # è·å–æ–‡ä»¶åï¼ˆå»æ‰è·¯å¾„å’Œæ‰©å±•åï¼‰
        base_name = os.path.splitext(os.path.basename(file_path))[0]
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        output_dir = f'/home/user/ustcchat/ustc/marker_outputs/{base_name}'
        os.makedirs(output_dir, exist_ok=True)
        
        # ä½¿ç”¨è½»é‡çº§å¤šæ ¼å¼è½¬æ¢å™¨
        logger.info(f"ğŸ”„ ä½¿ç”¨è½»é‡çº§è½¬æ¢å™¨å¤„ç†æ–‡ä»¶: {file_path}")
        from lightweight_marker_converter import convert_with_lightweight_marker
        result = convert_with_lightweight_marker(
            file_path=file_path,
            output_dir=output_dir,
            base_name=base_name
        )
        
        if not result["success"]:
            return {
                "success": False,
                "message": f"PDFè½¬æ¢å¤±è´¥: {result['message']}",
                "data": {"filename": filename}
            }
        
        # ç”Ÿæˆçš„markdownæ–‡ä»¶è·¯å¾„
        md_file_path = os.path.join(output_dir, f"{base_name}.md")
        
        if not os.path.exists(md_file_path):
            return {
                "success": False,
                "message": f"PDFè½¬æ¢å¤±è´¥ï¼Œæœªç”Ÿæˆmarkdownæ–‡ä»¶ã€‚æœŸæœ›è·¯å¾„: {md_file_path}",
                "data": {"filename": filename}
            }
        
        # æ·»åŠ åˆ°çŸ¥è¯†åº“ - ä½¿ç”¨æ–°å‡½æ•°å­˜å‚¨åŸæ–‡ä»¶å†…å®¹
        vector_store = embedding_init(collection_name=knowledge_base)
        operation_info = upsert_md_file_with_original(md_file_path, vector_store)
        
        return {
            "success": True,
            "message": f"PDFæ–‡ä»¶ {filename} å¤„ç†æˆåŠŸ",
            "data": {
                "filename": filename,
                "operation_info": operation_info,
                "marker_result": result["data"]
            }
        }
    
    except Exception as e:
        logger.error(f"å¤„ç†PDFæ–‡ä»¶å¤±è´¥: {str(e)}")
        return {
            "success": False,
            "message": f"å¤„ç†PDFæ–‡ä»¶å¤±è´¥: {str(e)}",
            "data": {"filename": filename}
        }


def process_markdown_file(file_path: str, knowledge_base: str, filename: str) -> Dict[str, Any]:
    """å¤„ç†Markdownæ–‡ä»¶"""
    try:
        # ç›´æ¥ä½¿ç”¨markdownæ–‡ä»¶
        vector_store = embedding_init(collection_name=knowledge_base)
        operation_info = upsert_md_file_with_original(file_path, vector_store)
        
        return {
            "success": True,
            "message": f"Markdownæ–‡ä»¶ {filename} å¤„ç†æˆåŠŸ",
            "data": {
                "filename": filename,
                "operation_info": operation_info
            }
        }
    
    except Exception as e:
        logger.error(f"å¤„ç†Markdownæ–‡ä»¶å¤±è´¥: {str(e)}")
        return {
            "success": False,
            "message": f"å¤„ç†Markdownæ–‡ä»¶å¤±è´¥: {str(e)}",
            "data": {"filename": filename}
        }


def process_word_file(file_path: str, knowledge_base: str, filename: str) -> Dict[str, Any]:
    """å¤„ç†Wordæ–‡æ¡£"""
    try:
        # è¯»å–Wordæ–‡æ¡£å†…å®¹
        doc = Document(file_path)
        text_content = []
        
        for paragraph in doc.paragraphs:
            if paragraph.text.strip():
                text_content.append(paragraph.text.strip())
        
        # å°†å†…å®¹è½¬æ¢ä¸ºmarkdownæ ¼å¼
        content = "\n\n".join(text_content)
        
        # åˆ›å»ºä¸´æ—¶markdownæ–‡ä»¶
        temp_md_path = file_path.replace('.docx', '.md').replace('.doc', '.md')
        with open(temp_md_path, 'w', encoding='utf-8') as f:
            f.write(f"# {os.path.splitext(filename)[0]}\n\n{content}")
        
        # æ·»åŠ åˆ°çŸ¥è¯†åº“ - ä½¿ç”¨æ–°å‡½æ•°å­˜å‚¨åŸæ–‡ä»¶å†…å®¹
        vector_store = embedding_init(collection_name=knowledge_base)
        operation_info = upsert_md_file_with_original(temp_md_path, vector_store)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.remove(temp_md_path)
        
        return {
            "success": True,
            "message": f"Wordæ–‡æ¡£ {filename} å¤„ç†æˆåŠŸ",
            "data": {
                "filename": filename,
                "operation_info": operation_info
            }
        }
    
    except Exception as e:
        logger.error(f"å¤„ç†Wordæ–‡æ¡£å¤±è´¥: {str(e)}")
        return {
            "success": False,
            "message": f"å¤„ç†Wordæ–‡æ¡£å¤±è´¥: {str(e)}",
            "data": {"filename": filename}
        }


def process_text_file(file_path: str, knowledge_base: str, filename: str) -> Dict[str, Any]:
    """å¤„ç†çº¯æ–‡æœ¬æ–‡ä»¶"""
    try:
        # è¯»å–æ–‡æœ¬å†…å®¹
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        # åˆ›å»ºä¸´æ—¶markdownæ–‡ä»¶ï¼Œä½†ä½¿ç”¨åŸå§‹æ–‡ä»¶å
        temp_md_path = file_path.replace('.txt', '.md')
        with open(temp_md_path, 'w', encoding='utf-8') as f:
            f.write(f"# {os.path.splitext(filename)[0]}\n\n{content}")
        
        # æ·»åŠ åˆ°çŸ¥è¯†åº“ï¼Œä¼ é€’åŸå§‹æ–‡ä»¶å
        vector_store = embedding_init(collection_name=knowledge_base)
        operation_info = upsert_md_file_with_source(temp_md_path, vector_store, filename)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.remove(temp_md_path)
        
        return {
            "success": True,
            "message": f"æ–‡æœ¬æ–‡ä»¶ {filename} å¤„ç†æˆåŠŸ",
            "data": {
                "filename": filename,
                "operation_info": operation_info
            }
        }
    
    except Exception as e:
        logger.error(f"å¤„ç†æ–‡æœ¬æ–‡ä»¶å¤±è´¥: {str(e)}", exc_info=True)
        import traceback
        error_details = traceback.format_exc()
        logger.error(f"TXTæ–‡ä»¶å¤„ç†è¯¦ç»†é”™è¯¯: {error_details}")
        return {
            "success": False,
            "message": f"å¤„ç†æ–‡æœ¬æ–‡ä»¶å¤±è´¥: {str(e)}",
            "data": {
                "filename": filename,
                "error_details": error_details
            }
        }


# ======================
# çŸ¥è¯†åº“ç®¡ç†APIè·¯ç”±
# ======================
# 1. å®šä¹‰APIè¯·æ±‚æ¨¡å‹
class KnowledgeBaseAction(str, Enum):
    CREATE = "create"
    UPLOAD = "upload"
    DELETE_DOCUMENT = "delete_document"
    DELETE = "delete"

class KnowledgeBaseRequest(BaseModel):
    action: KnowledgeBaseAction
    name: str
    document_name: Optional[str] = None
    filename: Optional[str] = None  # æ–°å¢ï¼šç”¨äºé‡å‘½åä¸‹è½½çš„æ–‡ä»¶

# 2. ä¿®å¤KnowledgeBaseResponseå®šä¹‰
class KnowledgeBaseResponse(BaseModel):
    success: bool
    message: str
    data: Optional[Dict[str, Any]] = None

# æ–°å¢ï¼šé—®ç­”å¯¹ä¸Šä¼ è¯·æ±‚æ¨¡å‹
class QAPairRequest(BaseModel):
    knowledge_base_name: str
    question: str
    answer: str
    document_name: Optional[str] = None  # æ–°å¢ï¼šæ–‡æ¡£åï¼Œå¦‚æœä¸æä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆ
    metadata: Optional[Dict[str, Any]] = None  # å¯é€‰çš„å…ƒæ•°æ®ï¼Œå¦‚æ¥æºã€æ ‡ç­¾ç­‰

# æ–°å¢ï¼šçŸ¥è¯†åº“æŸ¥è¯¢è¯·æ±‚æ¨¡å‹
class KnowledgeBaseQueryRequest(BaseModel):
    knowledge_base_name: str
    query: str
    search_type: str = "hybrid"  # "vector", "keyword", "hybrid"
    top_k: Optional[int] = 10
    similarity_threshold: Optional[float] = 0.5
    keyword_match_threshold: Optional[int] = 1

# æ–°å¢ï¼šçŸ¥è¯†åº“æŸ¥è¯¢ç»“æœæ¨¡å‹
class QueryResult(BaseModel):
    content: str
    document_name: str
    title: str
    score: float
    search_type: str
    metadata: Dict[str, Any]
    is_qa_pair: Optional[bool] = None  # æ˜¯å¦ä¸ºé—®ç­”å¯¹
    question: Optional[str] = None  # å¦‚æœæ˜¯QAå¯¹ï¼Œåˆ†ç¦»çš„é—®é¢˜
    answer: Optional[str] = None  # å¦‚æœæ˜¯QAå¯¹ï¼Œåˆ†ç¦»çš„ç­”æ¡ˆ

def query_knowledge_base_sync(knowledge_base_name: str, query: str, search_type: str = "hybrid",
                            top_k: int = 20, similarity_threshold: float = 0.3,
                            keyword_match_threshold: int = 1) -> List[QueryResult]:
    """åŒæ­¥æŸ¥è¯¢çŸ¥è¯†åº“å†…å®¹"""
    try:
        global qdrant_client
        if qdrant_client is None:
            qdrant_client = QdrantClient(host="localhost", port=6333)

        # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
        try:
            qdrant_client.get_collection(knowledge_base_name)
        except:
            logger.error(f"çŸ¥è¯†åº“ '{knowledge_base_name}' ä¸å­˜åœ¨")
            return []

        # ä½¿ç”¨å‘é‡åŒ–æ¨¡å‹è¿›è¡ŒæŸ¥è¯¢
        from chunks2embedding import embedding_init
        vector_store = embedding_init(collection_name=knowledge_base_name)
        search_results = vector_store.weighted_hybrid_search(query=query, k=top_k)

        results = []
        for doc, score in search_results:
            if score < similarity_threshold:
                continue

            # è·å–æ–‡æ¡£å
            document_name = doc.metadata.get("source", "æœªçŸ¥æ–‡æ¡£")
            if document_name.endswith('.md'):
                document_name = document_name[:-3]

            # æ£€æŸ¥æ˜¯å¦ä¸ºé—®ç­”å¯¹
            is_qa_pair = doc.metadata.get("is_qa_pair", False) or doc.metadata.get("type") == "qa"
            question = None
            answer = None

            if is_qa_pair:
                # ä»metadataä¸­æå–é—®é¢˜å’Œç­”æ¡ˆ
                question = doc.metadata.get("question", "")
                answer = doc.metadata.get("answer", "")
                # æˆ–è€…ä»å†…å®¹ä¸­è§£æ
                if not question and "é—®é¢˜ï¼š" in doc.page_content:
                    try:
                        parts = doc.page_content.split("\n\n", 1)
                        if len(parts) >= 1:
                            question_line = parts[0]
                            if question_line.startswith("é—®é¢˜ï¼š"):
                                question = question_line[3:].strip()
                        if len(parts) >= 2:
                            answer_part = parts[1]
                            if answer_part.startswith("ç­”æ¡ˆï¼š"):
                                answer = answer_part[3:].strip()
                    except:
                        pass

            result = QueryResult(
                content=doc.page_content,
                document_name=document_name,
                title=doc.metadata.get("title", "æ— æ ‡é¢˜"),
                score=score,
                search_type="hybrid",
                metadata=doc.metadata,
                is_qa_pair=is_qa_pair,
                question=question,
                answer=answer
            )
            results.append(result)

        logger.info(f"çŸ¥è¯†åº“æŸ¥è¯¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
        return results

    except Exception as e:
        logger.error(f"æŸ¥è¯¢çŸ¥è¯†åº“å¤±è´¥: {str(e)}")
        return []

# 6. ç®€åŒ–åçš„APIç«¯ç‚¹
@kb_router.post("/api/knowledge-base", response_model=KnowledgeBaseResponse)
async def manage_knowledge_base(request: KnowledgeBaseRequest):
    """ç»Ÿä¸€çš„çŸ¥è¯†åº“ç®¡ç†ç«¯ç‚¹ï¼Œåªè¿”å›å½“å‰çŸ¥è¯†åº“ä¿¡æ¯"""
    global qdrant_client
    try:
        if request.action == KnowledgeBaseAction.CREATE:
            # æ£€æŸ¥é›†åˆæ˜¯å¦å·²å­˜åœ¨
            if qdrant_client is None:
                qdrant_client = QdrantClient(host="localhost", port=6333)
            try:
                # å°è¯•è·å–é›†åˆï¼Œå¦‚æœå­˜åœ¨åˆ™è¿”å›exists
                qdrant_client.get_collection(request.name)
                status = "exists"
                message = f"çŸ¥è¯†åº“ '{request.name}' å·²å­˜åœ¨"
            except:
                # é›†åˆä¸å­˜åœ¨ï¼Œåˆ›å»ºå®ƒ
                try:
                    qdrant_client.create_collection(
                        collection_name=request.name,
                        vectors_config={
                            "title": VectorParams(size=896, distance=Distance.COSINE),
                            "content": VectorParams(size=896, distance=Distance.COSINE)
                        }
                    )
                    status = "created"
                    message = f"çŸ¥è¯†åº“ '{request.name}' åˆ›å»ºæˆåŠŸ"
                except Exception as e:
                    logger.error(f"åˆ›å»ºçŸ¥è¯†åº“å¤±è´¥: {str(e)}")
                    return KnowledgeBaseResponse(
                        success=False,
                        message=f"åˆ›å»ºçŸ¥è¯†åº“å¤±è´¥: {str(e)}",
                        data={"name": request.name}
                    )
            # ä»…è·å–å½“å‰çŸ¥è¯†åº“çš„ä¿¡æ¯
            current_kb = get_current_knowledge_base_info(request.name)
            return KnowledgeBaseResponse(
                success=True,
                message=message,
                data={
                    "name": request.name,
                    "status": status,
                    "document_count": current_kb["document_count"],
                    "points_count": current_kb["points_count"],
                    "documents": current_kb["documents"]
                }
            )
        elif request.action == KnowledgeBaseAction.DELETE:
            try:
                if qdrant_client is None:
                    qdrant_client = QdrantClient(host="localhost", port=6333)
                qdrant_client.delete_collection(request.name)
                message = f"çŸ¥è¯†åº“ '{request.name}' å·²åˆ é™¤"
                return KnowledgeBaseResponse(
                    success=True,
                    message=message,
                    data={
                        "name": request.name,
                        "document_count": 0,
                        "points_count": 0,
                        "documents": []
                    }
                )
            except Exception as e:
                logger.error(f"åˆ é™¤çŸ¥è¯†åº“å¤±è´¥: {str(e)}")
                return KnowledgeBaseResponse(
                    success=False,
                    message=f"åˆ é™¤çŸ¥è¯†åº“å¤±è´¥: {str(e)}",
                    data={"name": request.name}
                )
        elif request.action == KnowledgeBaseAction.UPLOAD:
            if not request.document_name:
                return KnowledgeBaseResponse(
                    success=False,
                    message="ä¸Šä¼ æ–‡æ¡£éœ€è¦æä¾› document_name",
                    data={"name": request.name}
                )
            # ä¸‹è½½æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
            oss_path = f"knowledge-documents/{request.name}/{request.document_name}.pdf"
            try:
                # ç”Ÿæˆä¸´æ—¶æ–‡ä»¶å
                orginal_file_name = f"{request.document_name}.pdf"
                local_input = os.path.join(LOCAL_DIR, orginal_file_name)
                if not download_pdf_from_oss(oss_path, local_input):
                    return KnowledgeBaseResponse(
                        success=False,
                        message="ä¸‹è½½æ–‡ä»¶å¤±è´¥",
                        data={"name": request.name}
                    )
                
                # å¦‚æœæä¾›äº†filenameå‚æ•°ï¼Œé‡å‘½åæ–‡ä»¶
                if request.filename:
                    # ç¡®ä¿filenameæœ‰.pdfæ‰©å±•å
                    if not request.filename.endswith('.pdf'):
                        request.filename = f"{request.filename}.pdf"
                    
                    # ç”Ÿæˆæ–°çš„æ–‡ä»¶è·¯å¾„
                    renamed_file_path = os.path.join(LOCAL_DIR, request.filename)
                    
                    # é‡å‘½åæ–‡ä»¶
                    try:
                        os.rename(local_input, renamed_file_path)
                        local_input = renamed_file_path
                        logger.info(f"æ–‡ä»¶å·²é‡å‘½å: {orginal_file_name} -> {request.filename}")
                        chat_logger.info(f"ğŸ“ æ–‡ä»¶é‡å‘½å: {orginal_file_name} -> {request.filename}")
                    except Exception as e:
                        logger.error(f"æ–‡ä»¶é‡å‘½åå¤±è´¥: {str(e)}")
                        chat_logger.error(f"âŒ æ–‡ä»¶é‡å‘½åå¤±è´¥: {str(e)}")
                        # é‡å‘½åå¤±è´¥ä¸å½±å“åç»­å¤„ç†ï¼Œç»§ç»­ä½¿ç”¨åŸæ–‡ä»¶å
                
                # =============== æ˜¾å­˜ä¼˜åŒ–ï¼šä½¿ç”¨OCRæ¨¡å‹ ===============
                logger.info("å¼€å§‹ä½¿ç”¨OCRæ¨¡å‹å¤„ç†PDFæ–‡ä»¶...")
                await gpu_resource_manager.acquire("ocr")
                try:
                    # ä¸‹è½½æ–‡ä»¶
                    pdf2md_func = gpu_resource_manager.get_ocr_model()
                    pdf2md_func(local_input)
                finally:
                    await gpu_resource_manager.release()
                logger.info("OCRæ¨¡å‹å¤„ç†å®Œæˆï¼Œèµ„æºå·²é‡Šæ”¾")
                
                # ç¡®å®šç”¨äºç”Ÿæˆmdæ–‡ä»¶è·¯å¾„çš„æ–‡ä»¶å
                # å¦‚æœæä¾›äº†filenameï¼Œä½¿ç”¨filenameï¼ˆå»æ‰.pdfæ‰©å±•åï¼‰
                # å¦åˆ™ä½¿ç”¨document_name
                if request.filename:
                    # å»æ‰.pdfæ‰©å±•å
                    base_filename = request.filename.replace('.pdf', '')
                    tempfile = f'/home/user/ustcchat/ustc/marker_outputs/{base_filename}/{base_filename}.md'
                    logger.info(f"ä½¿ç”¨é‡å‘½ååçš„æ–‡ä»¶åç”Ÿæˆmdè·¯å¾„: {tempfile}")
                    chat_logger.info(f"ğŸ“„ ä½¿ç”¨é‡å‘½ååçš„æ–‡ä»¶åç”Ÿæˆmdè·¯å¾„: {tempfile}")
                else:
                    tempfile = f'/home/user/ustcchat/ustc/marker_outputs/{request.document_name}/{request.document_name}.md'
                    logger.info(f"ä½¿ç”¨åŸå§‹æ–‡ä»¶åç”Ÿæˆmdè·¯å¾„: {tempfile}")
                    chat_logger.info(f"ğŸ“„ ä½¿ç”¨åŸå§‹æ–‡ä»¶åç”Ÿæˆmdè·¯å¾„: {tempfile}")
                
                # =============== æ˜¾å­˜ä¼˜åŒ–ï¼šä½¿ç”¨å‘é‡åŒ–æ¨¡å‹ ===============
                logger.info("å¼€å§‹ä½¿ç”¨å‘é‡åŒ–æ¨¡å‹å¤„ç†æ–‡æ¡£...")
                # ç›´æ¥ä½¿ç”¨embeddingæ¨¡å‹ï¼Œä¸éœ€è¦GPUèµ„æºç®¡ç†
                from chunks2embedding import embedding_init
                vector_store = embedding_init(collection_name=request.name)
                operation_info = upsert_md_file(tempfile, vector_store)
                logger.info("å‘é‡åŒ–æ¨¡å‹å¤„ç†å®Œæˆ")
                
                # ä»…è·å–å½“å‰çŸ¥è¯†åº“çš„ä¿¡æ¯
                current_kb = get_current_knowledge_base_info(request.name)
                return KnowledgeBaseResponse(
                    success=True,
                    message=f"æ–‡æ¡£ '{request.document_name}' å·²ä¸Šä¼ åˆ°çŸ¥è¯†åº“ '{request.name}'",
                    data={
                        "name": request.name,
                        "document": request.document_name,
                        "details": str(operation_info),
                        "document_count": current_kb["document_count"],
                        "points_count": current_kb["points_count"],
                        "documents": current_kb["documents"]
                    }
                )
            finally:
                if os.path.exists(local_input):
                    os.remove(local_input)
                # é¢å¤–æ¸…ç†ç¡®ä¿é‡Šæ”¾æ‰€æœ‰èµ„æº
                gpu_resource_manager.clear_gpu_memory()
        elif request.action == KnowledgeBaseAction.DELETE_DOCUMENT:
            if not request.document_name:
                return KnowledgeBaseResponse(
                    success=False,
                    message="åˆ é™¤æ–‡æ¡£éœ€è¦æä¾› document_name",
                    data={"name": request.name}
                )
            
            # ç›´æ¥è°ƒç”¨æ‚¨å·²æœ‰çš„å‡½æ•°
            # ä¸å†è‡ªåŠ¨æ·»åŠ .mdåç¼€ï¼Œå› ä¸ºQAå¯¹å·²ç»åŒ…å«äº†.mdåç¼€
            deletename = request.document_name
            
            # =============== æ˜¾å­˜ä¼˜åŒ–ï¼šä½¿ç”¨å‘é‡åŒ–æ¨¡å‹ ===============
            logger.info("å¼€å§‹ä½¿ç”¨å‘é‡åŒ–æ¨¡å‹åˆ é™¤æ–‡æ¡£...")
            # ç›´æ¥ä½¿ç”¨embeddingæ¨¡å‹ï¼Œä¸éœ€è¦GPUèµ„æºç®¡ç†
            from chunks2embedding import embedding_init
            vector_store = embedding_init(collection_name=request.name)
            operation_info = delete_by_source(deletename, vector_store)
            logger.info("å‘é‡åŒ–æ¨¡å‹æ“ä½œå®Œæˆ")
            
            # ä»…è·å–å½“å‰çŸ¥è¯†åº“çš„ä¿¡æ¯
            current_kb = get_current_knowledge_base_info(request.name)
            return KnowledgeBaseResponse(
                success=True,
                message=f"æ–‡æ¡£ '{request.document_name}' å·²ä»çŸ¥è¯†åº“ '{request.name}' ä¸­åˆ é™¤",
                data={
                    "name": request.name,
                    "document": request.document_name,
                    "details": str(operation_info),
                    "document_count": current_kb["document_count"],
                    "points_count": current_kb["points_count"],
                    "documents": current_kb["documents"]
                }
            )
    except Exception as e:
        logger.error(f"å¤„ç†è¯·æ±‚å¤±è´¥: {str(e)}", exc_info=True)
        return KnowledgeBaseResponse(
            success=False,
            message=f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}",
            data={"action": request.action, "name": request.name}
        )

# 7. ç®€åŒ–å…¶ä»–ç«¯ç‚¹
@kb_router.get("/api/knowledge-bases", response_model=KnowledgeBaseResponse)
async def list_knowledge_bases():
    """åˆ—å‡ºæ‰€æœ‰çŸ¥è¯†åº“ - ä¿æŒä¸å˜ï¼Œå› ä¸ºè¿™æ˜¯å¦ä¸€ä¸ªåŠŸèƒ½"""
    try:
        global qdrant_client
        if qdrant_client is None:
            qdrant_client = QdrantClient(host="localhost", port=6333)
        collections = qdrant_client.get_collections().collections
        kb_list = []
        for collection in collections:
            kb_info = get_current_knowledge_base_info(collection.name)
            kb_list.append({
                "name": collection.name,
                "display_name": kb_info["display_name"],
                "points_count": kb_info["points_count"],
                "document_count": kb_info["document_count"]
            })
        return KnowledgeBaseResponse(
            success=True,
            message="çŸ¥è¯†åº“åˆ—è¡¨è·å–æˆåŠŸ",
            data={
                "knowledge_bases": kb_list,
                "total": len(kb_list)
            }
        )
    except Exception as e:
        logger.error(f"è·å–çŸ¥è¯†åº“åˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)
        return KnowledgeBaseResponse(
            success=False,
            message=f"è·å–çŸ¥è¯†åº“åˆ—è¡¨å¤±è´¥: {str(e)}",
            data={}
        )

@kb_router.get("/api/knowledge-base/{kb_name}/documents", response_model=KnowledgeBaseResponse)
async def list_documents(kb_name: str):
    """åˆ—å‡ºçŸ¥è¯†åº“ä¸­çš„æ–‡æ¡£ - ä¿æŒä¸å˜"""
    try:
        # ä»…è·å–å½“å‰çŸ¥è¯†åº“çš„ä¿¡æ¯
        current_kb = get_current_knowledge_base_info(kb_name)
        if not current_kb["exists"]:
            return KnowledgeBaseResponse(
                success=False,
                message=f"çŸ¥è¯†åº“ '{kb_name}' ä¸å­˜åœ¨",
                data={"name": kb_name}
            )
        return KnowledgeBaseResponse(
            success=True,
            message=f"çŸ¥è¯†åº“ '{kb_name}' ä¸­çš„æ–‡æ¡£åˆ—è¡¨",
            data={
                "knowledge_base": kb_name,
                "documents": current_kb["documents"],
                "total": current_kb["document_count"],
                "points_count": current_kb["points_count"]
            }
        )
    except Exception as e:
        logger.error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)
        return KnowledgeBaseResponse(
            success=False,
            message=f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}",
            data={"name": kb_name}
        )

# ======================
# é—®ç­”å¯¹ä¸Šä¼ APIï¼ˆç»Ÿä¸€æ¥å£ï¼‰
# ======================
# è¯´æ˜ï¼š
# 1. å•ä¸ªä¸Šä¼ ï¼šPOST /api/qa-pairï¼Œä¼ å…¥å•ä¸ªQAPairRequestå¯¹è±¡
# 2. æ‰¹é‡ä¸Šä¼ ï¼šPOST /api/qa-pairs/batchï¼Œä¼ å…¥QAPairRequestå¯¹è±¡åˆ—è¡¨
# 3. ä¸¤ä¸ªæ¥å£å†…éƒ¨éƒ½è°ƒç”¨åŒä¸€ä¸ªå¤„ç†é€»è¾‘ï¼Œå®ç°ä»£ç å¤ç”¨
# 4. å•ä¸ªä¸Šä¼ å®é™…ä¸Šæ˜¯æ‰¹é‡ä¸Šä¼ çš„ç‰¹ä¾‹ï¼ˆåˆ—è¡¨é•¿åº¦ä¸º1ï¼‰
# ======================

# åˆ é™¤é‡å¤çš„å•ä¸ªä¸Šä¼ APIç«¯ç‚¹ï¼Œä½¿ç”¨ç»Ÿä¸€çš„æ‰¹é‡æ¥å£

# æ–°å¢ï¼šæ‰¹é‡é—®ç­”å¯¹ä¸Šä¼ APIç«¯ç‚¹ï¼ˆå…¼å®¹å•ä¸ªï¼‰
@kb_router.post("/api/qa-pairs/batch", response_model=KnowledgeBaseResponse)
async def upload_qa_pairs_batch(request: List[QAPairRequest]):
    """æ‰¹é‡ä¸Šä¼ é—®ç­”å¯¹åˆ°æŒ‡å®šçŸ¥è¯†åº“ï¼ˆå…¼å®¹å•ä¸ªä¸Šä¼ ï¼‰"""
    try:
        global qdrant_client
        if not request:
            return KnowledgeBaseResponse(
                success=False,
                message="è¯·æ±‚åˆ—è¡¨ä¸ºç©º",
                data={}
            )
        
        # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªè¯·æ±‚çš„çŸ¥è¯†åº“åç§°ï¼‰
        knowledge_base_name = request[0].knowledge_base_name
        if qdrant_client is None:
            qdrant_client = QdrantClient(host="localhost", port=6333)
        
        try:
            qdrant_client.get_collection(knowledge_base_name)
        except:
            return KnowledgeBaseResponse(
                success=False,
                message=f"çŸ¥è¯†åº“ '{knowledge_base_name}' ä¸å­˜åœ¨",
                data={"name": knowledge_base_name}
            )
        
        # =============== æ˜¾å­˜ä¼˜åŒ–ï¼šä½¿ç”¨å‘é‡åŒ–æ¨¡å‹ ===============
        logger.info(f"å¼€å§‹ä½¿ç”¨å‘é‡åŒ–æ¨¡å‹æ‰¹é‡å¤„ç† {len(request)} ä¸ªé—®ç­”å¯¹...")
        # ç›´æ¥ä½¿ç”¨embeddingæ¨¡å‹ï¼Œä¸éœ€è¦GPUèµ„æºç®¡ç†
        from chunks2embedding import embedding_init
        vector_store = embedding_init(collection_name=knowledge_base_name)
        
        success_count = 0
        failed_count = 0
        failed_items = []
        
        for qa_request in request:
            try:
                # æ„å»ºé—®ç­”å¯¹çš„æ–‡æœ¬å†…å®¹
                qa_content = f"é—®é¢˜ï¼š{qa_request.question}\n\nç­”æ¡ˆï¼š{qa_request.answer}"
                
                # æ„å»ºå…ƒæ•°æ®
                metadata = {
                    "source": "qa_pair",  # ä¿ç•™åŸæœ‰æ ‡è¯†ï¼Œä½†ä¼šè¢«upsert_qa_pairå‡½æ•°è¦†ç›–
                    "type": "qa",
                    "question": qa_request.question,
                    "answer": qa_request.answer,
                    "created_at": str(datetime.datetime.now())
                }
                # å¦‚æœæä¾›äº†æ–‡æ¡£åï¼Œæ·»åŠ åˆ°metadataä¸­
                if qa_request.document_name:
                    metadata["document_name"] = qa_request.document_name
                if qa_request.metadata:
                    metadata.update(qa_request.metadata)
                
                # ä¸Šä¼ é—®ç­”å¯¹
                operation_info = upsert_qa_pair(qa_content, metadata, vector_store)
                success_count += 1
                
            except Exception as e:
                failed_count += 1
                failed_items.append({
                    "question": qa_request.question[:50] + "..." if len(qa_request.question) > 50 else qa_request.question,
                    "error": str(e)
                })
                logger.error(f"å¤„ç†é—®ç­”å¯¹å¤±è´¥: {str(e)}")
        
        logger.info("å‘é‡åŒ–æ¨¡å‹æ‰¹é‡å¤„ç†å®Œæˆ")
        
        # è·å–æ›´æ–°åçš„çŸ¥è¯†åº“ä¿¡æ¯
        current_kb = get_current_knowledge_base_info(knowledge_base_name)
        
        # æ ¹æ®ä¸Šä¼ æ•°é‡è°ƒæ•´è¿”å›æ¶ˆæ¯
        if len(request) == 1:
            message = f"é—®ç­”å¯¹å·²æˆåŠŸæ·»åŠ åˆ°çŸ¥è¯†åº“ '{knowledge_base_name}'"
        else:
            message = f"æ‰¹é‡ä¸Šä¼ å®Œæˆï¼šæˆåŠŸ {success_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ª"
        
        return KnowledgeBaseResponse(
            success=True,
            message=message,
            data={
                "name": knowledge_base_name,
                "total_requested": len(request),
                "success_count": success_count,
                "failed_count": failed_count,
                "failed_items": failed_items if failed_items else None,
                "document_count": current_kb["document_count"],
                "points_count": current_kb["points_count"],
                "documents": current_kb["documents"]
            }
        )
        
    except Exception as e:
        logger.error(f"æ‰¹é‡ä¸Šä¼ é—®ç­”å¯¹å¤±è´¥: {str(e)}", exc_info=True)
        return KnowledgeBaseResponse(
            success=False,
            message=f"æ‰¹é‡ä¸Šä¼ é—®ç­”å¯¹å¤±è´¥: {str(e)}",
            data={}
        )

# æ–°å¢ï¼šç»Ÿä¸€é—®ç­”å¯¹ä¸Šä¼ APIç«¯ç‚¹ï¼ˆæ¨èä½¿ç”¨ï¼‰
@kb_router.post("/api/qa-pair", response_model=KnowledgeBaseResponse)
async def upload_qa_pair_unified(request: QAPairRequest):
    """ç»Ÿä¸€é—®ç­”å¯¹ä¸Šä¼ APIç«¯ç‚¹ï¼ˆå†…éƒ¨è°ƒç”¨æ‰¹é‡æ¥å£ï¼‰"""
    try:
        # å°†å•ä¸ªè¯·æ±‚åŒ…è£…æˆåˆ—è¡¨ï¼Œè°ƒç”¨æ‰¹é‡æ¥å£
        batch_request = [request]
        return await upload_qa_pairs_batch(batch_request)
        
    except Exception as e:
        logger.error(f"ç»Ÿä¸€é—®ç­”å¯¹ä¸Šä¼ å¤±è´¥: {str(e)}", exc_info=True)
        return KnowledgeBaseResponse(
            success=False,
            message=f"ç»Ÿä¸€é—®ç­”å¯¹ä¸Šä¼ å¤±è´¥: {str(e)}",
            data={"name": request.knowledge_base_name}
        )

# æ–°å¢ï¼šMarkdownæ–‡ä»¶ä¸Šä¼ è¯·æ±‚æ¨¡å‹
class MarkdownFileRequest(BaseModel):
    knowledge_base_name: str
    file_name: str
    file_path: str  # ç›´æ¥ä½¿ç”¨æ–‡ä»¶è·¯å¾„ï¼Œè€Œä¸æ˜¯æ–‡ä»¶å†…å®¹

# æ–°å¢ï¼šæ‰¹é‡Markdownæ–‡ä»¶ä¸Šä¼ APIç«¯ç‚¹
@kb_router.post("/api/md-files/batch", response_model=KnowledgeBaseResponse)
async def upload_md_files_batch(request: List[MarkdownFileRequest]):
    """æ‰¹é‡ä¸Šä¼ Markdownæ–‡ä»¶åˆ°æŒ‡å®šçŸ¥è¯†åº“ï¼ˆå…¼å®¹å•ä¸ªä¸Šä¼ ï¼‰"""
    try:
        global qdrant_client
        if not request:
            return KnowledgeBaseResponse(
                success=False,
                message="è¯·æ±‚åˆ—è¡¨ä¸èƒ½ä¸ºç©º",
                data={}
            )
        
        # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªè¯·æ±‚çš„çŸ¥è¯†åº“åç§°ï¼‰
        knowledge_base_name = request[0].knowledge_base_name
        if qdrant_client is None:
            qdrant_client = QdrantClient(host="localhost", port=6333)
        
        try:
            qdrant_client.get_collection(knowledge_base_name)
        except:
            return KnowledgeBaseResponse(
                success=False,
                message=f"çŸ¥è¯†åº“ '{knowledge_base_name}' ä¸å­˜åœ¨",
                data={}
            )
        
        # ä½¿ç”¨å‘é‡åŒ–æ¨¡å‹å¤„ç†æ–‡æ¡£
        # ç›´æ¥ä½¿ç”¨embeddingæ¨¡å‹ï¼Œä¸éœ€è¦GPUèµ„æºç®¡ç†
        from chunks2embedding import embedding_init
        vector_store = embedding_init(collection_name=knowledge_base_name)
        
        uploaded_files = []
        for md_request in request:
            try:
                # ç¡®ä¿æ–‡ä»¶åä»¥.mdç»“å°¾
                filename = md_request.file_name
                if not filename.endswith('.md'):
                    filename = f"{filename}.md"
                
                # æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦å­˜åœ¨
                if not os.path.exists(md_request.file_path):
                    raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {md_request.file_path}")
                
                # ç›´æ¥ä½¿ç”¨æ–‡ä»¶è·¯å¾„ä¸Šä¼ åˆ°å‘é‡æ•°æ®åº“
                operation_info = upsert_md_file(md_request.file_path, vector_store)
                
                uploaded_files.append({
                    "file_name": filename,
                    "file_path": md_request.file_path,
                    "operation_info": str(operation_info)
                })
                
            except Exception as e:
                logger.error(f"ä¸Šä¼ æ–‡ä»¶ {md_request.file_name} å¤±è´¥: {str(e)}")
                uploaded_files.append({
                    "file_name": md_request.file_name,
                    "file_path": md_request.file_path,
                    "error": str(e)
                })
        
        return KnowledgeBaseResponse(
            success=True,
            message=f"æˆåŠŸä¸Šä¼  {len(uploaded_files)} ä¸ªMarkdownæ–‡ä»¶åˆ°çŸ¥è¯†åº“ '{knowledge_base_name}'",
            data={
                "knowledge_base_name": knowledge_base_name,
                "uploaded_files": uploaded_files,
                "total_files": len(request)
            }
        )
        
    except Exception as e:
        logger.error(f"æ‰¹é‡Markdownæ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}", exc_info=True)
        return KnowledgeBaseResponse(
            success=False,
            message=f"æ‰¹é‡Markdownæ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}",
            data={}
        )

# æ–°å¢ï¼šå•ä¸ªMarkdownæ–‡ä»¶ä¸Šä¼ APIç«¯ç‚¹
@kb_router.post("/api/md-file", response_model=KnowledgeBaseResponse)
async def upload_md_file_unified(request: MarkdownFileRequest):
    """ç»Ÿä¸€Markdownæ–‡ä»¶ä¸Šä¼ APIç«¯ç‚¹ï¼ˆå†…éƒ¨è°ƒç”¨æ‰¹é‡æ¥å£ï¼‰"""
    try:
        # å°†å•ä¸ªè¯·æ±‚åŒ…è£…æˆåˆ—è¡¨ï¼Œè°ƒç”¨æ‰¹é‡æ¥å£
        batch_request = [request]
        return await upload_md_files_batch(batch_request)
        
    except Exception as e:
        logger.error(f"Markdownæ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}", exc_info=True)
        return KnowledgeBaseResponse(
            success=False,
            message=f"Markdownæ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}",
            data={}
        )

# ======================
# æ–‡ä»¶ä¸Šä¼ API
# ======================
@kb_router.post("/api/upload-file")
async def upload_file(
    request: Request,
    file: UploadFile = File(...),
    knowledge_base: str = Form(...),
    current_user: UserResponse = Depends(get_current_contributor_user)
):
    """ä¸Šä¼ æ–‡ä»¶åˆ°çŸ¥è¯†åº“ï¼ˆæ”¯æŒPDFã€MDã€Wordã€TXTç­‰æ ¼å¼ï¼‰"""
    try:
        # æ¸…ç†æ–‡ä»¶åï¼Œé˜²æ­¢HTTPå“åº”å¤´æ³¨å…¥æ”»å‡»
        original_filename = file.filename
        safe_filename = sanitize_filename(file.filename)
        
        # è®°å½•è¯¦ç»†çš„è¯·æ±‚ä¿¡æ¯ç”¨äºè°ƒè¯•
        logger.info(f"æ–‡ä»¶ä¸Šä¼ è¯·æ±‚è¯¦æƒ…:")
        logger.info(f"  - åŸå§‹æ–‡ä»¶å: {original_filename}")
        logger.info(f"  - æ¸…ç†åæ–‡ä»¶å: {safe_filename}")
        logger.info(f"  - æ–‡ä»¶å¤§å°: {file.size if hasattr(file, 'size') else 'unknown'}")
        logger.info(f"  - æ–‡ä»¶ç±»å‹: {file.content_type}")
        logger.info(f"  - çŸ¥è¯†åº“: {knowledge_base}")
        logger.info(f"  - è¯·æ±‚URL: {request.url}")
        logger.info(f"  - è¯·æ±‚å¤´: {dict(request.headers)}")
        logger.info(f"  - å®¢æˆ·ç«¯IP: {request.client.host if request.client else 'unknown'}")
        
        # éªŒè¯æ–‡ä»¶ç±»å‹ï¼ˆä½¿ç”¨åŸå§‹æ–‡ä»¶åè¿›è¡Œæ‰©å±•åæ£€æŸ¥ï¼‰
        allowed_extensions = ['.pdf', '.md', '.markdown', '.docx', '.txt']
        file_extension = os.path.splitext(original_filename)[1].lower()
        
        if file_extension not in allowed_extensions:
            return {
                "success": False,
                "message": f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_extension}ã€‚æ”¯æŒçš„æ ¼å¼: {', '.join(allowed_extensions)}",
                "data": {"filename": safe_filename}
            }
        
        # éªŒè¯çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
        try:
            collections = qdrant_client.get_collections()
            collection_names = [col.name for col in collections.collections]
            if knowledge_base not in collection_names:
                return {
                    "success": False,
                    "message": f"çŸ¥è¯†åº“ '{knowledge_base}' ä¸å­˜åœ¨",
                    "data": {"filename": safe_filename}
                }
        except Exception as e:
            logger.error(f"éªŒè¯çŸ¥è¯†åº“å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "message": f"éªŒè¯çŸ¥è¯†åº“å¤±è´¥: {str(e)}",
                "data": {"filename": safe_filename}
            }
        
        # å¤„ç†æ–‡ä»¶
        result = process_uploaded_file(file, knowledge_base)
        
        if result["success"]:
            logger.info(f"æ–‡ä»¶ {safe_filename} ä¸Šä¼ æˆåŠŸ")
        else:
            logger.error(f"æ–‡ä»¶ {safe_filename} ä¸Šä¼ å¤±è´¥: {result['message']}")
        
        return result
        
    except Exception as e:
        logger.error(f"æ–‡ä»¶ä¸Šä¼ APIå‡ºé”™: {str(e)}", exc_info=True)
        import traceback
        error_details = traceback.format_exc()
        logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {error_details}")
        return {
            "success": False,
            "message": f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}",
            "data": {
                "filename": safe_filename if 'safe_filename' in locals() else "unknown",
                "error_details": error_details
            }
        }


# ======================
# åˆ é™¤APIï¼ˆç»Ÿä¸€æ¥å£ï¼‰
# ======================
# è¯´æ˜ï¼š
# 1. å•ä¸ªåˆ é™¤ï¼šPOST /api/deleteï¼Œä¼ å…¥å•ä¸ªDeleteRequestå¯¹è±¡
# 2. æ‰¹é‡åˆ é™¤ï¼šPOST /api/delete/batchï¼Œä¼ å…¥DeleteRequestå¯¹è±¡åˆ—è¡¨
# 3. ä¸¤ä¸ªæ¥å£å†…éƒ¨éƒ½è°ƒç”¨åŒä¸€ä¸ªå¤„ç†é€»è¾‘ï¼Œå®ç°ä»£ç å¤ç”¨
# 4. æ”¯æŒåˆ é™¤æ–‡æ¡£å’Œé—®ç­”å¯¹
# ======================

# åˆ é™¤è¯·æ±‚æ¨¡å‹
class DeleteRequest(BaseModel):
    knowledge_base_name: str
    document_name: str  # è¦åˆ é™¤çš„æ–‡æ¡£åæˆ–é—®ç­”å¯¹å
    delete_type: str = "document"  # "document" æˆ– "qa_pair"ï¼Œé»˜è®¤ä¸ºæ–‡æ¡£

# ç»Ÿä¸€åˆ é™¤APIç«¯ç‚¹ï¼ˆå…¼å®¹å•ä¸ªå’Œæ‰¹é‡ï¼‰
@kb_router.post("/api/delete/batch", response_model=KnowledgeBaseResponse)
async def delete_items_batch(request: List[DeleteRequest]):
    """æ‰¹é‡åˆ é™¤æ–‡æ¡£æˆ–é—®ç­”å¯¹ï¼ˆå…¼å®¹å•ä¸ªåˆ é™¤ï¼‰"""
    try:
        global qdrant_client
        if not request:
            return KnowledgeBaseResponse(
                success=False,
                message="è¯·æ±‚åˆ—è¡¨ä¸ºç©º",
                data={}
            )
        
        # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªè¯·æ±‚çš„çŸ¥è¯†åº“åç§°ï¼‰
        knowledge_base_name = request[0].knowledge_base_name
        if qdrant_client is None:
            qdrant_client = QdrantClient(host="localhost", port=6333)
        
        try:
            qdrant_client.get_collection(knowledge_base_name)
        except:
            return KnowledgeBaseResponse(
                success=False,
                message=f"çŸ¥è¯†åº“ '{knowledge_base_name}' ä¸å­˜åœ¨",
                data={"name": knowledge_base_name}
            )
        
        # =============== æ˜¾å­˜ä¼˜åŒ–ï¼šä½¿ç”¨å‘é‡åŒ–æ¨¡å‹ ===============
        logger.info(f"å¼€å§‹ä½¿ç”¨å‘é‡åŒ–æ¨¡å‹æ‰¹é‡åˆ é™¤ {len(request)} ä¸ªé¡¹ç›®...")
        # ç›´æ¥ä½¿ç”¨embeddingæ¨¡å‹ï¼Œä¸éœ€è¦GPUèµ„æºç®¡ç†
        from chunks2embedding import embedding_init
        vector_store = embedding_init(collection_name=knowledge_base_name)
        
        success_count = 0
        failed_count = 0
        failed_items = []
        deleted_items = []
        
        for delete_request in request:
            try:
                # æ ¹æ®åˆ é™¤ç±»å‹å¤„ç†
                if delete_request.delete_type == "qa_pair":
                    # åˆ é™¤é—®ç­”å¯¹ï¼šç›´æ¥ä½¿ç”¨æ–‡æ¡£åä½œä¸ºsource
                    document_name = delete_request.document_name
                    if not document_name.endswith('.md'):
                        document_name = f"{document_name}.md"
                    
                    operation_info = delete_by_source(document_name, vector_store)
                    deleted_items.append({
                        "document_name": document_name,
                        "type": "qa_pair",
                        "operation_info": str(operation_info)
                    })
                else:
                    # åˆ é™¤æ–‡æ¡£ï¼šç›´æ¥ä½¿ç”¨æ–‡æ¡£åä½œä¸ºsource
                    document_name = delete_request.document_name
                    # ä¸å†è‡ªåŠ¨æ·»åŠ .mdåç¼€ï¼Œå› ä¸ºæ–°æ–‡ä»¶ä½¿ç”¨åŸå§‹æ–‡ä»¶åä½œä¸ºsource
                    
                    operation_info = delete_by_source(document_name, vector_store)
                    deleted_items.append({
                        "document_name": document_name,
                        "type": "document",
                        "operation_info": str(operation_info)
                    })
                
                success_count += 1
                logger.info(f"æˆåŠŸåˆ é™¤: {document_name}")
                
            except Exception as e:
                failed_count += 1
                failed_items.append({
                    "document_name": delete_request.document_name,
                    "type": delete_request.delete_type,
                    "error": str(e)
                })
                logger.error(f"åˆ é™¤å¤±è´¥: {delete_request.document_name}, é”™è¯¯: {str(e)}")
        
        logger.info("å‘é‡åŒ–æ¨¡å‹æ‰¹é‡åˆ é™¤å®Œæˆ")
        
        # è·å–æ›´æ–°åçš„çŸ¥è¯†åº“ä¿¡æ¯
        current_kb = get_current_knowledge_base_info(knowledge_base_name)
        
        # æ ¹æ®åˆ é™¤æ•°é‡è°ƒæ•´è¿”å›æ¶ˆæ¯
        if len(request) == 1:
            if request[0].delete_type == "qa_pair":
                message = f"é—®ç­”å¯¹ '{request[0].document_name}' å·²ä»çŸ¥è¯†åº“ '{knowledge_base_name}' ä¸­åˆ é™¤"
            else:
                message = f"æ–‡æ¡£ '{request[0].document_name}' å·²ä»çŸ¥è¯†åº“ '{knowledge_base_name}' ä¸­åˆ é™¤"
        else:
            message = f"æ‰¹é‡åˆ é™¤å®Œæˆï¼šæˆåŠŸ {success_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ª"
        
        return KnowledgeBaseResponse(
            success=True,
            message=message,
            data={
                "name": knowledge_base_name,
                "total_requested": len(request),
                "success_count": success_count,
                "failed_count": failed_count,
                "failed_items": failed_items if failed_items else None,
                "deleted_items": deleted_items if deleted_items else None,
                "document_count": current_kb["document_count"],
                "points_count": current_kb["points_count"],
                "documents": current_kb["documents"]
            }
        )
        
    except Exception as e:
        logger.error(f"æ‰¹é‡åˆ é™¤å¤±è´¥: {str(e)}", exc_info=True)
        return KnowledgeBaseResponse(
            success=False,
            message=f"æ‰¹é‡åˆ é™¤å¤±è´¥: {str(e)}",
            data={}
        )

# å•ä¸ªåˆ é™¤APIç«¯ç‚¹ï¼ˆæ¨èä½¿ç”¨ï¼‰
@kb_router.post("/api/delete", response_model=KnowledgeBaseResponse)
async def delete_item_unified(request: DeleteRequest):
    """ç»Ÿä¸€åˆ é™¤APIç«¯ç‚¹ï¼ˆå†…éƒ¨è°ƒç”¨æ‰¹é‡æ¥å£ï¼‰"""
    try:
        # å°†å•ä¸ªè¯·æ±‚åŒ…è£…æˆåˆ—è¡¨ï¼Œè°ƒç”¨æ‰¹é‡æ¥å£
        batch_request = [request]
        return await delete_items_batch(batch_request)
        
    except Exception as e:
        logger.error(f"ç»Ÿä¸€åˆ é™¤å¤±è´¥: {str(e)}", exc_info=True)
        return KnowledgeBaseResponse(
            success=False,
            message=f"ç»Ÿä¸€åˆ é™¤å¤±è´¥: {str(e)}",
            data={"name": request.knowledge_base_name}
        )

# æ–°å¢ï¼šçŸ¥è¯†åº“æŸ¥è¯¢APIç«¯ç‚¹
@kb_router.post("/api/query", response_model=KnowledgeBaseResponse)
async def query_knowledge_base(request: KnowledgeBaseQueryRequest):
    """æŸ¥è¯¢çŸ¥è¯†åº“å†…å®¹ï¼Œæ”¯æŒå‘é‡æœç´¢ã€å…³é”®è¯æœç´¢å’Œæ··åˆæœç´¢"""
    try:
        logger.info(f"æ”¶åˆ°çŸ¥è¯†åº“æŸ¥è¯¢è¯·æ±‚: {request.knowledge_base_name}, æŸ¥è¯¢: {request.query}")

        # æ‰§è¡ŒæŸ¥è¯¢
        results = query_knowledge_base_sync(
            knowledge_base_name=request.knowledge_base_name,
            query=request.query,
            search_type=request.search_type,
            top_k=request.top_k,
            similarity_threshold=request.similarity_threshold,
            keyword_match_threshold=request.keyword_match_threshold
        )

        if not results:
            return KnowledgeBaseResponse(
                success=True,
                message=f"æœªæ‰¾åˆ°ç›¸å…³ç»“æœï¼Œå°è¯•è°ƒæ•´æŸ¥è¯¢æ¡ä»¶æˆ–æ£€æŸ¥çŸ¥è¯†åº“å†…å®¹",
                data={
                    "query": request.query,
                    "knowledge_base_name": request.knowledge_base_name,
                    "search_type": request.search_type,
                    "results": [],
                    "total_results": 0
                }
            )

        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        results_data = []
        for result in results:
            result_dict = {
                "content": result.content,
                "document_name": result.document_name,
                "title": result.title,
                "score": result.score,
                "search_type": result.search_type,
                "metadata": result.metadata,
                "is_qa_pair": result.is_qa_pair
            }

            # å¦‚æœæ˜¯é—®ç­”å¯¹ï¼Œæ·»åŠ åˆ†ç¦»çš„é—®é¢˜å’Œç­”æ¡ˆ
            if result.is_qa_pair and result.question and result.answer:
                result_dict["question"] = result.question
                result_dict["answer"] = result.answer

            results_data.append(result_dict)

        return KnowledgeBaseResponse(
            success=True,
            message=f"æŸ¥è¯¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ",
            data={
                "query": request.query,
                "knowledge_base_name": request.knowledge_base_name,
                "search_type": request.search_type,
                "results": results_data,
                "total_results": len(results)
            }
        )

    except Exception as e:
        logger.error(f"æŸ¥è¯¢çŸ¥è¯†åº“å¤±è´¥: {str(e)}", exc_info=True)
        return KnowledgeBaseResponse(
            success=False,
            message=f"æŸ¥è¯¢å¤±è´¥: {str(e)}",
            data={
                "query": request.query,
                "knowledge_base_name": request.knowledge_base_name,
                "search_type": request.search_type
            }
        )

# ======================
# å¯¹è¯Agent APIè·¯ç”±
# ======================
# 3. å®šä¹‰çŠ¶æ€
class AgentState(TypedDict):
    messages: Annotated[list, add]
    tool_call_count: int  # ç§»é™¤ç´¯åŠ æ“ä½œï¼Œæ”¹ä¸ºæ™®é€šintï¼Œæ¯æ¬¡æ–°æ¶ˆæ¯é‡ç½®ä¸º0
    knowledge_base_name: str  # æ–°å¢å­—æ®µï¼Œç”¨äºå­˜å‚¨å½“å‰ä¼šè¯ä½¿ç”¨çš„çŸ¥è¯†åº“åç§°
    user_document_tools: List[str]  # æ–°å¢å­—æ®µï¼Œç”¨äºå­˜å‚¨å½“å‰ä¼šè¯å¯ç”¨çš„ç”¨æˆ·æ–‡æ¡£å·¥å…·åç§°
    web_search_enabled: bool  # æ–°å¢ï¼šè®°å½•webæœç´¢æ˜¯å¦å¯ç”¨

# 4. ä¿®æ”¹æ¨¡å‹è°ƒç”¨èŠ‚ç‚¹
async def call_model(state: AgentState):
    """æ¨¡å‹è‡ªä¸»å†³ç­–æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·ï¼ŒåŒ…å«å‚æ•°éªŒè¯å’ŒçŠ¶æ€æ›´æ–°"""
    messages = state["messages"]
    knowledge_base_name = state.get("knowledge_base_name", "nsrl_tech_docs")
    
    # =============== æ–°å¢ï¼šè¯¦ç»†æ—¥å¿—è®°å½• ===============
    chat_logger.info(f"ğŸ§  æ¨¡å‹å¼€å§‹æ€è€ƒ - æ¶ˆæ¯æ•°é‡: {len(messages)}")
    if messages and isinstance(messages[-1], HumanMessage):
        chat_logger.info(f"ğŸ’­ ç”¨æˆ·é—®é¢˜: {messages[-1].content}")
    # =============== æ–°å¢ç»“æŸ ===============
    
    # æ˜¾å­˜ä¼˜åŒ–ï¼šé™åˆ¶ä¼šè¯é•¿åº¦ï¼Œé˜²æ­¢æ˜¾å­˜ç´¯ç§¯
    if len(messages) > 25:  # å¢åŠ é™åˆ¶åˆ°25æ¡æ¶ˆæ¯
        # ä¿ç•™æœ€æ–°çš„20æ¡æ¶ˆæ¯å’Œç³»ç»Ÿæç¤ºï¼Œç¡®ä¿ä¸ä¸¢å¤±é‡è¦ä¸Šä¸‹æ–‡
        messages = messages[-20:]
        logger.info("âš ï¸ ä¼šè¯å†å²è¿‡é•¿ï¼Œå·²æˆªæ–­ä»¥èŠ‚çœæ˜¾å­˜")
        chat_logger.info(f"âš ï¸ ä¼šè¯å†å²è¿‡é•¿ï¼Œå·²æˆªæ–­è‡³ {len(messages)} æ¡æ¶ˆæ¯")
        chat_logger.info(f"ğŸ“ ä¿ç•™çš„æ¶ˆæ¯ç±»å‹: {[type(msg).__name__ for msg in messages]}")
        chat_logger.info(f"ğŸ“ ä¿ç•™çš„æ¶ˆæ¯å†…å®¹é¢„è§ˆ:")
        for i, msg in enumerate(messages[-5:], 1):  # æ˜¾ç¤ºæœ€å5æ¡æ¶ˆæ¯çš„é¢„è§ˆ
            if hasattr(msg, 'content') and msg.content:
                preview = msg.content[:100] + "..." if len(msg.content) > 100 else msg.content
                chat_logger.info(f"   ğŸ“ æ¶ˆæ¯ {i}: {type(msg).__name__} - {preview}")
    
    # åŠ¨æ€è·å–å½“å‰çŸ¥è¯†åº“çš„RAGå·¥å…·
    rag_tool = get_rag_tool(knowledge_base_name)
    available_tools = [rag_tool]
    
    # è”ç½‘æœç´¢åŠŸèƒ½å·²ç¦ç”¨
    chat_logger.info(f"âš ï¸ è”ç½‘æœç´¢åŠŸèƒ½å·²ç¦ç”¨")
    
    # æ·»åŠ ç”¨æˆ·æ–‡æ¡£å·¥å…·ï¼ˆå¦‚æœæœ‰ï¼‰
    user_document_tools_list = state.get("user_document_tools", [])
    chat_logger.info(f"ğŸ” ç”¨æˆ·æ–‡æ¡£å·¥å…·åˆ—è¡¨: {user_document_tools_list}")
    
    for tool_name in user_document_tools_list:
        tool_info = get_user_document_tool(tool_name)
        chat_logger.info(f"ğŸ” è·å–å·¥å…· {tool_name}: {tool_info}")
        if tool_info and "tool" in tool_info:
            available_tools.append(tool_info["tool"])
            chat_logger.info(f"âœ… æˆåŠŸæ·»åŠ å·¥å…·: {tool_info['tool'].name if hasattr(tool_info['tool'], 'name') else 'æœªçŸ¥åç§°'}")
        else:
            chat_logger.warning(f"âš ï¸ å·¥å…· {tool_name} è·å–å¤±è´¥æˆ–æ ¼å¼ä¸æ­£ç¡®")
    
    chat_logger.info(f"ğŸ”§ æœ€ç»ˆå¯ç”¨å·¥å…·: {[tool.name if hasattr(tool, 'name') else str(tool) for tool in available_tools]}")
    
    # =============== æ˜¾å­˜ä¼˜åŒ–ï¼šè·å–DeepSeek APIæ¨¡å‹ ===============
    logger.info("å‡†å¤‡ä½¿ç”¨DeepSeek APIæ¨¡å‹å¤„ç†å¯¹è¯...")
    chat_logger.info(f"ğŸ¤– è·å–DeepSeek APIæ¨¡å‹...")
    await gpu_resource_manager.acquire("ollama")
    try:
        # æ¯æ¬¡è°ƒç”¨éƒ½æ·»åŠ ç³»ç»Ÿæç¤ºï¼Œç¡®ä¿å·¥å…·æè¿°å®Œæ•´
        chat_logger.info(f"ğŸ”„ æ„å»ºç³»ç»Ÿæç¤ºå’Œå·¥å…·æè¿°")
        # æ„å»ºå·¥å…·åˆ—è¡¨æè¿°
        tools_description = f"""1. rag_knowledge_search: æŸ¥è¯¢NSRLç»¼åˆçŸ¥è¯†åº“ï¼ˆåŒ…å«æŠ€æœ¯ã€ç®¡ç†ã€è´¢åŠ¡ç­‰å…¨æ–¹ä½å†…å®¹ï¼‰
        - å¿…é¡»å‚æ•°: query (string)
        - å½“å‰çŸ¥è¯†åº“: {knowledge_base_name}
        - è°ƒç”¨ç¤ºä¾‹: {{"name": "rag_knowledge_search", "arguments": {{"query": "å®éªŒè´¹ç”¨æ ‡å‡†"}}}}
        - çŸ¥è¯†åº“å†…å®¹æ¶µç›–ï¼š
          * å®éªŒçº¿ç«™æŠ€æœ¯å‚æ•°å’Œä½¿ç”¨æŒ‡å—
          * NSRLç®¡ç†è§„å®šå’Œåˆ¶åº¦æ–‡ä»¶
          * è´¢åŠ¡æ”¿ç­–å’Œæ”¶è´¹æ ‡å‡†
          * å®‰å…¨é˜²æŠ¤å’Œæ“ä½œè§„èŒƒ
          * è®¾å¤‡ç»´æŠ¤å’Œæ•…éšœå¤„ç†
          * ç”¨æˆ·æœåŠ¡å’Œç”³è¯·æµç¨‹
          * æŠ€æœ¯åŸ¹è®­å’Œæ“ä½œæ‰‹å†Œ
        - ç‰¹åˆ«è¯´æ˜: çŸ¥è¯†åº“åŒ…å«ä¸¤ç§ç±»å‹çš„å†…å®¹ï¼š
          * QAå¯¹çŸ¥è¯†åº“: ä»¥"é—®é¢˜ï¼š...ç­”æ¡ˆï¼š..."æ ¼å¼è¿”å›å®Œæ•´é—®ç­”å¯¹ï¼Œé—®é¢˜æƒé‡æ›´é«˜
          * æ–‡æ¡£ç‰‡æ®µ: è¿”å›ç›¸å…³æ–‡æ¡£å†…å®¹ç‰‡æ®µ
        - å½“æ£€ç´¢åˆ°QAå¯¹æ—¶ï¼Œç³»ç»Ÿä¼šä¼˜å…ˆè¿”å›é—®é¢˜åŒ¹é…åº¦é«˜çš„ç»“æœï¼Œå¹¶æ ‡è®°ä¸º"QAå¯¹çŸ¥è¯†åº“"
        - æ³¨æ„: ç³»ç»Ÿä»…ä½¿ç”¨æœ¬åœ°çŸ¥è¯†åº“ï¼Œä¸æä¾›è”ç½‘æœç´¢åŠŸèƒ½"""
        # æ·»åŠ ç”¨æˆ·æ–‡æ¡£å·¥å…·æè¿°
        if user_document_tools_list:
            tools_description += "\nç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£æœç´¢å·¥å…·:"
            for tool_name in user_document_tools_list:
                tool_info = get_user_document_tool(tool_name)
                if tool_info:
                    tools_description += f"\n{tool_info['tool'].name}: {tool_info['tool'].description}"
                    tools_description += "\n   - å¿…é¡»å‚æ•°: query (string)"
                    chat_logger.info(f"ğŸ“‹ æ·»åŠ ç”¨æˆ·æ–‡æ¡£å·¥å…·æè¿°: {tool_info['tool'].name} - {tool_info['tool'].description}")
        
        # æ·»åŠ QAå¯¹è¯´æ˜
        tools_description += f"""
        3. çŸ¥è¯†åº“å†…å®¹è¯´æ˜:
        - çŸ¥è¯†åº“ '{knowledge_base_name}' åŒ…å«PDFæ–‡æ¡£å’Œé—®ç­”å¯¹
        - PDFæ–‡æ¡£: æŒ‰åŸå§‹æ–‡ä»¶å.mdå­˜å‚¨
        - é—®ç­”å¯¹: æŒ‰ç”¨æˆ·æŒ‡å®šçš„æ–‡æ¡£å.mdå­˜å‚¨ï¼ˆå¦‚test.mdï¼‰
        - æ‰€æœ‰å†…å®¹éƒ½å¯é€šè¿‡rag_knowledge_searchç»Ÿä¸€æœç´¢"""
        
        # æ„å»ºç³»ç»Ÿæç¤º
        system_prompt = f"""ä½ æ˜¯NSRLï¼ˆå›½å®¶åŒæ­¥è¾å°„å®éªŒå®¤ï¼‰ç»¼åˆæ™ºèƒ½åŠ©æ‰‹ï¼Œä¸“é—¨å›ç­”å…³äºNSRLçš„å„ç§é—®é¢˜ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š

## ä¸»è¦æœåŠ¡é¢†åŸŸ
1. **å®éªŒçº¿ç«™æ¨è**ï¼šæ ¹æ®ç”¨æˆ·å®éªŒéœ€æ±‚æ¨èåˆé€‚çš„åŒæ­¥è¾å°„å®éªŒçº¿ç«™
2. **æŠ€æœ¯å’¨è¯¢**ï¼šå›ç­”åŒæ­¥è¾å°„æŠ€æœ¯ã€å®éªŒæ–¹æ³•ã€è®¾å¤‡ä½¿ç”¨ç­‰æŠ€æœ¯é—®é¢˜
3. **ç®¡ç†è§„å®š**ï¼šè§£ç­”NSRLçš„ç®¡ç†åˆ¶åº¦ã€ä½¿ç”¨è§„èŒƒã€å®‰å…¨è§„å®šç­‰
4. **è´¢åŠ¡æ”¿ç­–**ï¼šå›ç­”å…³äºå®éªŒè´¹ç”¨ã€æ”¶è´¹æ ‡å‡†ã€è´¢åŠ¡æµç¨‹ç­‰é—®é¢˜
5. **ç”³è¯·æµç¨‹**ï¼šæŒ‡å¯¼ç”¨æˆ·å¦‚ä½•ç”³è¯·å®éªŒæ—¶é—´ã€æäº¤ææ¡ˆç­‰
6. **å®‰å…¨é˜²æŠ¤**ï¼šæä¾›è¾å°„å®‰å…¨ã€å®éªŒå®‰å…¨ç­‰ç›¸å…³æŒ‡å¯¼
7. **è®¾å¤‡ç»´æŠ¤**ï¼šå›ç­”è®¾å¤‡çŠ¶æ€ã€ç»´æŠ¤è®¡åˆ’ã€æ•…éšœå¤„ç†ç­‰é—®é¢˜
8. **ç”¨æˆ·æœåŠ¡**ï¼šè§£ç­”ç”¨æˆ·æœåŠ¡ã€æŠ€æœ¯æ”¯æŒã€åŸ¹è®­ç­‰ç›¸å…³é—®é¢˜

## çº¿ç«™æŠ€æœ¯åˆ†ç±»æ€»è¡¨
ä»¥ä¸‹æ˜¯å¯ç”¨çš„å®éªŒçº¿ç«™åŠå…¶æŠ€æœ¯å‚æ•°ï¼š

| å®éªŒæŠ€æœ¯ç±»åˆ« | çº¿ç«™åç§° | èƒ½é‡èŒƒå›´ | å¯ç”¨äºçš„å­¦ç§‘ | èƒ½é‡åˆ†è¾¨ç‡ |
| :--- | :--- | :--- | :--- | :--- |
| **å…‰ç”µç¦»è´¨è°±æŠ€æœ¯** | è´¨è°±åˆ†æçº¿ç«™ (SVUV-PIMS) | 5~24.5 eV | æœ‰æœºåˆæˆã€çŸ³æ²¹åŒ–å·¥ã€ç¯å¢ƒç›‘æµ‹ã€ç”Ÿç‰©åŒ–å­¦ã€ç”Ÿç‰©æŠ€æœ¯ã€ä¸´åºŠåˆ†æã€æ–°é™ˆä»£è°¢ | 575 @ 16 eV |
| ^^ | ç‡ƒçƒ§å…‰æŸçº¿ç«™ | 5~24.5 eV | ç‡ƒçƒ§ååº”åŠ¨åŠ›å­¦ã€èƒ½æºåŠ¨åŠ›ç³»ç»Ÿã€æ±¡æŸ“ç‰©æ§åˆ¶ã€ç”Ÿç‰©ç‡ƒæ–™è¯„ä¼° | 3900 @ 7.9 eV; 4200 @ 14.6 eV |
| **çº¢å¤–å…‰è°±æŠ€æœ¯** | çº¢å¤–è°±å­¦å’Œæ˜¾å¾®æˆåƒå…‰æŸçº¿ç«™ | 20-8000 cmâ»Â¹ (å…‰è°±)<br>700-8000 cmâ»Â¹ (æ˜¾å¾®) | å‡èšæ€ç‰©ç†ã€åŒ–å­¦ååº”ã€ææ–™ç§‘å­¦ã€é«˜åˆ†å­ææ–™ã€ç”Ÿå‘½ç§‘å­¦ã€åŒ»å­¦ã€åœ°å­¦ã€ç¯å¢ƒã€å¤ç”Ÿç‰©å­¦ã€äººæ–‡è€ƒå¤ | 0.2 cmâ»Â¹ |
| **è½¯Xå°„çº¿æ•£å°„æŠ€æœ¯** | å…±æŒ¯è½¯Xå°„çº¿æ•£å°„çº¿ç«™ (RSoXS) | 220-700 eV | æœ‰æœºå…‰ç”µã€æœ‰æœºçƒ­ç”µã€ç¦»å­äº¤æ¢è†œç­‰è½¯ç‰©è´¨ææ–™ | 1619 @ 244.4 eV |
| **è½¯Xå°„çº¿æˆåƒæŠ€æœ¯** | è½¯Xå°„çº¿æˆåƒçº¿ç«™ | 260-800 eV | ç”Ÿå‘½ç§‘å­¦ï¼ˆç»†èƒæˆåƒï¼‰ã€ææ–™ç§‘å­¦ã€èƒ½æºã€å‚¬åŒ– | 500 @ 520 eV |
| **è½¯Xå°„çº¿å¸æ”¶å…‰è°±æŠ€æœ¯** | è½¯Xå°„çº¿ç£åœ†äºŒè‰²å…‰æŸçº¿ç«™ | 50-1000 eV | ææ–™ç§‘å­¦ã€ç‰©ç†å­¦ã€ç£æ€§ææ–™ç ”ç©¶ | 2000 @ 244 eV |
| **å…‰ç”µå­èƒ½è°±æŠ€æœ¯** | BL10Bå…‰æŸçº¿ (è½¯Xå°„çº¿è°±å­¦) | 100-1000 eV | ææ–™ç§‘å­¦ã€åŒ–å­¦ã€è¡¨é¢ç§‘å­¦ | E/Î”E > 1000 |
| ^^ | è§’åˆ†è¾¨å…‰ç”µå­èƒ½è°±çº¿ç«™ (ARPES) | 7-40 eV | å‡èšæ€ç‰©ç†ï¼ˆé«˜æ¸©è¶…å¯¼ã€æ‹“æ‰‘ç»ç¼˜ä½“ã€çŸ³å¢¨çƒ¯ï¼‰ | 10000 @ 14.6 eV |
| **è¡¨é¢ç§‘å­¦ä¸å‚¬åŒ–æŠ€æœ¯** | å‚¬åŒ–ä¸è¡¨é¢ç§‘å­¦å…‰æŸçº¿ (BL11U) | 20-600 eV | å‚¬åŒ–ç§‘å­¦ã€è¡¨é¢ç§‘å­¦ã€åŠå¯¼ä½“ææ–™ã€çº³ç±³ææ–™ | 15000 @ 29 eV |
| **å…‰è°±è®¡é‡æŠ€æœ¯** | å…‰è°±è¾å°„æ ‡å‡†å’Œè®¡é‡å…‰æŸçº¿ (è®¡é‡çº¿) | 1.2-200 nm<br>(çº¦ 6.2 - 1033 eV) | å…‰å­¦è®¡é‡ã€æ¢æµ‹å™¨ä¸å…‰å­¦å…ƒä»¶æ€§èƒ½æµ‹è¯• | < 1/1000 (Î”Î»/Î») |
| **åŸå­åˆ†å­å…‰è°±æŠ€æœ¯** | åŸå­åˆ†å­ç‰©ç†å…‰æŸçº¿ç«™ | 7-124 eV | åŸå­åˆ†å­ç‰©ç†ã€å›¢ç°‡ç§‘å­¦ã€å¤§æ°”æ°”æº¶èƒ¶ç§‘å­¦ã€åŒ–å­¦åŠ¨åŠ›å­¦ | 3000 @ 15 eV |

å¯ç”¨å·¥å…·ï¼š
{tools_description}

å·¥ä½œæµç¨‹ï¼š
1. å¯¹äºæ‰€æœ‰NSRLç›¸å…³é—®é¢˜ï¼Œå¿…é¡»é¦–å…ˆä½¿ç”¨ rag_knowledge_search æœç´¢çŸ¥è¯†åº“
2. æ£€æŸ¥è¿”å›ç»“æœçš„æœ€é«˜ç›¸ä¼¼åº¦ï¼š
   * å¦‚æœæœ€é«˜ç›¸ä¼¼åº¦ â‰¥ 0.3ï¼Œç»“æœç›¸å…³ï¼ŒåŸºäºæ­¤ç”Ÿæˆå›ç­”
   * å¦‚æœæœ€é«˜ç›¸ä¼¼åº¦ < 0.3ï¼Œç»“æœä¸ç›¸å…³ï¼Œæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·çŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯
3. ä¸¥æ ¼åŸºäºçŸ¥è¯†åº“å†…å®¹å›ç­”ï¼Œä¸å¾—ç¼–é€ æˆ–æ¨æµ‹ä¿¡æ¯

é‡è¦æŒ‡å¯¼åŸåˆ™:
1. å›ç­”å¿…é¡»ä¸¥æ ¼åŸºäºçŸ¥è¯†åº“å†…å®¹ï¼Œä¸å¾—ç¼–é€ ä¿¡æ¯
2. å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·"çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
3. æä¾›ä¿¡æ¯æ—¶è¦æ³¨æ˜æ¥æºï¼ˆæ¥è‡ªçŸ¥è¯†åº“ï¼‰
4. å¯¹äºæŠ€æœ¯å»ºè®®ï¼Œå¿…é¡»åŸºäºçŸ¥è¯†åº“ä¸­çš„æƒå¨èµ„æ–™
5. å¦‚æœçŸ¥è¯†åº“æœç´¢ç»“æœç›¸ä¼¼åº¦ < 0.3ï¼Œä¸å¾—åŸºäºä½ç›¸ä¼¼åº¦ç»“æœç”Ÿæˆå›ç­”
6. å›ç­”è¦ä¸“ä¸šã€å‡†ç¡®ã€ä¸¥è°¨
7. å¦‚æœå·²ç»è°ƒç”¨å·¥å…·è¶…è¿‡3æ¬¡ä»æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·çŸ¥è¯†åº“ä¸­æ— ç›¸å…³ä¿¡æ¯
å·¥å…·è°ƒç”¨æ ¼å¼è¦æ±‚:
- ä»…ä½¿ç”¨æŒ‡å®šçš„å·¥å…·åç§°
- ä»…ä¼ é€’å·¥å…·å®šä¹‰ä¸­è¦æ±‚çš„å‚æ•°
- ç»å¯¹ä¸è¦æ·»åŠ é¢å¤–å‚æ•°ï¼ˆå¦‚"using"ã€"reason"ç­‰ï¼‰
- ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºå·¥å…·è°ƒç”¨
- ä¾‹å¦‚: {{"name": "rag_knowledge_search", "arguments": {{"query": "ä½ çš„æŸ¥è¯¢"}}}}
- é‡è¦: ä¸è¦åœ¨å·¥å…·è°ƒç”¨ä¸­åŒ…å«ä»»ä½•é¢å¤–æ–‡æœ¬ã€è§£é‡Šæˆ–<think>æ ‡ç­¾
- å·¥å…·è°ƒç”¨å¿…é¡»æ˜¯çº¯JSONæ ¼å¼ï¼Œä¸èƒ½æœ‰å…¶ä»–å†…å®¹
- é”™è¯¯ç¤ºä¾‹: {{"name": "rag_knowledge_search", "arguments": {{"query": "...", "using": "..."}}}}"""
            
        messages = [SystemMessage(content=system_prompt)] + messages
        
        # æ£€æŸ¥å·¥å…·è°ƒç”¨æ¬¡æ•° - å¦‚æœè¶…è¿‡é™åˆ¶ï¼Œå¼ºåˆ¶æ¨¡å‹æä¾›ç­”æ¡ˆ
        # æ³¨æ„ï¼štool_call_count ä¼šåœ¨æ¯æ¬¡æ–°æ¶ˆæ¯æ—¶é‡ç½®ä¸º0ï¼Œæ‰€ä»¥è¿™é‡Œçš„æ£€æŸ¥æ˜¯é’ˆå¯¹å•æ¬¡å¯¹è¯çš„å·¥å…·è°ƒç”¨æ¬¡æ•°
        if state.get("tool_call_count", 0) >= 5:
            messages.append(SystemMessage(
                content="âš ï¸ é‡è¦æç¤ºï¼šæ‚¨å·²ç»è°ƒç”¨äº†å¤šæ¬¡å·¥å…·ä½†ä»æœªèƒ½æä¾›æœ€ç»ˆç­”æ¡ˆã€‚"
                        "è¯·åŸºäºå·²æœ‰ä¿¡æ¯ç«‹å³æä¾›å®Œæ•´å›ç­”ï¼Œä¸è¦å†è°ƒç”¨å·¥å…·ã€‚"
            ))
        
        # è®¡æ•°web_searchè°ƒç”¨æ¬¡æ•°
        web_search_count = sum(1 for m in messages
                            if isinstance(m, AIMessage) and
                            m.tool_calls and
                            any(tc["name"] == "web_search_tool" for tc in m.tool_calls))
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»è°ƒç”¨è¿‡web_searchä½†ç»“æœä¸ç†æƒ³
        if web_search_count >= 5:
            messages.append(SystemMessage(
                content="âš ï¸ é‡è¦æç¤ºï¼šæ‚¨å·²ç»å¤šæ¬¡ä½¿ç”¨ç½‘ç»œæœç´¢ä½†ä»æœªæä¾›æœ€ç»ˆç­”æ¡ˆã€‚"
                        "è¯·åŸºäºå·²æœ‰ä¿¡æ¯ç«‹å³æä¾›å®Œæ•´å›ç­”ï¼Œä¸è¦å†è°ƒç”¨å·¥å…·ã€‚"
            ))
        
        # === å…³é”®æ–°å¢ï¼šæ£€æŸ¥ä¸Šä¸€æ¬¡å·¥å…·è°ƒç”¨ç»“æœçš„ç›¸ä¼¼åº¦ ===
        if len(messages) >= 3:
            last_tool_response = messages[-2]  # ä¸Šä¸€æ¡æ˜¯å·¥å…·å“åº”
            if isinstance(last_tool_response, ToolMessage) and last_tool_response.content:
                # ä»å·¥å…·å“åº”ä¸­æå–æœ€é«˜ç›¸ä¼¼åº¦
                highest_similarity = extract_highest_similarity(last_tool_response.content)
                # å¦‚æœæœ€é«˜ç›¸ä¼¼åº¦ä½äºé˜ˆå€¼ï¼Œæ·»åŠ ç³»ç»Ÿæç¤ºå¼ºåˆ¶ä½¿ç”¨web_search_tool
                if highest_similarity < 0.3:
                    logger.warning(f"æ£€æµ‹åˆ°ä½ç›¸å…³æ€§ç»“æœ (ç›¸ä¼¼åº¦: {highest_similarity:.4f})ï¼Œå¼ºåˆ¶ä½¿ç”¨ç½‘ç»œæœç´¢")
                    messages.append(SystemMessage(
                        content="âš ï¸ é‡è¦æç¤ºï¼šæœ¬åœ°çŸ¥è¯†åº“æœç´¢ç»“æœç›¸å…³æ€§è¾ƒä½ï¼ˆæœ€é«˜ç›¸ä¼¼åº¦: {:.4f}ï¼‰ã€‚"
                                "è¯·ä½¿ç”¨web_search_toolè·å–æœ€æ–°äº’è”ç½‘ä¿¡æ¯ã€‚".format(highest_similarity)
                    ))
        
        # å§‹ç»ˆç»‘å®šæ‰€æœ‰å¯ç”¨å·¥å…·
        model = gpu_resource_manager.get_ollama_model()
        chat_logger.info(f"ğŸ¤– è·å–åˆ°æ¨¡å‹ç±»å‹: {type(model).__name__}")
        model_with_tools = model.bind_tools(available_tools)
        
        chat_logger.info(f"ğŸ¤– å¼€å§‹è°ƒç”¨DeepSeek APIæ¨¡å‹...")
        # è°ƒç”¨æ¨¡å‹
        response = await model_with_tools.ainvoke(messages)
        chat_logger.info(f"âœ… æ¨¡å‹å“åº”å®Œæˆ")
        
        # è®°å½•æ¨¡å‹å“åº”å†…å®¹
        if hasattr(response, "content") and response.content:
            # è®°å½•å®Œæ•´çš„æ¨¡å‹å›ç­”å†…å®¹
            chat_logger.info(f"ğŸ’¬ æ¨¡å‹å›ç­”å®Œæ•´å†…å®¹:")
            chat_logger.info(f"ğŸ“ {response.content}")
            # åŒæ—¶è®°å½•é•¿åº¦ä¿¡æ¯
            chat_logger.info(f"ğŸ“Š å›ç­”é•¿åº¦: {len(response.content)} å­—ç¬¦")
        
        # è®°å½•å·¥å…·è°ƒç”¨
        if hasattr(response, "tool_calls") and response.tool_calls:
            chat_logger.info(f"ğŸ”§ æ¨¡å‹å†³å®šè°ƒç”¨å·¥å…·: {len(response.tool_calls)} ä¸ª")
            for i, tool_call in enumerate(response.tool_calls):
                chat_logger.info(f"  ğŸ”§ å·¥å…· {i+1}: {tool_call['name']} - å‚æ•°: {tool_call['args']}")
        else:
            # å°è¯•ä»æ–‡æœ¬å†…å®¹ä¸­è§£æå·¥å…·è°ƒç”¨
            import re
            import json
            content = response.content if hasattr(response, "content") else ""
            
            # åŒ¹é…å·¥å…·è°ƒç”¨æ ¼å¼ï¼š{"name": "tool_name", "arguments": {...}}
            tool_call_pattern = r'\{[^}]*"name"\s*:\s*"([^"]+)"[^}]*"arguments"\s*:\s*(\{[^}]*\})[^}]*\}'
            tool_calls = []
            
            for match in re.finditer(tool_call_pattern, content):
                tool_name = match.group(1)
                try:
                    tool_args = json.loads(match.group(2))
                    tool_calls.append({
                        "name": tool_name,
                        "args": tool_args,
                        "id": f"call_{len(tool_calls)}"
                    })
                    chat_logger.info(f"ğŸ” ä»æ–‡æœ¬è§£æåˆ°å·¥å…·è°ƒç”¨: {tool_name} - å‚æ•°: {tool_args}")
                except json.JSONDecodeError:
                    chat_logger.warning(f"âš ï¸ å·¥å…·è°ƒç”¨å‚æ•°JSONè§£æå¤±è´¥: {match.group(2)}")
            
            if tool_calls:
                # å°†è§£æçš„å·¥å…·è°ƒç”¨æ·»åŠ åˆ°responseå¯¹è±¡
                response.tool_calls = tool_calls
                chat_logger.info(f"ğŸ”§ ä»æ–‡æœ¬è§£æåˆ° {len(tool_calls)} ä¸ªå·¥å…·è°ƒç”¨")
            else:
                chat_logger.info(f"ğŸ’¬ æ¨¡å‹ç›´æ¥å›ç­”ï¼Œæ— å·¥å…·è°ƒç”¨")
        
        # å…³é”®ä¿®å¤ï¼šéªŒè¯å¹¶æ¸…ç†å·¥å…·è°ƒç”¨å‚æ•°
        if hasattr(response, "tool_calls") and response.tool_calls:
            chat_logger.info(f"ğŸ§¹ å¼€å§‹æ¸…ç†å·¥å…·è°ƒç”¨å‚æ•°...")
            cleaned_tool_calls = []
            for tool_call in response.tool_calls:
                # åªä¿ç•™æœ‰æ•ˆçš„å‚æ•°
                valid_args = {}
                # æ ¹æ®å·¥å…·åç§°å¤„ç†å‚æ•°
                if tool_call["name"] == "rag_knowledge_search":
                    # ä»…ä¿ç•™queryå‚æ•°
                    if "query" in tool_call["args"]:
                        valid_args["query"] = tool_call["args"]["query"]
                    else:
                        # å¦‚æœæ²¡æœ‰queryå‚æ•°ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå‚æ•°æˆ–æ•´ä¸ªå†…å®¹ä½œä¸ºæŸ¥è¯¢
                        first_arg = next(iter(tool_call["args"].values()), "æœªçŸ¥æŸ¥è¯¢")
                        valid_args["query"] = str(first_arg)
                        logger.warning(f"rag_knowledge_searchç¼ºå°‘queryå‚æ•°ï¼Œä½¿ç”¨å¤‡ç”¨å‚æ•°: {first_arg}")
                        chat_logger.warning(f"âš ï¸ rag_knowledge_searchç¼ºå°‘queryå‚æ•°ï¼Œä½¿ç”¨å¤‡ç”¨å‚æ•°: {first_arg}")
                elif tool_call["name"] == "web_search_tool":
                    # ä»…ä¿ç•™queryå‚æ•°
                    if "query" in tool_call["args"]:
                        valid_args["query"] = tool_call["args"]["query"]
                    else:
                        first_arg = next(iter(tool_call["args"].values()), "æœªçŸ¥æŸ¥è¯¢")
                        valid_args["query"] = str(first_arg)
                        logger.warning(f"web_search_toolç¼ºå°‘queryå‚æ•°ï¼Œä½¿ç”¨å¤‡ç”¨å‚æ•°: {first_arg}")
                        chat_logger.warning(f"âš ï¸ web_search_toolç¼ºå°‘queryå‚æ•°ï¼Œä½¿ç”¨å¤‡ç”¨å‚æ•°: {first_arg}")
                # æ·»åŠ ç”¨æˆ·æ–‡æ¡£å·¥å…·å‚æ•°å¤„ç†
                elif tool_call["name"].startswith("search_"):
                    # ä»…ä¿ç•™queryå‚æ•°
                    if "query" in tool_call["args"]:
                        valid_args["query"] = tool_call["args"]["query"]
                    else:
                        first_arg = next(iter(tool_call["args"].values()), "æœªçŸ¥æŸ¥è¯¢")
                        valid_args["query"] = str(first_arg)
                        logger.warning(f"{tool_call['name']}ç¼ºå°‘queryå‚æ•°ï¼Œä½¿ç”¨å¤‡ç”¨å‚æ•°: {first_arg}")
                        chat_logger.warning(f"âš ï¸ {tool_call['name']}ç¼ºå°‘queryå‚æ•°ï¼Œä½¿ç”¨å¤‡ç”¨å‚æ•°: {first_arg}")
                # åˆ›å»ºæ¸…ç†åçš„å·¥å…·è°ƒç”¨
                cleaned_tool_call = {
                    "name": tool_call["name"],
                    "args": valid_args,
                    "id": tool_call["id"]
                }
                cleaned_tool_calls.append(cleaned_tool_call)
            # æ›¿æ¢åŸå§‹çš„tool_calls
            response.tool_calls = cleaned_tool_calls
            logger.info(f"å·²æ¸…ç†å·¥å…·è°ƒç”¨å‚æ•°ï¼Œç§»é™¤æ— æ•ˆå‚æ•°")
            chat_logger.info(f"âœ… å·¥å…·è°ƒç”¨å‚æ•°æ¸…ç†å®Œæˆ")
        
        # è®¡ç®—å·¥å…·è°ƒç”¨å¢é‡
        tool_call_increment = 1 if (hasattr(response, "tool_calls") and response.tool_calls) else 0
        
        # =============== æ–°å¢ï¼šè¯¦ç»†æ—¥å¿—è®°å½• ===============
        chat_logger.info(f"ğŸ“Š å·¥å…·è°ƒç”¨ç»Ÿè®¡ - æœ¬æ¬¡å¢é‡: {tool_call_increment}")
        chat_logger.info(f"ğŸ¯ æ¨¡å‹æ€è€ƒå®Œæˆï¼Œå‡†å¤‡è¿”å›ç»“æœ")
        # =============== æ–°å¢ç»“æŸ ===============
        
        # ç¡®ä¿web_search_enabledçŠ¶æ€æ­£ç¡®ä¼ é€’
        web_search_enabled = state.get("web_search_enabled", True)
        chat_logger.info(f"ğŸ“¤ è¿”å›çŠ¶æ€ - Webæœç´¢çŠ¶æ€: {'å¯ç”¨' if web_search_enabled else 'ç¦ç”¨'}")
        
        return {
            "messages": [response],
            "tool_call_count": tool_call_increment,  # è¿™é‡Œä¼šç´¯åŠ ï¼Œä½†æ¯æ¬¡æ–°æ¶ˆæ¯ä¼šé‡ç½®
            "knowledge_base_name": knowledge_base_name,  # ç¡®ä¿ä¼ é€’çŸ¥è¯†åº“åç§°
            "user_document_tools": user_document_tools_list,  # ç¡®ä¿ä¼ é€’ç”¨æˆ·æ–‡æ¡£å·¥å…·åˆ—è¡¨
            "web_search_enabled": web_search_enabled  # ç¡®ä¿çŠ¶æ€æ­£ç¡®ä¼ é€’
        }
    finally:
        await gpu_resource_manager.release()
        logger.info("DeepSeek APIæ¨¡å‹å¤„ç†å®Œæˆï¼Œèµ„æºå·²é‡Šæ”¾")
        chat_logger.info(f"ğŸ§¹ GPUèµ„æºå·²é‡Šæ”¾")

# 5. ä¿®æ”¹æ¡ä»¶å‡½æ•°
def should_continue(state: AgentState):
    """å†³å®šæ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·æˆ–ç»“æŸ"""
    messages = state["messages"]
    last_message = messages[-1]
    tool_call_count = state.get("tool_call_count", 0)
    
    # =============== æ–°å¢ï¼šè¯¦ç»†æ—¥å¿—è®°å½• ===============
    chat_logger.info(f"ğŸ¤” å†³ç­–æ˜¯å¦ç»§ç»­ - å·¥å…·è°ƒç”¨æ¬¡æ•°: {tool_call_count}")
    # =============== æ–°å¢ç»“æŸ ===============
    
    # æ£€æŸ¥å·¥å…·è°ƒç”¨æ¬¡æ•° - è¶…è¿‡5æ¬¡å¼ºåˆ¶ç»“æŸ
    # æ³¨æ„ï¼štool_call_count ä¼šåœ¨æ¯æ¬¡æ–°æ¶ˆæ¯æ—¶é‡ç½®ä¸º0ï¼Œæ‰€ä»¥è¿™é‡Œçš„æ£€æŸ¥æ˜¯é’ˆå¯¹å•æ¬¡å¯¹è¯çš„å·¥å…·è°ƒç”¨æ¬¡æ•°
    if tool_call_count >= 5:
        chat_logger.info(f"ğŸ›‘ å·¥å…·è°ƒç”¨æ¬¡æ•°å·²è¾¾ä¸Šé™({tool_call_count})ï¼Œç»“æŸå¯¹è¯")
        return END
    # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œåˆ™ç»§ç»­
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        chat_logger.info(f"ğŸ”„ æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼Œç»§ç»­æ‰§è¡Œå·¥å…·èŠ‚ç‚¹")
        return "tools"
    # å¦åˆ™ç»“æŸ
    chat_logger.info(f"âœ… æ— å·¥å…·è°ƒç”¨ï¼Œå¯¹è¯ç»“æŸ")
    return END

# 6. å…¨å±€å˜é‡å­˜å‚¨ç¼–è¯‘åçš„å›¾
graph = None

def tool_node(state: AgentState):
    """è‡ªå®šä¹‰å·¥å…·èŠ‚ç‚¹ï¼Œèƒ½æ ¹æ®çŸ¥è¯†åº“åç§°åŠ¨æ€è·å–å·¥å…·"""
    messages = state["messages"]
    last_message = messages[-1]
    knowledge_base_name = state.get("knowledge_base_name", "nsrl_tech_docs")
    
    # =============== æ–°å¢ï¼šè¯¦ç»†æ—¥å¿—è®°å½• ===============
    chat_logger.info(f"ğŸ”§ å¼€å§‹æ‰§è¡Œå·¥å…·èŠ‚ç‚¹ - çŸ¥è¯†åº“: {knowledge_base_name}")
    chat_logger.info(f"ğŸ“ éœ€è¦æ‰§è¡Œçš„å·¥å…·è°ƒç”¨: {len(last_message.tool_calls)} ä¸ª")
    for i, tool_call in enumerate(last_message.tool_calls):
        chat_logger.info(f"  ğŸ”§ å·¥å…· {i+1}: {tool_call['name']} - å‚æ•°: {tool_call['args']}")
    # =============== æ–°å¢ç»“æŸ ===============
    
    # åŠ¨æ€è·å–å½“å‰çŸ¥è¯†åº“çš„RAGå·¥å…·
    rag_tool = get_rag_tool(knowledge_base_name)
    # åˆ›å»ºå·¥å…·æ˜ å°„
    tools = {
        "rag_knowledge_search": rag_tool
    }
    # è”ç½‘æœç´¢åŠŸèƒ½å·²ç¦ç”¨
    chat_logger.info(f"âš ï¸ å·¥å…·èŠ‚ç‚¹ - è”ç½‘æœç´¢åŠŸèƒ½å·²ç¦ç”¨")
    # æ·»åŠ ç”¨æˆ·æ–‡æ¡£å·¥å…·
    user_document_tools_list = state.get("user_document_tools", [])
    for tool_name in user_document_tools_list:
        tool_info = get_user_document_tool(tool_name)
        if tool_info and "tool" in tool_info:
            tools[tool_info["tool"].name] = tool_info["tool"]
    chat_logger.info(f"ğŸ”§ å¯ç”¨å·¥å…·: {list(tools.keys())}")
    
    # æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
    outputs = []
    for i, tool_call in enumerate(last_message.tool_calls):
        tool_name = tool_call["name"]
        chat_logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œå·¥å…· {i+1}: {tool_name}")
        
        if tool_name in tools:
            tool = tools[tool_name]
            try:
                # è°ƒç”¨å·¥å…·
                chat_logger.info(f"ğŸ”§ è°ƒç”¨å·¥å…· {tool_name} å‚æ•°: {tool_call['args']}")
                response = tool.invoke(tool_call["args"])
                
                # è®°å½•å·¥å…·è¿”å›ç»“æœ
                response_str = str(response)
                chat_logger.info(f"âœ… å·¥å…· {tool_name} æ‰§è¡ŒæˆåŠŸ: {len(response_str)}å­—ç¬¦")
                
                # =============== æ–°å¢ï¼šè®°å½•å·¥å…·è¿”å›çš„å®Œæ•´å†…å®¹ ===============
                chat_logger.info(f"ğŸ“¤ å·¥å…· {tool_name} è¿”å›å†…å®¹:")
                chat_logger.info(f"ğŸ“ {response_str}")
                chat_logger.info(f"ğŸ“Š å·¥å…·è¿”å›å†…å®¹é•¿åº¦: {len(response_str)} å­—ç¬¦")
                # =============== æ–°å¢ç»“æŸ ===============
                
                # å¦‚æœæ˜¯RAGå·¥å…·ï¼Œè®°å½•ç›¸ä¼¼åº¦ä¿¡æ¯
                if tool_name == "rag_knowledge_search" and "ç›¸ä¼¼åº¦:" in response_str:
                    similarities = re.findall(r"ç›¸ä¼¼åº¦: ([\d.]+)", response_str)
                    if similarities:
                        max_sim = max(float(s) for s in similarities)
                        chat_logger.info(f"ğŸ¯ RAGå·¥å…·æœ€é«˜ç›¸ä¼¼åº¦: {max_sim:.4f}")
                
                # åœ¨å·¥å…·ç»“æœä¸­åŒ…å«ç”¨æˆ·çš„åŸå§‹é—®é¢˜
                user_question = ""
                for msg in reversed(state["messages"]):
                    if isinstance(msg, HumanMessage):
                        user_question = msg.content
                        break
                
                # æ„å»ºåŒ…å«ç”¨æˆ·é—®é¢˜çš„å·¥å…·å“åº”
                enhanced_content = f"ç”¨æˆ·é—®é¢˜: {user_question}\n\nå·¥å…·ç»“æœ:\n{str(response)}"
                
                # =============== æ–°å¢ï¼šè¯¦ç»†è®°å½•å·¥å…·è¿”å›ç»“æœ ===============
                chat_logger.info(f"ğŸ”§ å·¥å…· {tool_name} æ‰§è¡ŒæˆåŠŸ")
                chat_logger.info(f"ğŸ“¤ å·¥å…·è¿”å›å†…å®¹é•¿åº¦: {len(str(response))} å­—ç¬¦")
                chat_logger.info(f"ğŸ“ å·¥å…·è¿”å›å†…å®¹é¢„è§ˆ: {str(response)[:200]}...")
                chat_logger.info(f"ğŸ¯ å·¥å…·è¿”å›å®Œæ•´å†…å®¹:")
                chat_logger.info(f"{str(response)}")
                chat_logger.info(f"ğŸ”§ å·¥å…· {tool_name} è¿”å›è®°å½•å®Œæˆ")
                # =============== æ–°å¢ç»“æŸ ===============
                
                outputs.append(
                    ToolMessage(
                        content=enhanced_content,
                        name=tool_name,
                        tool_call_id=tool_call["id"]
                    )
                )
            except Exception as e:
                chat_logger.error(f"âŒ å·¥å…· {tool_name} æ‰§è¡Œå¤±è´¥: {str(e)}")
                # =============== æ–°å¢ï¼šè®°å½•å·¥å…·æ‰§è¡Œå¤±è´¥çš„è¯¦ç»†ä¿¡æ¯ ===============
                chat_logger.error(f"ğŸ“¤ å·¥å…· {tool_name} æ‰§è¡Œå¤±è´¥è¯¦æƒ…:")
                chat_logger.error(f"ğŸ“ é”™è¯¯ä¿¡æ¯: {str(e)}")
                chat_logger.error(f"ğŸ”§ å·¥å…·å‚æ•°: {tool_call['args']}")
                chat_logger.error(f"ğŸ“Š é”™è¯¯ç±»å‹: {type(e).__name__}")
                # =============== æ–°å¢ç»“æŸ ===============
                outputs.append(
                    ToolMessage(
                        content=f"å·¥å…·è°ƒç”¨é”™è¯¯: {str(e)}",
                        name=tool_name,
                        status="error",
                        tool_call_id=tool_call["id"]
                    )
                )
        else:
            chat_logger.error(f"âŒ å·¥å…· {tool_name} ä¸å­˜åœ¨ï¼Œå¯ç”¨å·¥å…·: {list(tools.keys())}")
            # =============== æ–°å¢ï¼šè®°å½•å·¥å…·ä¸å­˜åœ¨çš„è¯¦ç»†ä¿¡æ¯ ===============
            chat_logger.error(f"ğŸ“¤ å·¥å…· {tool_name} ä¸å­˜åœ¨è¯¦æƒ…:")
            chat_logger.error(f"ğŸ”§ è¯·æ±‚çš„å·¥å…·åç§°: {tool_name}")
            chat_logger.error(f"ğŸ“‹ å¯ç”¨å·¥å…·åˆ—è¡¨: {list(tools.keys())}")
            chat_logger.error(f"ğŸ“Š å¯ç”¨å·¥å…·æ•°é‡: {len(tools.keys())}")
            # =============== æ–°å¢ç»“æŸ ===============
            outputs.append(
                ToolMessage(
                    content=f"Error: {tool_name} is not a valid tool, try one of [{', '.join(tools.keys())}]",
                    name=tool_name,
                    status="error"
                )
            )
    
    chat_logger.info(f"âœ… å·¥å…·èŠ‚ç‚¹æ‰§è¡Œå®Œæˆï¼Œè¿”å› {len(outputs)} ä¸ªç»“æœ")
    return {"messages": outputs}

# 8. æ ¸å¿ƒAPIç«¯ç‚¹
class ChatRequest(BaseModel):
    message: str
    session_id: Optional[str] = None
    conversation_id: Optional[str] = None  # æ–°å¢ï¼šå¯¹è¯IDï¼Œç”¨äºè·¨è®¾å¤‡åŒæ­¥
    stream: Optional[bool] = False  # æ˜¯å¦å¯ç”¨æµå¼å“åº”
    knowledge_base_name: Optional[str] = "nsrl_tech_docs"  # æ–°å¢å‚æ•°ï¼Œé»˜è®¤ä¸º"nsrl_tech_docs"
    url: Optional[str] = None
    enable_web_search: Optional[bool] = True
    message_id: Optional[str] = None

class ChatResponse(BaseModel):
    response: str
    session_id: str
    conversation_id: Optional[str] = None  # æ–°å¢ï¼šå¯¹è¯ID
    conversation_history: List[Dict[str, str]]
    tool_calls: Optional[List[Dict]] = None

class ErrorResponse(BaseModel):
    detail: str

@agent_router.post(
    "/chat",
    response_model=ChatResponse,
    responses={
        400: {"model": ErrorResponse, "description": "æ— æ•ˆçš„ä¼šè¯ID"},
        500: {"model": ErrorResponse, "description": "å†…éƒ¨æœåŠ¡é”™è¯¯"}
    }
)
async def chat_endpoint(request: ChatRequest, http_request: Request = None):
    """
    å¤„ç†èŠå¤©è¯·æ±‚
    - æ–°ä¼šè¯: ä¸æä¾›session_id
    - ç»­ä¼šè¯: æä¾›æœ‰æ•ˆçš„session_id
    - url: å¯é€‰ï¼Œç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£URL
    """
    global graph
    if not graph:
        raise HTTPException(
            status_code=503,
            detail="æœåŠ¡å°šæœªåˆå§‹åŒ–å®Œæˆ"
        )
    # ç”Ÿæˆ/éªŒè¯ä¼šè¯ID
    # è·å–å½“å‰ç”¨æˆ·ä¿¡æ¯
    current_user = get_current_user_from_token(http_request) if http_request else None
    user_id = current_user.id if current_user else "anonymous"
    
    # ä½¿ç”¨ç”¨æˆ·ID + ä¼šè¯åç§°ç”Ÿæˆsession_idï¼Œç¡®ä¿è·¨è®¾å¤‡ä¸€è‡´æ€§
    if request.session_id:
        session_id = f"user_{user_id}_{request.session_id}"
    else:
        # é»˜è®¤ä¼šè¯ä½¿ç”¨ç”¨æˆ·ID
        session_id = f"user_{user_id}_default"
    
    # å¤„ç†å¯¹è¯ID
    conversation_id = request.conversation_id
    if not conversation_id:
        # å¦‚æœæ²¡æœ‰æä¾›å¯¹è¯IDï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„
        conversation_id = f"conv_{user_id}_{int(time.time())}"
    
    config = {"configurable": {"thread_id": session_id}}
    
    # =============== æ–°å¢ï¼šè¯¦ç»†æ—¥å¿—è®°å½• ===============
    chat_logger.info(f"ğŸš€ å¼€å§‹å¤„ç†èŠå¤©è¯·æ±‚ - Session: {session_id}")
    chat_logger.info(f"ğŸ“ ç”¨æˆ·è¾“å…¥: {request.message}")
    chat_logger.info(f"ğŸ”§ çŸ¥è¯†åº“: {request.knowledge_base_name}")
    chat_logger.info(f"ğŸŒ Webæœç´¢: {'å¯ç”¨' if request.enable_web_search else 'ç¦ç”¨'}")
    if request.url:
        chat_logger.info(f"ğŸ“„ æ–‡æ¡£URL: {request.url}")
    # =============== æ–°å¢ç»“æŸ ===============
    
    try:
        # è·å–å½“å‰ä¼šè¯çŠ¶æ€
        state = await graph.aget_state(config)
        chat_logger.info(f"ğŸ” è°ƒè¯• - è·å–åˆ°çš„çŠ¶æ€: {state}")
        chat_logger.info(f"ğŸ” è°ƒè¯• - session_id: {session_id}")
        chat_logger.info(f"ğŸ” è°ƒè¯• - config: {config}")
        if state:
            chat_logger.info(f"ğŸ” è°ƒè¯• - state.valuesç±»å‹: {type(state.values)}")
            chat_logger.info(f"ğŸ” è°ƒè¯• - state.valueså†…å®¹: {state.values}")
            chat_logger.info(f"ğŸ” è°ƒè¯• - æ˜¯å¦æœ‰messages: {'messages' in state.values}")
            if 'messages' in state.values:
                chat_logger.info(f"ğŸ” è°ƒè¯• - messagesæ•°é‡: {len(state.values['messages'])}")
                for i, msg in enumerate(state.values['messages']):
                    chat_logger.info(f"ğŸ” è°ƒè¯• - æ¶ˆæ¯{i}: {type(msg).__name__} - {msg.content[:50]}...")
        else:
            chat_logger.info(f"ğŸ” è°ƒè¯• - æ²¡æœ‰æ‰¾åˆ°å†å²çŠ¶æ€ï¼Œè¿™æ˜¯æ–°ä¼šè¯")
        
        # æ„å»ºæ–°çŠ¶æ€
        if state is None or not isinstance(state.values, dict) or "messages" not in state.values:
            # æ–°ä¼šè¯ï¼ˆåŒ…æ‹¬çŠ¶æ€ä¸å®Œæ•´çš„æƒ…å†µï¼‰
            chat_logger.info(f"ğŸ†• åˆ›å»ºæ–°ä¼šè¯")
            user_document_tools_list = []
            # =============== æ–°å¢ï¼šå¤„ç†æ–‡æ¡£URL ===============
            if request.url:
                try:
                    logger.info(f"å¤„ç†ç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£URL: {request.url}")
                    # ä½¿ç”¨session_idä½œä¸ºdocument_id
                    tool_name = await register_user_document_tool(
                        url=request.url,
                        document_id=session_id,
                        document_name="ç”¨æˆ·ä¸Šä¼ çš„åŸºå› æ£€æµ‹æŠ¥å‘Š"
                    )
                    user_document_tools_list.append(tool_name)
                    logger.info(f"æˆåŠŸæ³¨å†Œç”¨æˆ·æ–‡æ¡£å·¥å…·: {tool_name}")
                    chat_logger.info(f"âœ… æ³¨å†Œæ–‡æ¡£å·¥å…·: {tool_name}")
                except Exception as e:
                    logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")
                    chat_logger.error(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")
                    # å³ä½¿æ–‡æ¡£å¤„ç†å¤±è´¥ï¼Œä¹Ÿè¦ç»§ç»­å¯¹è¯
            # =============== æ–°å¢ç»“æŸ ===============
            chat_logger.info(f"ğŸ†• æ–°ä¼šè¯ - è¯·æ±‚ä¸­çš„enable_web_search: {request.enable_web_search}")
            initial_state = {
                "messages": [HumanMessage(content=request.message)],
                "knowledge_base_name": request.knowledge_base_name,
                "tool_call_count": 0,
                "user_document_tools": user_document_tools_list,
                "web_search_enabled": request.enable_web_search  # ä¿å­˜webæœç´¢å¼€å…³çŠ¶æ€
            }
            chat_logger.info(f"ğŸ†• æ–°ä¼šè¯ - åˆå§‹çŠ¶æ€ä¸­çš„web_search_enabled: {initial_state['web_search_enabled']}")
        else:
            # ç»­ä¼šè¯ - å¤åˆ¶ç°æœ‰çŠ¶æ€å¹¶æ·»åŠ æ–°æ¶ˆæ¯
            chat_logger.info(f"ğŸ”„ ç»§ç»­ç°æœ‰ä¼šè¯ï¼Œå†å²æ¶ˆæ¯æ•°: {len(state.values.get('messages', []))}")
            # ä¿ç•™ä¹‹å‰çš„çŸ¥è¯†åº“åç§°ï¼Œå³ä½¿è¯·æ±‚ä¸­æä¾›äº†æ–°å€¼ï¼ˆé¿å…ä¸­é€”åˆ‡æ¢çŸ¥è¯†åº“å¯¼è‡´æ··æ·†ï¼‰
            knowledge_base_name = state.values.get("knowledge_base_name", request.knowledge_base_name)
            # =============== æ–°å¢ï¼šå¤„ç†æ–‡æ¡£URLï¼ˆå¦‚æœæ˜¯æ–°ä¸Šä¼ çš„æ–‡æ¡£ï¼‰ ===============
            user_document_tools_list = state.values.get("user_document_tools", [])
            chat_logger.info(f"ğŸ“š ç°æœ‰æ–‡æ¡£å·¥å…·: {user_document_tools_list}")
            # å¦‚æœæœ‰æ–°çš„URLä¸”è¿˜æ²¡æœ‰æ³¨å†Œè¿‡å¯¹åº”çš„å·¥å…·
            if request.url and not any(
                    tool_name.startswith("search_" + session_id) for tool_name in user_document_tools_list):
                try:
                    logger.info(f"å¤„ç†ç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£URL: {request.url}")
                    # ä½¿ç”¨session_idä½œä¸ºdocument_id
                    tool_name = await register_user_document_tool(
                        url=request.url,
                        document_id=session_id,
                        document_name="ç”¨æˆ·ä¸Šä¼ çš„åŸºå› æ£€æµ‹æŠ¥å‘Š"
                    )
                    user_document_tools_list.append(tool_name)
                    logger.info(f"æˆåŠŸæ³¨å†Œç”¨æˆ·æ–‡æ¡£å·¥å…·: {tool_name}")
                    chat_logger.info(f"âœ… æ–°å¢æ–‡æ¡£å·¥å…·: {tool_name}")
                except Exception as e:
                    logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")
                    chat_logger.error(f"âŒ æ–°å¢æ–‡æ¡£å¤±è´¥: {str(e)}")
            # =============== æ–°å¢ç»“æŸ ===============
            # ä¿ç•™ä¹‹å‰çš„webæœç´¢è®¾ç½®ï¼Œé™¤éè¯·æ±‚ä¸­æä¾›äº†æ–°å€¼
            chat_logger.info(f"ğŸ” è°ƒè¯• - å†å²çŠ¶æ€ä¸­çš„web_search_enabled: {state.values.get('web_search_enabled', 'æœªè®¾ç½®')}")
            chat_logger.info(f"ğŸ” è°ƒè¯• - è¯·æ±‚ä¸­çš„enable_web_search: {request.enable_web_search}")
            
            # ä¸´æ—¶ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨è¯·æ±‚å‚æ•°ï¼Œç¡®ä¿webæœç´¢çŠ¶æ€æ­£ç¡®ä¼ é€’
            if request.enable_web_search is not None:
                web_search_enabled = request.enable_web_search
                chat_logger.info(f"ğŸ”§ ä¸´æ—¶ä¿®å¤ - ä½¿ç”¨è¯·æ±‚å‚æ•°: {web_search_enabled}")
            else:
                web_search_enabled = state.values.get("web_search_enabled", True)
                chat_logger.info(f"ğŸ”§ ä¸´æ—¶ä¿®å¤ - ä½¿ç”¨å†å²çŠ¶æ€: {web_search_enabled}")
            
            chat_logger.info(f"ğŸ”„ ç»­ä¼šè¯ - Webæœç´¢çŠ¶æ€: {'å¯ç”¨' if web_search_enabled else 'ç¦ç”¨'}")
            initial_state = {
                "messages": state.values["messages"] + [HumanMessage(content=request.message)],
                "knowledge_base_name": knowledge_base_name,
                "tool_call_count": 0,  # æ¯æ¬¡æ–°æ¶ˆæ¯éƒ½é‡ç½®å·¥å…·è°ƒç”¨æ¬¡æ•°
                "user_document_tools": user_document_tools_list,
                "web_search_enabled": web_search_enabled
            }
        
        chat_logger.info(f"ğŸ¯ åˆå§‹çŠ¶æ€æ„å»ºå®Œæˆï¼Œå·¥å…·æ•°é‡: {len(initial_state.get('user_document_tools', []))}")
        
        # æ‰§è¡Œå¯¹è¯æµ
        chat_logger.info(f"ğŸ”„ å¼€å§‹æ‰§è¡Œå¯¹è¯æµç¨‹...")
        final_state = None
        async for step in graph.astream(initial_state, config=config, stream_mode="values"):
            final_state = step
        if not final_state:
            chat_logger.error(f"âŒ å¯¹è¯æµç¨‹æœªäº§ç”Ÿæœ‰æ•ˆå“åº”")
            raise HTTPException(
                status_code=500,
                detail="å¯¹è¯æµç¨‹æœªäº§ç”Ÿæœ‰æ•ˆå“åº”"
            )
        
        chat_logger.info(f"âœ… å¯¹è¯æµç¨‹æ‰§è¡Œå®Œæˆ")
        
        # æå–æœ€æ–°å›å¤
        last_msg = final_state["messages"][-1]
        if not isinstance(last_msg, AIMessage):
            chat_logger.error(f"âŒ æ— æ•ˆçš„æ¨¡å‹å“åº”ç±»å‹: {type(last_msg)}")
            raise HTTPException(
                status_code=500,
                detail="æ— æ•ˆçš„æ¨¡å‹å“åº”ç±»å‹"
            )
        
        # æ”¶é›†å·¥å…·è°ƒç”¨ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        tool_calls = []
        if hasattr(last_msg, "tool_calls") and last_msg.tool_calls:
            chat_logger.info(f"ğŸ”§ æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨: {len(last_msg.tool_calls)} ä¸ª")
            for i, tool_call in enumerate(last_msg.tool_calls):
                tool_calls.append({
                    "name": tool_call["name"],
                    "args": tool_call["args"],
                    "id": tool_call["id"]
                })
                chat_logger.info(f"  ğŸ”§ å·¥å…· {i+1}: {tool_call['name']} - å‚æ•°: {tool_call['args']}")
        else:
            chat_logger.info(f"ğŸ’¬ æ— å·¥å…·è°ƒç”¨ï¼Œç›´æ¥å›ç­”")
        
        # æ„å»ºå†å²è®°å½•
        history = []
        for msg in final_state["messages"]:
            if isinstance(msg, HumanMessage):
                history.append({"role": "user", "content": msg.content})
            elif isinstance(msg, AIMessage):
                history.append({"role": "assistant", "content": msg.content})
            elif isinstance(msg, SystemMessage):
                history.append({"role": "system", "content": "ç³»ç»Ÿæç¤º"})
            elif isinstance(msg, ToolMessage):
                history.append({"role": "tool", "content": msg.content})
        
        # =============== æ–°å¢ï¼šè®°å½•æ­£å¸¸è¾“å‡ºçš„è¯¦ç»†ä¿¡æ¯ ===============
        chat_logger.info(f"ğŸš€ ====== æ­£å¸¸è¾“å‡ºAPIè¿”å›å†…å®¹ ======")
        chat_logger.info(f"ğŸ†” ä¼šè¯ID: {session_id}")
        chat_logger.info(f"ğŸ“¤ è¿”å›çš„æœ€ç»ˆå›ç­”:")
        chat_logger.info(f"   ğŸ“„ å›ç­”é•¿åº¦: {len(last_msg.content)} å­—ç¬¦")
        chat_logger.info(f"   ğŸ“ å®Œæ•´å›ç­”å†…å®¹: {last_msg.content}")
        chat_logger.info(f"ğŸ“Š è¿”å›çš„å¯¹è¯å†å²:")
        chat_logger.info(f"   ğŸ“ å†å²è®°å½•æ•°é‡: {len(history)}")
        for i, hist in enumerate(history):
            # è®°å½•å®Œæ•´çš„å¯¹è¯å†å²å†…å®¹
            chat_logger.info(f"     ğŸ“¤ å†å² {i+1} [{hist['role']}]:")
            chat_logger.info(f"        ğŸ“ å®Œæ•´å†…å®¹: {hist['content']}")
            chat_logger.info(f"        ğŸ“Š å†…å®¹é•¿åº¦: {len(hist['content'])} å­—ç¬¦")
            # åŒæ—¶ä¿ç•™é¢„è§ˆä¿¡æ¯
            chat_logger.info(f"        ğŸ“„ å†…å®¹é¢„è§ˆ: {hist['content'][:100]}{'...' if len(hist['content']) > 100 else ''}")
        chat_logger.info(f"ğŸ”§ è¿”å›çš„å·¥å…·è°ƒç”¨ä¿¡æ¯:")
        if tool_calls:
            for i, tool_call in enumerate(tool_calls):
                chat_logger.info(f"   ğŸ”§ å·¥å…· {i+1}: {tool_call['name']} - å‚æ•°: {tool_calls[i]['args']}")
        else:
            chat_logger.info(f"   â„¹ï¸ æ— å·¥å…·è°ƒç”¨")
        chat_logger.info(f"ğŸš€ ====== æ­£å¸¸è¾“å‡ºAPIè¿”å›å†…å®¹ç»“æŸ ======")
        # =============== æ–°å¢ç»“æŸ ===============
        
        chat_logger.info(f"ğŸ“¤ è¿”å›æœ€ç»ˆå›ç­”ï¼Œé•¿åº¦: {len(last_msg.content)}")
        chat_logger.info(f"ğŸ¯ å¯¹è¯å®Œæˆ - Session: {session_id}")
        
        # ä¿å­˜èŠå¤©æ—¥å¿—åˆ°æ–‡ä»¶
        try:
            chat_log_entry = {
                "timestamp": datetime.datetime.now().isoformat(),
                "session_id": session_id,
                "user_message": request.message,
                "assistant_response": last_msg.content,
                "knowledge_base": request.knowledge_base_name,
                "web_search_enabled": request.enable_web_search,
                "tool_calls": tool_calls if tool_calls else [],
                "conversation_length": len(history)
            }
            
            # ä¿å­˜åˆ°èŠå¤©æ—¥å¿—æ–‡ä»¶
            chat_log_file = os.path.join('logs', f"chat_logs_{datetime.datetime.now().strftime('%Y%m%d')}.log")
            with open(chat_log_file, 'a', encoding='utf-8') as f:
                f.write(f"[{chat_log_entry['timestamp']}] Session: {chat_log_entry['session_id']}\n")
                f.write(f"User: {chat_log_entry['user_message']}\n")
                f.write(f"Assistant: {chat_log_entry['assistant_response']}\n")
                f.write(f"Knowledge Base: {chat_log_entry['knowledge_base']}\n")
                f.write(f"Web Search: {'Enabled' if chat_log_entry['web_search_enabled'] else 'Disabled'}\n")
                f.write(f"Tool Calls: {len(chat_log_entry['tool_calls'])}\n")
                f.write(f"Conversation Length: {chat_log_entry['conversation_length']}\n")
                f.write("-" * 80 + "\n\n")
            
            chat_logger.info(f"ğŸ’¾ èŠå¤©æ—¥å¿—å·²ä¿å­˜åˆ°: {chat_log_file}")
        except Exception as e:
            chat_logger.error(f"âŒ ä¿å­˜èŠå¤©æ—¥å¿—å¤±è´¥: {str(e)}")
        
        return ChatResponse(
            response=last_msg.content,
            session_id=session_id,
            conversation_id=conversation_id,
            conversation_history=history,
            tool_calls=tool_calls if tool_calls else None
        )
    except HTTPException:
        raise
    except Exception as e:
        chat_logger.error(f"âŒ å¤„ç†è¯·æ±‚å¤±è´¥: {str(e)}")
        logger.error(f"âŒ å¤„ç†è¯·æ±‚å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"è¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}"
        )

# 9. æµå¼å“åº”APIï¼ˆå¯é€‰ï¼‰
@agent_router.post("/chat/stream")
async def chat_stream_endpoint(request: ChatRequest, http_request: Request = None):
    """
    æµå¼å“åº”èŠå¤©è¯·æ±‚ï¼ˆå›ºå®šåˆ†å—å¤§å°ï¼‰
    - ä½¿ç”¨æ ‡å‡†SSEæ ¼å¼
    - ä¿®å¤æµå¼å“åº”é—®é¢˜
    """
    global graph
    if not graph:
        raise HTTPException(
            status_code=503,
            detail="æœåŠ¡å°šæœªåˆå§‹åŒ–å®Œæˆ"
        )
    # ç”Ÿæˆ/éªŒè¯ä¼šè¯ID
    # è·å–å½“å‰ç”¨æˆ·ä¿¡æ¯
    current_user = get_current_user_from_token(http_request) if http_request else None
    user_id = current_user.id if current_user else "anonymous"
    
    # ä½¿ç”¨ç”¨æˆ·ID + ä¼šè¯åç§°ç”Ÿæˆsession_idï¼Œç¡®ä¿è·¨è®¾å¤‡ä¸€è‡´æ€§
    if request.session_id:
        session_id = f"user_{user_id}_{request.session_id}"
    else:
        # é»˜è®¤ä¼šè¯ä½¿ç”¨ç”¨æˆ·ID
        session_id = f"user_{user_id}_default"
    
    message_id = request.message_id
    config = {"configurable": {"thread_id": session_id}}
    
    async def event_generator():
        try:
            # è·å–å½“å‰ä¼šè¯çŠ¶æ€å’Œæ„å»ºåˆå§‹çŠ¶æ€
            state = await graph.aget_state(config)
            if state is None or not isinstance(state.values, dict) or "messages" not in state.values:
                # æ–°ä¼šè¯å¤„ç†...
                user_document_tools_list = []
                if request.url:
                    try:
                        logger.info(f"å¤„ç†ç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£URL: {request.url}")
                        document_id = f"{session_id}_{uuid.uuid4().hex[:8]}"
                        tool_name = await register_user_document_tool(
                            url=request.url,
                            document_id=document_id,
                            document_name="ç”¨æˆ·ä¸Šä¼ çš„åŸºå› æ£€æµ‹æŠ¥å‘Š"
                        )
                        user_document_tools_list.append(tool_name)
                        logger.info(f"æˆåŠŸæ³¨å†Œç”¨æˆ·æ–‡æ¡£å·¥å…·: {tool_name}")
                    except Exception as e:
                        logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")
                initial_state = {
                    "messages": [HumanMessage(content=request.message)],
                    "knowledge_base_name": request.knowledge_base_name,
                    "tool_call_count": 0,
                    "user_document_tools": user_document_tools_list,
                    "web_search_enabled": request.enable_web_search
                }
            else:
                # ç»­ä¼šè¯å¤„ç†...
                knowledge_base_name = state.values.get("knowledge_base_name", request.knowledge_base_name)
                user_document_tools_list = state.values.get("user_document_tools", [])
                
                # åº”ç”¨ä¸´æ—¶ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨è¯·æ±‚å‚æ•°ï¼Œç¡®ä¿webæœç´¢çŠ¶æ€æ­£ç¡®ä¼ é€’
                chat_logger.info(f"ğŸ” æµå¼APIè°ƒè¯• - å†å²çŠ¶æ€ä¸­çš„web_search_enabled: {state.values.get('web_search_enabled', 'æœªè®¾ç½®')}")
                chat_logger.info(f"ğŸ” æµå¼APIè°ƒè¯• - è¯·æ±‚ä¸­çš„enable_web_search: {request.enable_web_search}")
                
                if request.enable_web_search is not None:
                    web_search_enabled = request.enable_web_search
                    chat_logger.info(f"ğŸ”§ æµå¼APIä¸´æ—¶ä¿®å¤ - ä½¿ç”¨è¯·æ±‚å‚æ•°: {web_search_enabled}")
                else:
                    web_search_enabled = state.values.get("web_search_enabled", True)
                    chat_logger.info(f"ğŸ”§ æµå¼APIä¸´æ—¶ä¿®å¤ - ä½¿ç”¨å†å²çŠ¶æ€: {web_search_enabled}")
                
                chat_logger.info(f"ğŸ”„ æµå¼APIç»­ä¼šè¯ - Webæœç´¢çŠ¶æ€: {'å¯ç”¨' if web_search_enabled else 'ç¦ç”¨'}")
                if request.url and not any(
                        tool_name.startswith("search_" + session_id) for tool_name in user_document_tools_list):
                    try:
                        logger.info(f"å¤„ç†ç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£URL: {request.url}")
                        document_id = f"{session_id}_{uuid.uuid4().hex[:8]}"
                        tool_name = await register_user_document_tool(
                            url=request.url,
                            document_id=document_id,
                            document_name="ç”¨æˆ·ä¸Šä¼ çš„åŸºå› æ£€æµ‹æŠ¥å‘Š"
                        )
                        user_document_tools_list.append(tool_name)
                        logger.info(f"æˆåŠŸæ³¨å†Œç”¨æˆ·æ–‡æ¡£å·¥å…·: {tool_name}")
                    except Exception as e:
                        logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")
                initial_state = {
                    "messages": state.values["messages"] + [HumanMessage(content=request.message)],
                    "knowledge_base_name": knowledge_base_name,
                    "tool_call_count": 0,  # æ¯æ¬¡æ–°æ¶ˆæ¯éƒ½é‡ç½®å·¥å…·è°ƒç”¨æ¬¡æ•°
                    "user_document_tools": user_document_tools_list,
                    "web_search_enabled": web_search_enabled
                }
            
            # ä¼˜åŒ–åˆ†å—å¤§å°å’Œæµå¼å¤„ç†é€»è¾‘
            CHUNK_SIZE = 50  # å¢å¤§åˆ†å—å¤§å°ï¼Œå‡å°‘è¿‡åº¦åˆ†å‰²
            full_text = ""
            last_sent_length = 0  # è®°å½•å·²å‘é€çš„æ–‡æœ¬é•¿åº¦
            
            logger.info(f"ğŸš€ å¼€å§‹æµå¼å“åº”ï¼Œåˆ†å—å¤§å°: {CHUNK_SIZE}")
            
            # =============== æ–°å¢ï¼šè®°å½•æµå¼è¾“å‡ºå¼€å§‹ä¿¡æ¯ ===============
            chat_logger.info(f"ğŸŒŠ ====== æµå¼è¾“å‡ºAPIå¼€å§‹ ======")
            chat_logger.info(f"ğŸ†” ä¼šè¯ID: {session_id}")
            chat_logger.info(f"ğŸ†” æ¶ˆæ¯ID: {message_id}")
            chat_logger.info(f"ğŸ“ ç”¨æˆ·è¾“å…¥: {request.message}")
            chat_logger.info(f"ğŸ”§ çŸ¥è¯†åº“åç§°: {request.knowledge_base_name}")
            chat_logger.info(f"ğŸŒ Webæœç´¢å¯ç”¨: {request.enable_web_search}")
            if request.url:
                chat_logger.info(f"ğŸ“„ æ–‡æ¡£URL: {request.url}")
            # =============== æ–°å¢ç»“æŸ ===============
            
            # å‘é€å¼€å§‹æ ‡è®°
            start_data = {
                "text": "",
                "finish_reason": "start",
                "session_id": session_id,
                "message_id": message_id
            }
            yield f"data: {json.dumps(start_data)}\n"
            await asyncio.sleep(0.01)
            
            try:
                # æŒ‰ç…§ä½ çš„æ€è·¯ï¼šåœ¨astream_logè¿‡ç¨‹ä¸­å®æ—¶æ£€æµ‹å¯¹è¯ç»“æŸä¿¡å·
                chat_logger.info(f"ğŸ” å¼€å§‹å®æ—¶æ£€æµ‹å¯¹è¯æµç¨‹...")
                
                # ç”¨äºè·Ÿè¸ªå¯¹è¯çŠ¶æ€
                thinking_content = ""
                tool_results = []
                final_answer = ""
                conversation_ended = False
                
                async for step in graph.astream_log(initial_state, config=config):
                    if isinstance(step, dict) and "ops" in step:
                        ops = step["ops"]
                    elif hasattr(step, "ops"):
                        ops = step.ops
                    else:
                        continue
                    
                    for op in ops:
                        path = op.get("path", "") if isinstance(op, dict) else getattr(op, "path", "")
                        value = op.get("value") if isinstance(op, dict) else getattr(op, "value", None)
                        
                        # =============== æ–¹æ¡ˆä¸€ï¼šä¿®å¤è·¯å¾„åŒ¹é…é€»è¾‘ ===============
                        
                        # è®°å½•å…³é”®è·¯å¾„ï¼ŒåŒ…æ‹¬å·¥å…·è°ƒç”¨ç›¸å…³çš„è·¯å¾„
                        if value is not None and ("final_output" in path or "call_model" in path or "tools" in path or "search_" in path):
                            chat_logger.info(f"è·¯å¾„: {path}")
                        
                        # 1. å¤„ç†æ¨¡å‹è°ƒç”¨æ—¥å¿—ï¼ˆæ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆå›ç­”ï¼‰- æ›´ç²¾ç¡®çš„è·¯å¾„åŒ¹é…
                        if (path.startswith("/logs/call_model/") or 
                            path.startswith("/state/messages") or
                            path.endswith("/final_output") or
                            path.endswith("/streamed_output/-") or
                            "call_model" in path) and value is not None:
                            if isinstance(value, dict) and "messages" in value:
                                for msg in value["messages"]:
                                    if isinstance(msg, AIMessage) and hasattr(msg, "content") and msg.content:
                                        content = msg.content
                                        
                                        # æ£€æŸ¥æ˜¯å¦åŒ…å«<think>æ ‡ç­¾
                                        if "<think>" in content:
                                            # è¿™æ˜¯æ€è€ƒè¿‡ç¨‹
                                            thinking_content = content
                                            chat_logger.info(f"ğŸ’­ æ€è€ƒè¿‡ç¨‹: {len(content)}å­—ç¬¦")
                                            
                                            # å®æ—¶å‘é€æ€è€ƒè¿‡ç¨‹
                                            if content not in full_text:
                                                # åˆ†å—å‘é€æ€è€ƒå†…å®¹
                                                content_chunks = [content[i:i+CHUNK_SIZE] for i in range(0, len(content), CHUNK_SIZE)]
                                                for chunk in content_chunks:
                                                    data = {
                                                        "text": chunk,
                                                        "finish_reason": None,
                                                        "session_id": session_id,
                                                        "message_id": message_id
                                                    }
                                                    yield f"data: {json.dumps(data)}\n"
                                                    await asyncio.sleep(0.01)
                                                
                                                full_text += content
                                                last_sent_length = len(full_text)
                                                chat_logger.info(f"ğŸ“¤ æ€è€ƒè¿‡ç¨‹å·²å‘é€")
                                        else:
                                            # è¿™æ˜¯æœ€ç»ˆå›ç­”ï¼ˆä¸åŒ…å«<think>æ ‡ç­¾ï¼‰
                                            final_answer = content
                                            chat_logger.info(f"ğŸ¯ æœ€ç»ˆå›ç­”: {len(content)}å­—ç¬¦")
                                            
                                            # æ£€æŸ¥æ˜¯å¦å·²ç»å‘é€è¿‡è¿™ä¸ªå›ç­”
                                            if content not in full_text:
                                                # åˆ†å—å‘é€æœ€ç»ˆå›ç­”
                                                answer_chunks = [content[i:i+CHUNK_SIZE] for i in range(0, len(content), CHUNK_SIZE)]
                                                for chunk in answer_chunks:
                                                    data = {
                                                        "text": chunk,
                                                        "finish_reason": None,
                                                        "session_id": session_id,
                                                        "message_id": message_id
                                                    }
                                                    yield f"data: {json.dumps(data)}\n"
                                                    await asyncio.sleep(0.01)
                                                
                                                full_text += content
                                                last_sent_length = len(full_text)
                                                chat_logger.info(f"âœ… æœ€ç»ˆå›ç­”å·²å‘é€")
                                                
                                                # æ£€æµ‹åˆ°æœ€ç»ˆå›ç­”åï¼Œæ ‡è®°å¯¹è¯å³å°†ç»“æŸ
                                                conversation_ended = True
                                                chat_logger.info(f"ğŸ å¯¹è¯å³å°†ç»“æŸ")
                        
                        # 2. å¤„ç†å·¥å…·è°ƒç”¨ç»“æœï¼ˆä»tool_nodeçš„æ‰§è¡Œç»“æœä¸­è·å–ï¼‰- æ›´ç²¾ç¡®çš„è·¯å¾„åŒ¹é…
                        elif (path.startswith("/logs/tools/") or 
                              path.startswith("/logs/") or
                              "tools" in path or
                              "tool_node" in path or
                              "search_" in path or
                              path.endswith("/final_output")) and value is not None:
                            # æ£€æŸ¥æ˜¯å¦æ˜¯å·¥å…·æ‰§è¡Œç»“æœ
                            if isinstance(value, dict) and "messages" in value:
                                for msg in value["messages"]:
                                    if isinstance(msg, ToolMessage) and hasattr(msg, "content") and msg.content:
                                        tool_content = msg.content
                                        tool_name = getattr(msg, "name", "æœªçŸ¥å·¥å…·")
                                        
                                        # æ¸…ç†å·¥å…·è°ƒç”¨ç»“æœå†…å®¹
                                        if isinstance(tool_content, str):
                                            import re
                                            clean_content = re.sub(r'<[^>]+>', '', tool_content)
                                            clean_content = re.sub(r'https?://[^\s]+', '', clean_content)
                                            clean_content = re.sub(r'\n\s*\n', '\n', clean_content)
                                            clean_content = clean_content.strip()
                                        else:
                                            clean_content = str(tool_content)
                                        
                                        # æ£€æŸ¥æ˜¯å¦å·²ç»å‘é€è¿‡è¿™ä¸ªå·¥å…·ç»“æœ
                                        if clean_content not in [result["content"] for result in tool_results]:
                                            tool_results.append({
                                                "name": tool_name,
                                                "content": clean_content
                                            })
                                            
                                            # æ·»åŠ å·¥å…·è°ƒç”¨ç»“æœçš„é†’ç›®æ ‡è¯†
                                            tool_result_content = f"\n\n{'='*50}\nğŸ”§ å·¥å…·è°ƒç”¨ç»“æœ: {tool_name}\n{'='*50}\n{clean_content}\n{'='*50}\n"
                                            
                                            # åˆ†å—å‘é€å·¥å…·è°ƒç”¨ç»“æœ
                                            tool_chunks = [tool_result_content[i:i+CHUNK_SIZE] for i in range(0, len(tool_result_content), CHUNK_SIZE)]
                                            for chunk in tool_chunks:
                                                data = {
                                                    "text": chunk,
                                                    "finish_reason": None,
                                                    "session_id": session_id,
                                                    "message_id": message_id
                                                }
                                                yield f"data: {json.dumps(data)}\n"
                                                await asyncio.sleep(0.01)
                                            
                                            full_text += clean_content
                                            last_sent_length = len(full_text)
                                            chat_logger.info(f"ğŸ”§ å·¥å…·è°ƒç”¨ç»“æœå·²å‘é€: {tool_name}, é•¿åº¦: {len(clean_content)}")
                                            # =============== æ–°å¢ï¼šè®°å½•æµå¼è¾“å‡ºä¸­çš„å·¥å…·è¿”å›å®Œæ•´å†…å®¹ ===============
                                            chat_logger.info(f"ğŸ“¤ æµå¼è¾“å‡º - å·¥å…· {tool_name} è¿”å›å†…å®¹:")
                                            chat_logger.info(f"ğŸ“ {clean_content}")
                                            chat_logger.info(f"ğŸ“Š æµå¼è¾“å‡º - å·¥å…·è¿”å›å†…å®¹é•¿åº¦: {len(clean_content)} å­—ç¬¦")
                                            # =============== æ–°å¢ç»“æŸ ===============
                                        
                                        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¯¹è¯ç»“æŸä¿¡å·
                                        if "æ— å·¥å…·è°ƒç”¨ï¼Œå¯¹è¯ç»“æŸ" in str(msg.content):
                                            conversation_ended = True
                                            chat_logger.info(f"ğŸ ä»å·¥å…·æ¶ˆæ¯ä¸­æ£€æµ‹åˆ°å¯¹è¯ç»“æŸä¿¡å·")
                                    
                                    # æ£€æŸ¥AIæ¶ˆæ¯ä¸­æ˜¯å¦åŒ…å«å¯¹è¯ç»“æŸä¿¡å·
                                    elif isinstance(msg, AIMessage) and hasattr(msg, "content") and msg.content:
                                        if "æ— å·¥å…·è°ƒç”¨ï¼Œå¯¹è¯ç»“æŸ" in str(msg.content):
                                            conversation_ended = True
                                            chat_logger.info(f"ğŸ ä»AIæ¶ˆæ¯ä¸­æ£€æµ‹åˆ°å¯¹è¯ç»“æŸä¿¡å·")
                        

                        
                        # 3. æ£€æµ‹å¯¹è¯ç»“æŸä¿¡å· - æ›´ç²¾ç¡®çš„è·¯å¾„åŒ¹é…
                        elif (path.startswith("/logs/should_continue/") or
                              path.startswith("/logs/") or
                              "should_continue" in path or
                              "END" in str(value) or
                              "end" in path.lower()) and value is not None:
                            # æ£€æŸ¥æ˜¯å¦åŒ…å«"æ— å·¥å…·è°ƒç”¨ï¼Œå¯¹è¯ç»“æŸ"çš„ä¿¡å·
                            if isinstance(value, str) and "æ— å·¥å…·è°ƒç”¨ï¼Œå¯¹è¯ç»“æŸ" in value:
                                conversation_ended = True
                                chat_logger.info(f"ğŸ æ£€æµ‹åˆ°å¯¹è¯ç»“æŸä¿¡å·: {value}")
                            elif isinstance(value, dict) and value.get("messages"):
                                # æ£€æŸ¥æ¶ˆæ¯ä¸­æ˜¯å¦åŒ…å«ç»“æŸä¿¡å·
                                for msg in value["messages"]:
                                    if hasattr(msg, "content") and msg.content:
                                        if "æ— å·¥å…·è°ƒç”¨ï¼Œå¯¹è¯ç»“æŸ" in str(msg.content):
                                            conversation_ended = True
                                            chat_logger.info(f"ğŸ ä»æ¶ˆæ¯ä¸­æ£€æµ‹åˆ°å¯¹è¯ç»“æŸä¿¡å·")
                                            break
                        
                        # 4. ä¸“é—¨æ£€æµ‹å·¥å…·è°ƒç”¨ç»“æœ - æ›´å®½æ³›çš„æ£€æµ‹
                        if value is not None:
                            # æ£€æŸ¥æ˜¯å¦åŒ…å«ToolMessageï¼Œæ— è®ºè·¯å¾„å¦‚ä½•
                            if isinstance(value, dict) and "messages" in value:
                                chat_logger.info(f"ğŸ” æ£€æŸ¥è·¯å¾„ {path} çš„messageså­—æ®µï¼Œæ¶ˆæ¯æ•°é‡: {len(value['messages'])}")
                                for i, msg in enumerate(value["messages"]):
                                    chat_logger.info(f"ğŸ” æ¶ˆæ¯ {i}: ç±»å‹={type(msg).__name__}, å†…å®¹é•¿åº¦={len(str(msg.content)) if hasattr(msg, 'content') else 'N/A'}")
                                    if isinstance(msg, ToolMessage) and hasattr(msg, "content") and msg.content:
                                        chat_logger.info(f"ğŸ”§ å‘ç°å·¥å…·è°ƒç”¨ç»“æœ: {getattr(msg, 'name', 'æœªçŸ¥å·¥å…·')}, è·¯å¾„: {path}")
                                        tool_content = msg.content
                                        tool_name = getattr(msg, "name", "æœªçŸ¥å·¥å…·")
                                        
                                        # æ¸…ç†å·¥å…·è°ƒç”¨ç»“æœå†…å®¹
                                        if isinstance(tool_content, str):
                                            import re
                                            clean_content = re.sub(r'<[^>]+>', '', tool_content)
                                            clean_content = re.sub(r'https?://[^\s]+', '', clean_content)
                                            clean_content = re.sub(r'\n\s*\n', '\n', clean_content)
                                            clean_content = clean_content.strip()
                                        else:
                                            clean_content = str(tool_content)
                                        
                                        # æ£€æŸ¥æ˜¯å¦å·²ç»å‘é€è¿‡è¿™ä¸ªå·¥å…·ç»“æœ
                                        if clean_content not in [result["content"] for result in tool_results]:
                                            tool_results.append({
                                                "name": tool_name,
                                                "content": clean_content
                                            })
                                            
                                            # æ·»åŠ å·¥å…·è°ƒç”¨ç»“æœçš„é†’ç›®æ ‡è¯†
                                            tool_result_content = f"\n\n{'='*50}\nğŸ”§ å·¥å…·è°ƒç”¨ç»“æœ: {tool_name}\n{'='*50}\n{clean_content}\n{'='*50}\n"
                                            
                                            # åˆ†å—å‘é€å·¥å…·è°ƒç”¨ç»“æœ
                                            tool_chunks = [tool_result_content[i:i+CHUNK_SIZE] for i in range(0, len(tool_result_content), CHUNK_SIZE)]
                                            for chunk in tool_chunks:
                                                data = {
                                                    "text": chunk,
                                                    "finish_reason": None,
                                                    "session_id": session_id,
                                                    "message_id": message_id
                                                }
                                                yield f"data: {json.dumps(data)}\n"
                                                await asyncio.sleep(0.01)
                                            
                                            full_text += clean_content
                                            last_sent_length = len(full_text)
                                            chat_logger.info(f"ğŸ”§ å·¥å…·è°ƒç”¨ç»“æœå·²å‘é€: {tool_name}, é•¿åº¦: {len(clean_content)}")
                                            # =============== æ–°å¢ï¼šè®°å½•æµå¼è¾“å‡ºä¸­çš„å·¥å…·è¿”å›å®Œæ•´å†…å®¹ï¼ˆå¤‡ç”¨æ£€æµ‹ï¼‰ ===============
                                            chat_logger.info(f"ğŸ“¤ æµå¼è¾“å‡ºå¤‡ç”¨æ£€æµ‹ - å·¥å…· {tool_name} è¿”å›å†…å®¹:")
                                            chat_logger.info(f"ğŸ“ {clean_content}")
                                            chat_logger.info(f"ğŸ“Š æµå¼è¾“å‡ºå¤‡ç”¨æ£€æµ‹ - å·¥å…·è¿”å›å†…å®¹é•¿åº¦: {len(clean_content)} å­—ç¬¦")
                                            # =============== æ–°å¢ç»“æŸ ===============
                        
                        
                        # 5. å¤‡ç”¨æ£€æµ‹æœºåˆ¶ - æ•è·ä»»ä½•åŒ…å«AIMessageçš„è·¯å¾„
                        if value is not None:
                            # æ£€æŸ¥æ˜¯å¦åŒ…å«AIMessageï¼Œä½œä¸ºå¤‡ç”¨æ£€æµ‹
                            if isinstance(value, dict) and "messages" in value:
                                for msg in value["messages"]:
                                    if isinstance(msg, AIMessage) and hasattr(msg, "content") and msg.content:
                                        content = msg.content
                                        # å¦‚æœä¸åŒ…å«<think>æ ‡ç­¾ï¼Œå¯èƒ½æ˜¯æœ€ç»ˆå›ç­”
                                        if "<think>" not in content and len(content) > 50:  # è¿‡æ»¤æ‰å¤ªçŸ­çš„å†…å®¹
                                            chat_logger.info(f"ğŸ”„ å¤‡ç”¨æ£€æµ‹æœ€ç»ˆå›ç­”: {len(content)}å­—ç¬¦")
                                            
                                            # æ£€æŸ¥æ˜¯å¦å·²ç»å‘é€è¿‡è¿™ä¸ªå†…å®¹
                                            if content not in full_text:
                                                final_answer = content
                                                
                                                # åˆ†å—å‘é€æœ€ç»ˆå›ç­”
                                                answer_chunks = [content[i:i+CHUNK_SIZE] for i in range(0, len(content), CHUNK_SIZE)]
                                                for chunk in answer_chunks:
                                                    data = {
                                                        "text": chunk,
                                                        "finish_reason": None,
                                                        "session_id": session_id,
                                                        "message_id": message_id
                                                    }
                                                    yield f"data: {json.dumps(data)}\n"
                                                    await asyncio.sleep(0.01)
                                                
                                                full_text += content
                                                last_sent_length = len(full_text)
                                                conversation_ended = True
                                                chat_logger.info(f"âœ… å¤‡ç”¨æœºåˆ¶å·²å‘é€")
                        
                        # =============== åŸºäºä½ çš„æ€è·¯ç»“æŸ ===============
                
                # åŸºäºä½ çš„æ€è·¯ï¼šæ£€æµ‹åˆ°å¯¹è¯ç»“æŸä¿¡å·åï¼Œè®°å½•çŠ¶æ€ä½†ä¸å‘é€é‡å¤çš„ç»“æŸä¿¡å·
                if conversation_ended:
                    chat_logger.info(f"ğŸ æ£€æµ‹åˆ°å¯¹è¯ç»“æŸï¼Œæ€»å‘é€é•¿åº¦: {len(full_text)}")
                else:
                    chat_logger.warning(f"âš ï¸ æœªæ£€æµ‹åˆ°å¯¹è¯ç»“æŸä¿¡å·ï¼Œä½¿ç”¨é»˜è®¤ç»“æŸ")
                
                # è®°å½•æµå¼è¾“å‡ºå®Œæˆä¿¡æ¯
                chat_logger.info(f"ğŸŒŠ ====== æµå¼è¾“å‡ºAPIå®Œæˆ ======")
                chat_logger.info(f"ğŸ“Š æµå¼è¾“å‡ºç»Ÿè®¡:")
                chat_logger.info(f"   ğŸ“ æ€»å†…å®¹é•¿åº¦: {len(full_text)} å­—ç¬¦")
                chat_logger.info(f"   ğŸ“¤ å·²å‘é€é•¿åº¦: {last_sent_length} å­—ç¬¦")
                chat_logger.info(f"   ğŸ“‹ åˆ†å—å¤§å°: {CHUNK_SIZE}")
                chat_logger.info(f"   ğŸ ç»“æŸçŠ¶æ€: {'æ­£å¸¸ç»“æŸ' if conversation_ended else 'é»˜è®¤ç»“æŸ'}")
            except Exception as e:
                logger.error(f"âŒ æµå¼å“åº”å¤±è´¥: {e}")
                # æ˜¾å­˜æ¸…ç†
                import gc
                gc.collect()
                raise e
            
            # å‘é€å‰©ä½™å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
            if full_text and len(full_text) > last_sent_length:
                remaining_text = full_text[last_sent_length:]
                if remaining_text:
                    data = {
                        "text": remaining_text,
                        "finish_reason": None,
                        "session_id": session_id,
                        "message_id": message_id
                    }
                    yield f"data: {json.dumps(data)}\n"
                    await asyncio.sleep(0.01)
                    logger.debug(f"ğŸ“¤ å‘é€æœ€ç»ˆå‰©ä½™å†…å®¹ï¼Œé•¿åº¦: {len(remaining_text)}")
                    
                    # =============== æ–°å¢ï¼šè®°å½•æœ€ç»ˆå‰©ä½™å†…å®¹å‘é€ ===============
                    chat_logger.info(f"ğŸ“¤ æµå¼æœ€ç»ˆå‰©ä½™å†…å®¹å‘é€: é•¿åº¦={len(remaining_text)}, å†…å®¹={remaining_text}")
                    # =============== æ–°å¢ç»“æŸ ===============
            
            # ç»Ÿä¸€å‘é€ç»“æŸæ ‡è®°ï¼ˆé¿å…é‡å¤ï¼‰
            end_data = {
                "text": "",
                "finish_reason": "stop",
                "session_id": session_id,
                "message_id": message_id
            }
            yield f"data: {json.dumps(end_data)}\n"
            await asyncio.sleep(0.01)
            
            # =============== æ–°å¢ï¼šè®°å½•æµå¼è¾“å‡ºç»“æŸä¿¡æ¯ ===============
            chat_logger.info(f"ğŸŒŠ ====== æµå¼è¾“å‡ºAPIç»“æŸ ======")
            chat_logger.info(f"ğŸ†” ä¼šè¯ID: {session_id}")
            chat_logger.info(f"ğŸ†” æ¶ˆæ¯ID: {message_id}")
            chat_logger.info(f"ğŸ“Š æµå¼è¾“å‡ºç»Ÿè®¡:")
            chat_logger.info(f"   ğŸ“ æ€»å†…å®¹é•¿åº¦: {len(full_text)} å­—ç¬¦")
            chat_logger.info(f"   ğŸ“¤ å·²å‘é€é•¿åº¦: {last_sent_length} å­—ç¬¦")
            chat_logger.info(f"   ğŸ“‹ åˆ†å—å¤§å°: {CHUNK_SIZE}")
            chat_logger.info(f"ğŸŒŠ ====== æµå¼è¾“å‡ºAPIç»“æŸ ======")
            
            # ä¿å­˜æµå¼èŠå¤©æ—¥å¿—åˆ°æ–‡ä»¶
            try:
                chat_log_entry = {
                    "timestamp": datetime.datetime.now().isoformat(),
                    "session_id": session_id,
                    "message_id": message_id,
                    "user_message": request.message,
                    "assistant_response": full_text,
                    "knowledge_base": request.knowledge_base_name,
                    "web_search_enabled": request.enable_web_search,
                    "response_type": "streaming",
                    "total_length": len(full_text),
                    "sent_length": last_sent_length
                }
                
                # ä¿å­˜åˆ°èŠå¤©æ—¥å¿—æ–‡ä»¶
                chat_log_file = os.path.join('logs', f"chat_logs_{datetime.datetime.now().strftime('%Y%m%d')}.log")
                with open(chat_log_file, 'a', encoding='utf-8') as f:
                    f.write(f"[{chat_log_entry['timestamp']}] Session: {chat_log_entry['session_id']} (Streaming)\n")
                    f.write(f"Message ID: {chat_log_entry['message_id']}\n")
                    f.write(f"User: {chat_log_entry['user_message']}\n")
                    f.write(f"Assistant: {chat_log_entry['assistant_response']}\n")
                    f.write(f"Knowledge Base: {chat_log_entry['knowledge_base']}\n")
                    f.write(f"Web Search: {'Enabled' if chat_log_entry['web_search_enabled'] else 'Disabled'}\n")
                    f.write(f"Response Type: {chat_log_entry['response_type']}\n")
                    f.write(f"Total Length: {chat_log_entry['total_length']}\n")
                    f.write(f"Sent Length: {chat_log_entry['sent_length']}\n")
                    f.write("-" * 80 + "\n\n")
                
                chat_logger.info(f"ğŸ’¾ æµå¼èŠå¤©æ—¥å¿—å·²ä¿å­˜åˆ°: {chat_log_file}")
            except Exception as e:
                chat_logger.error(f"âŒ ä¿å­˜æµå¼èŠå¤©æ—¥å¿—å¤±è´¥: {str(e)}")
            # =============== æ–°å¢ç»“æŸ ===============
            
            # æœ€ç»ˆæ˜¾å­˜æ¸…ç†
            import gc
            gc.collect()
            logger.info(f"ğŸ§¹ æµå¼å“åº”å®Œæˆï¼Œæ‰§è¡Œæœ€ç»ˆæ˜¾å­˜æ¸…ç†ã€‚æ€»å†…å®¹é•¿åº¦: {len(full_text)}, å·²å‘é€é•¿åº¦: {last_sent_length}")
            
            # =============== æœ€å°åŒ–ä¿®å¤æ€»ç»“ ===============
            # 1. âœ… è§£å†³äº†é‡å¤å‘é€ç»“æŸä¿¡å·çš„é—®é¢˜ï¼ˆæ ¸å¿ƒé—®é¢˜ï¼‰
            # 2. ğŸ”„ ä¿æŒäº†åŸæœ‰çš„å¤æ‚æ£€æµ‹é€»è¾‘ä¸å˜
            # 3. ğŸ”„ ä¿æŒäº†æ‰€æœ‰å¤‡ç”¨æ£€æµ‹æœºåˆ¶
            # 4. ğŸ”„ ä¿æŒäº†åŸæœ‰çš„è·¯å¾„åŒ¹é…é€»è¾‘
            # 5. âœ… ç¡®ä¿åªå‘é€ä¸€æ¬¡ç»“æŸä¿¡å·
            # 
            # è¯´æ˜ï¼šè¿™æ˜¯æœ€å°åŒ–ä¿®å¤æ–¹æ¡ˆï¼Œåªè§£å†³é‡å¤å‘é€é—®é¢˜ï¼Œ
            # ä¸æ”¹å˜åŸæœ‰çš„æ£€æµ‹é€»è¾‘ï¼Œé™ä½ä¿®æ”¹é£é™©
            # =============== æœ€å°åŒ–ä¿®å¤æ€»ç»“ç»“æŸ ===============
        except Exception as e:
            logger.error(f"âŒ event_generator å¤±è´¥: {e}")
            # å‘é€é”™è¯¯ä¿¡æ¯
            error_data = {
                "text": f"é”™è¯¯: {str(e)}",
                "finish_reason": "error",
                "session_id": session_id,
                "message_id": message_id
            }
            yield f"data: {json.dumps(error_data)}\n"
            
            # =============== æ–°å¢ï¼šè®°å½•æµå¼è¾“å‡ºé”™è¯¯ä¿¡æ¯ ===============
            chat_logger.error(f"âŒ æµå¼è¾“å‡ºAPIé”™è¯¯:")
            chat_logger.error(f"   ğŸ†” ä¼šè¯ID: {session_id}")
            chat_logger.error(f"   ğŸ†” æ¶ˆæ¯ID: {message_id}")
            chat_logger.error(f"   âŒ é”™è¯¯ä¿¡æ¯: {str(e)}")
            chat_logger.error(f"   ğŸ“š é”™è¯¯ç±»å‹: {type(e).__name__}")
            # =============== æ–°å¢ç»“æŸ ===============
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream"  # ç¡®ä¿è¿™æ˜¯æ­£ç¡®çš„åª’ä½“ç±»å‹
    )

# 10. ä¼šè¯ç®¡ç†API
@agent_router.get("/sessions")
async def get_user_sessions(request: Request):
    """è·å–å½“å‰ç”¨æˆ·çš„æ‰€æœ‰ä¼šè¯åˆ—è¡¨"""
    try:
        # è·å–å½“å‰ç”¨æˆ·ä¿¡æ¯
        current_user = get_current_user_from_token(request)
        if not current_user:
            raise HTTPException(status_code=401, detail="æœªæˆæƒè®¿é—®")
        
        user_id = current_user.id
        user_sessions = []
        
        # ä»ä¸“é—¨çš„å¯¹è¯å†å²æ•°æ®åº“è·å–ä¼šè¯æ•°æ®
        sessions = []
        try:
            # ä»ä¸“é—¨çš„å¯¹è¯å†å²æ•°æ®åº“è·å–ä¼šè¯åˆ—è¡¨
            async with chat_history_pool.connection() as conn:
                async with conn.cursor() as cur:
                    await cur.execute("""
                        SELECT conversation_id, title, updated_at, message_count
                        FROM conversations 
                        WHERE user_id = %s 
                        ORDER BY updated_at DESC
                    """, (user_id,))
                    
                    rows = await cur.fetchall()
                    for row in rows:
                        conversation_id, title, updated_at, message_count = row
                        sessions.append({
                            "session_id": conversation_id,
                            "session_name": title or "æ–°å¯¹è¯",
                            "last_updated": str(updated_at),
                            "message_count": message_count or 0
                        })
            
            logger.info(f"ä»ä¸“ç”¨æ•°æ®åº“è·å–ç”¨æˆ·å¯¹è¯: {user_id}, æ‰¾åˆ° {len(sessions)} ä¸ªå¯¹è¯")
            
        except Exception as e:
            logger.error(f"ä»ä¸“ç”¨æ•°æ®åº“è·å–ä¼šè¯å¤±è´¥: {str(e)}")
        
        return {
            "user_id": user_id,
            "sessions": sessions
        }
    except Exception as e:
        logger.error(f"âŒ è·å–ç”¨æˆ·ä¼šè¯åˆ—è¡¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–ä¼šè¯åˆ—è¡¨å¤±è´¥: {str(e)}")

@agent_router.get("/conversations/{user_id}")
async def get_user_conversations(user_id: str, request: Request):
    """è·å–ç”¨æˆ·çš„æ‰€æœ‰å¯¹è¯åˆ—è¡¨"""
    try:
        # éªŒè¯ç”¨æˆ·æƒé™
        current_user = get_current_user_from_token(request)
        if not current_user or str(current_user.id) != str(user_id):
            raise HTTPException(status_code=403, detail="æ— æƒé™è®¿é—®è¯¥ç”¨æˆ·çš„å¯¹è¯")
        
        # ä»checkpointerè·å–ç”¨æˆ·çš„å¯¹è¯åˆ—è¡¨
        global checkpointer
        conversations = []
        
        if checkpointer:
            try:
                # è¿™é‡Œåº”è¯¥æŸ¥è¯¢æ•°æ®åº“ä¸­æ‰€æœ‰å±äºè¯¥ç”¨æˆ·çš„å¯¹è¯
                # ç”±äºcheckpointerçš„é™åˆ¶ï¼Œæˆ‘ä»¬æš‚æ—¶ä½¿ç”¨å·²çŸ¥çš„ä¼šè¯IDæ¨¡å¼
                # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œåº”è¯¥æœ‰ä¸€ä¸ªä¸“é—¨çš„conversationsè¡¨æ¥å­˜å‚¨å¯¹è¯å…ƒæ•°æ®
                
                # æ£€æŸ¥é»˜è®¤ä¼šè¯
                default_session_id = f"user_{user_id}_default"
                config = {"configurable": {"thread_id": default_session_id}}
                state = await checkpointer.aget_state(config)
                
                if state and state.values and "messages" in state.values:
                    messages = state.values["messages"]
                    if messages and len(messages) > 0:
                        # è·å–ç¬¬ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä½œä¸ºæ ‡é¢˜
                        first_user_msg = None
                        for msg in messages:
                            if hasattr(msg, 'type') and msg.type == 'human':
                                first_user_msg = msg.content
                                break
                        
                        title = first_user_msg[:30] + "..." if first_user_msg and len(first_user_msg) > 30 else (first_user_msg or "æ–°å¯¹è¯")
                        
                        conversations.append({
                            "conversation_id": default_session_id,
                            "title": title,
                            "created_at": state.config.get("configurable", {}).get("checkpoint_id", ""),
                            "message_count": len(messages)
                        })
            except Exception as e:
                logger.error(f"ä»checkpointerè·å–å¯¹è¯å¤±è´¥: {str(e)}")
        
        logger.info(f"ä»æ•°æ®åº“è·å–ç”¨æˆ·å¯¹è¯: {user_id}, æ‰¾åˆ° {len(conversations)} ä¸ªå¯¹è¯")
        
        return {
            "status": "success",
            "user_id": user_id,
            "conversations": conversations
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–ç”¨æˆ·å¯¹è¯å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–ç”¨æˆ·å¯¹è¯å¤±è´¥: {str(e)}")

@agent_router.get("/conversations/{user_id}/{conversation_id}")
async def get_conversation_history(user_id: str, conversation_id: str, request: Request):
    """è·å–ç‰¹å®šå¯¹è¯çš„å†å²è®°å½•"""
    try:
        # éªŒè¯ç”¨æˆ·æƒé™
        current_user = get_current_user_from_token(request)
        if not current_user or str(current_user.id) != str(user_id):
            raise HTTPException(status_code=403, detail="æ— æƒé™è®¿é—®è¯¥å¯¹è¯")
        
        # ä»checkpointerè·å–å¯¹è¯å†å²
        global checkpointer
        history = []
        
        if checkpointer:
            try:
                # ä½¿ç”¨conversation_idä½œä¸ºsession_idæ¥è·å–çŠ¶æ€
                config = {"configurable": {"thread_id": conversation_id}}
                state = await checkpointer.aget_state(config)
                
                if state and state.values and "messages" in state.values:
                    messages = state.values["messages"]
                    for msg in messages:
                        if hasattr(msg, 'type') and hasattr(msg, 'content'):
                            role = "user" if msg.type == "human" else "assistant"
                            history.append({
                                "role": role,
                                "content": msg.content
                            })
            except Exception as e:
                logger.error(f"ä»checkpointerè·å–å¯¹è¯å†å²å¤±è´¥: {str(e)}")
        
        return {
            "status": "success",
            "conversation_id": conversation_id,
            "history": history
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–å¯¹è¯å†å²å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–å¯¹è¯å†å²å¤±è´¥: {str(e)}")

@agent_router.post("/conversations")
async def create_conversation(request: Request):
    """åˆ›å»ºæ–°å¯¹è¯"""
    try:
        # è·å–å½“å‰ç”¨æˆ·ä¿¡æ¯
        current_user = get_current_user_from_token(request)
        user_id = current_user.id if current_user else "anonymous"
        
        # ç”Ÿæˆæ–°çš„å¯¹è¯ID
        conversation_id = f"conv_{user_id}_{int(time.time())}"
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        # è¿™é‡Œéœ€è¦å®ç°æ•°æ®åº“ä¿å­˜é€»è¾‘
        
        return {
            "status": "success",
            "conversation_id": conversation_id,
            "message": "å¯¹è¯åˆ›å»ºæˆåŠŸ"
        }
    except Exception as e:
        logger.error(f"åˆ›å»ºå¯¹è¯å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºå¯¹è¯å¤±è´¥: {str(e)}")

@agent_router.post("/conversations/save")
async def save_conversation(request: Request):
    """ä¿å­˜å¯¹è¯åˆ°æ•°æ®åº“"""
    try:
        # è·å–å½“å‰ç”¨æˆ·ä¿¡æ¯
        current_user = get_current_user_from_token(request)
        user_id = current_user.id if current_user else "anonymous"
        
        # è§£æè¯·æ±‚æ•°æ®
        data = await request.json()
        conversation_id = data.get("conversation_id")
        title = data.get("title", "æ–°å¯¹è¯")
        messages = data.get("messages", [])
        
        if not conversation_id:
            raise HTTPException(status_code=400, detail="conversation_id is required")
        
        # ä½¿ç”¨checkpointerä¿å­˜å¯¹è¯åˆ°æ•°æ®åº“
        global checkpointer
        if checkpointer:
            try:
                # å°†æ¶ˆæ¯è½¬æ¢ä¸ºLangChainæ¶ˆæ¯æ ¼å¼
                from langchain_core.messages import HumanMessage, AIMessage
                langchain_messages = []
                
                for msg in messages:
                    if msg["role"] == "user":
                        langchain_messages.append(HumanMessage(content=msg["content"]))
                    elif msg["role"] == "assistant":
                        langchain_messages.append(AIMessage(content=msg["content"]))
                
                # ä½¿ç”¨ä¸“é—¨çš„å¯¹è¯å†å²æ•°æ®åº“ä¿å­˜
                try:
                    async with chat_history_pool.connection() as conn:
                        async with conn.cursor() as cur:
                            # æ’å…¥æˆ–æ›´æ–°å¯¹è¯è®°å½•
                            await cur.execute("""
                                INSERT INTO conversations (conversation_id, user_id, title, message_count, updated_at)
                                VALUES (%s, %s, %s, %s, CURRENT_TIMESTAMP)
                                ON CONFLICT (conversation_id) 
                                DO UPDATE SET 
                                    title = EXCLUDED.title,
                                    message_count = EXCLUDED.message_count,
                                    updated_at = CURRENT_TIMESTAMP
                            """, (conversation_id, user_id, title, len(messages)))
                            
                            # åˆ é™¤æ—§æ¶ˆæ¯
                            await cur.execute("DELETE FROM messages WHERE conversation_id = %s", (conversation_id,))
                            
                            # æ’å…¥æ–°æ¶ˆæ¯
                            for msg in messages:
                                await cur.execute("""
                                    INSERT INTO messages (conversation_id, role, content)
                                    VALUES (%s, %s, %s)
                                """, (conversation_id, msg["role"], msg["content"]))
                            
                            await conn.commit()
                            logger.info(f"å¯¹è¯å·²ä¿å­˜åˆ°ä¸“ç”¨æ•°æ®åº“: {conversation_id}, ç”¨æˆ·: {user_id}, æ¶ˆæ¯æ•°: {len(messages)}")
                            
                except Exception as e:
                    logger.error(f"ä¿å­˜åˆ°ä¸“ç”¨æ•°æ®åº“å¤±è´¥: {str(e)}")
                    logger.info(f"å¯¹è¯æ•°æ®å·²å‡†å¤‡: {conversation_id}, æ¶ˆæ¯æ•°: {len(messages)}")
                
                logger.info(f"å¯¹è¯å·²ä¿å­˜åˆ°æ•°æ®åº“: {conversation_id}, ç”¨æˆ·: {user_id}, æ¶ˆæ¯æ•°: {len(messages)}")
                
                return {
                    "status": "success",
                    "conversation_id": conversation_id,
                    "message": "å¯¹è¯ä¿å­˜æˆåŠŸ"
                }
            except Exception as e:
                logger.error(f"ä¿å­˜å¯¹è¯åˆ°checkpointerå¤±è´¥: {str(e)}")
                raise HTTPException(status_code=500, detail=f"ä¿å­˜å¯¹è¯åˆ°æ•°æ®åº“å¤±è´¥: {str(e)}")
        else:
            logger.error("checkpointeræœªåˆå§‹åŒ–")
            raise HTTPException(status_code=503, detail="æ•°æ®åº“æœåŠ¡æœªåˆå§‹åŒ–")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ä¿å­˜å¯¹è¯å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ä¿å­˜å¯¹è¯å¤±è´¥: {str(e)}")

@agent_router.get("/sessions/{session_id}")
async def get_session(session_id: str):
    """è·å–ç‰¹å®šä¼šè¯çš„å®Œæ•´å†å²"""
    # æ£€æŸ¥session_idæ˜¯å¦æœ‰æ•ˆ
    if not session_id or session_id == "undefined":
        raise HTTPException(status_code=400, detail="æ— æ•ˆçš„ä¼šè¯ID")
        
    try:
        # ä»ä¸“é—¨çš„å¯¹è¯å†å²æ•°æ®åº“è·å–ä¼šè¯å†å²
        async with chat_history_pool.connection() as conn:
            async with conn.cursor() as cur:
                # è·å–ä¼šè¯åŸºæœ¬ä¿¡æ¯
                await cur.execute("""
                    SELECT user_id, title, updated_at, message_count
                    FROM conversations 
                    WHERE conversation_id = %s
                """, (session_id,))
                
                conv_row = await cur.fetchone()
                if not conv_row:
                    return {
                        "session_id": session_id,
                        "conversation_history": [],
                        "last_updated": None
                    }
                
                user_id, title, updated_at, message_count = conv_row
                
                # è·å–æ¶ˆæ¯å†å²
                await cur.execute("""
                    SELECT role, content, created_at
                    FROM messages 
                    WHERE conversation_id = %s 
                    ORDER BY created_at ASC
                """, (session_id,))
                
                message_rows = await cur.fetchall()
                history = []
                for row in message_rows:
                    role, content, created_at = row
                    history.append({
                        "role": role,
                        "content": content
                    })
                
                return {
                    "session_id": session_id,
                    "conversation_history": history,
                    "last_updated": str(updated_at)
                }
                
    except Exception as e:
        logger.error(f"âŒ è·å–ä¼šè¯å¤±è´¥: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"è·å–ä¼šè¯å¤±è´¥: {str(e)}"
        )

@agent_router.delete("/sessions/{session_id}")
async def delete_session(session_id: str):
    """åˆ é™¤ç‰¹å®šä¼šè¯"""
    try:
        # ä»ä¸“é—¨çš„å¯¹è¯å†å²æ•°æ®åº“åˆ é™¤ä¼šè¯
        async with chat_history_pool.connection() as conn:
            async with conn.cursor() as cur:
                # åˆ é™¤æ¶ˆæ¯ï¼ˆå¤–é”®çº¦æŸä¼šè‡ªåŠ¨å¤„ç†ï¼‰
                await cur.execute("DELETE FROM messages WHERE conversation_id = %s", (session_id,))
                # åˆ é™¤ä¼šè¯
                await cur.execute("DELETE FROM conversations WHERE conversation_id = %s", (session_id,))
                await conn.commit()
                
                return {"status": "success", "message": f"ä¼šè¯ {session_id} å·²åˆ é™¤"}
                
    except Exception as e:
        logger.error(f"âŒ åˆ é™¤ä¼šè¯å¤±è´¥: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"åˆ é™¤ä¼šè¯å¤±è´¥: {str(e)}"
        )

# ======================
# ç»Ÿä¸€å¥åº·æ£€æŸ¥å’Œä¸»åº”ç”¨
# ======================
@asynccontextmanager
async def lifespan(app: FastAPI):
    global qdrant_client, checkpointer, web_search_tool, graph
    try:
        # 1. åˆå§‹åŒ–Qdrantå®¢æˆ·ç«¯ï¼ˆçŸ¥è¯†åº“ç®¡ç†éœ€è¦ï¼‰
        logger.info("åˆå§‹åŒ–Qdrantå®¢æˆ·ç«¯...")
        qdrant_host = os.getenv("QDRANT_HOST", "localhost")
        qdrant_port = int(os.getenv("QDRANT_PORT", "6333"))
        qdrant_client = QdrantClient(host=qdrant_host, port=qdrant_port)
        logger.info("âœ… Qdrantå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        
        # 2. åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± ï¼ˆAgentéœ€è¦ï¼‰
        logger.info("åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± ...")
        # å¯¹è¯å†å²ä¸“ç”¨æ•°æ®åº“ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        CHAT_HISTORY_DB_URI = os.getenv(
            "CHAT_HISTORY_DB_URI",
            "postgresql://chat_history_user:chat_history_pass@localhost:5432/chat_history_db?sslmode=disable"
        )
        # åŸæœ‰çš„LangGraphæ•°æ®åº“ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        DB_URI = os.getenv(
            "DB_URI",
            "postgresql://postgres:postgres@localhost:5432/langgraph_db?sslmode=disable"
        )
        connection_kwargs = {
            "autocommit": True,
            "prepare_threshold": 0,
        }
        # LangGraphæ•°æ®åº“è¿æ¥æ± 
        pool = AsyncConnectionPool(
            conninfo=DB_URI,
            max_size=20,
            kwargs=connection_kwargs
        )
        await pool.open()
        logger.info("åˆå§‹åŒ–æ•°æ®åº“æ£€æŸ¥ç‚¹å­˜å‚¨...")
        checkpointer = AsyncPostgresSaver(pool)
        await checkpointer.setup()
        logger.info("âœ… LangGraphæ•°æ®åº“æ£€æŸ¥ç‚¹å­˜å‚¨åˆå§‹åŒ–æˆåŠŸ")
        
        # å¯¹è¯å†å²æ•°æ®åº“è¿æ¥æ± 
        global chat_history_pool
        chat_history_pool = AsyncConnectionPool(
            conninfo=CHAT_HISTORY_DB_URI,
            max_size=10,
            kwargs=connection_kwargs
        )
        await chat_history_pool.open()
        logger.info("âœ… å¯¹è¯å†å²æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–æˆåŠŸ")
        
        # è®¾ç½®PDFæ¨¡å—çš„æ•°æ®åº“è¿æ¥æ± å¼•ç”¨
        set_db_pool(pool)
        logger.info("âœ… PDFæ¨¡å—æ•°æ®åº“è¿æ¥æ± å¼•ç”¨å·²è®¾ç½®")
        
        # åˆå§‹åŒ–ç”¨æˆ·è®¤è¯ç³»ç»Ÿ
        logger.info("åˆå§‹åŒ–ç”¨æˆ·è®¤è¯ç³»ç»Ÿ...")
        await create_users_table(pool)
        set_global_pool(pool)
        set_auth_user_manager(pool)
        
        # åˆå§‹åŒ–USTC OAuthï¼ˆå¦‚æœé…ç½®äº†ç¯å¢ƒå˜é‡ï¼‰
        # è·å–åº”ç”¨çš„åŸºç¡€URLï¼Œæ”¯æŒä»ç¯å¢ƒå˜é‡è¯»å–
        base_url = os.getenv("BASE_URL", "http://localhost:8000")
        init_ustc_oauth(base_url)
        
        logger.info("âœ… ç”¨æˆ·è®¤è¯ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
        
        # ä»æ•°æ®åº“åŠ è½½ç”¨æˆ·æ–‡æ¡£å·¥å…·
        try:
            await load_user_document_tools_from_db()
            logger.info("âœ… ç”¨æˆ·æ–‡æ¡£å·¥å…·åŠ è½½å®Œæˆ")
        except Exception as e:
            logger.warning(f"âš ï¸ åŠ è½½ç”¨æˆ·æ–‡æ¡£å·¥å…·å¤±è´¥: {str(e)}")
        
        # 3. åˆå§‹åŒ–webæœç´¢å·¥å…·
        try:
            logger.info("åˆå§‹åŒ–webæœç´¢å·¥å…·...")
            web_search_tool = create_search_tool()
            logger.info("âœ… å·¥å…·åŠ è½½æˆåŠŸ: web_search")
        except Exception as e:
            logger.error(f"âŒ å·¥å…·åŠ è½½å¤±è´¥: {str(e)}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œç»§ç»­åˆå§‹åŒ–å…¶ä»–ç»„ä»¶
        
        # 4. ç¼–è¯‘Agentå›¾
        logger.info("ç¼–è¯‘Agentå›¾...")
        builder = StateGraph(AgentState)
        builder.add_node("call_model", call_model)
        builder.add_node("tools", tool_node)
        builder.add_edge(START, "call_model")
        builder.add_conditional_edges(
            "call_model",
            should_continue,
            {
                "tools": "tools",
                END: END
            }
        )
        builder.add_edge("tools", "call_model")
        # ç¼–è¯‘å›¾
        graph = builder.compile(checkpointer=checkpointer)
        logger.info("âœ… Agentå›¾ç¼–è¯‘æˆåŠŸ")
        
        yield  # åº”ç”¨è¿è¡Œä¸­
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–å¤±è´¥: {str(e)}", exc_info=True)
        raise
    finally:
        # æ¸…ç†èµ„æº
        if 'pool' in locals():
            await pool.close()
            logger.info("âœ… æ•°æ®åº“è¿æ¥æ± å·²å…³é—­")

# åˆ›å»ºç»Ÿä¸€çš„FastAPIåº”ç”¨
# æ”¯æŒå­è·¯å¾„éƒ¨ç½²
app = FastAPI(
    title="ç»Ÿä¸€çŸ¥è¯†åº“ä¸å¯¹è¯AgentæœåŠ¡",
    description="æ•´åˆçŸ¥è¯†åº“ç®¡ç†å’Œå¯¹è¯AgentåŠŸèƒ½",
    version="1.0.0",
    lifespan=lifespan,
    root_path="/nsrlchat"  # æ”¯æŒå­è·¯å¾„éƒ¨ç½²
)

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ·»åŠ æ–‡ä»¶ä¸Šä¼ ä¸­é—´ä»¶æ”¯æŒ
from fastapi.middleware.gzip import GZipMiddleware
app.add_middleware(GZipMiddleware, minimum_size=1000)

# æ·»åŠ å®‰å…¨å“åº”å¤´ä¸­é—´ä»¶ï¼ˆé˜²æ­¢HTTPå“åº”å¤´æ³¨å…¥æ”»å‡»ï¼‰
app.add_middleware(SecurityHeadersMiddleware)

# æ·»åŠ è®¤è¯ä¸­é—´ä»¶
app.add_middleware(create_auth_middleware())

# æ–‡æ¡£é¢„è§ˆAPI
@kb_router.get("/api/document/{kb_name}/{document_name}/preview")
async def preview_document(kb_name: str, document_name: str):
    """é¢„è§ˆæ–‡æ¡£å†…å®¹ - ä¼˜å…ˆä½¿ç”¨åŸæ–‡ä»¶å†…å®¹"""
    logger.info(f"é¢„è§ˆæ–‡æ¡£è¯·æ±‚: kb_name={kb_name}, document_name={document_name}")
    try:
        # ä»Qdrantä¸­è·å–æ–‡æ¡£å†…å®¹
        collection_name = kb_name
        client = QdrantClient(host="localhost", port=6333)
        
        # æœç´¢åŒ…å«è¯¥æ–‡æ¡£çš„ç‚¹ - æ”¯æŒéƒ¨åˆ†åŒ¹é…
        search_result = client.scroll(
            collection_name=collection_name,
            scroll_filter=qdrant_models.Filter(
                must=[
                    qdrant_models.FieldCondition(
                        key="metadata.source",
                        match=qdrant_models.MatchText(text=document_name)  # ä½¿ç”¨æ–‡æœ¬åŒ¹é…è€Œä¸æ˜¯ç²¾ç¡®åŒ¹é…
                    )
                ]
            ),
            limit=1  # åªéœ€è¦ä¸€ä¸ªç‚¹å°±èƒ½è·å–åŸæ–‡ä»¶å†…å®¹
        )
        
        logger.info(f"é¢„è§ˆæ–‡æ¡£æŸ¥è¯¢ç»“æœ: {len(search_result[0]) if search_result[0] else 0} ä¸ªç‚¹")
        
        if not search_result[0]:  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£
            return {
                "success": False,
                "message": f"æ–‡æ¡£ '{document_name}' ä¸å­˜åœ¨",
                "data": {}
            }
        
        # è·å–ç¬¬ä¸€ä¸ªç‚¹çš„payload
        first_point = search_result[0][0]
        payload = first_point.payload
        
        # ä¼˜å…ˆä½¿ç”¨åŸæ–‡ä»¶å†…å®¹
        original_content = payload.get("original_content", "")
        source_name = payload.get("source_name", document_name)
        
        if original_content:
            # ä½¿ç”¨åŸæ–‡ä»¶å†…å®¹
            logger.info(f"ä½¿ç”¨åŸæ–‡ä»¶å†…å®¹é¢„è§ˆ: {source_name}")
            
            # å¤„ç†å›¾ç‰‡è·¯å¾„ï¼šå°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
            import re
            processed_content = original_content
            
            # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡å¼•ç”¨ ![](image_path)
            def replace_image_path(match):
                image_path = match.group(1)
                # å¦‚æœå·²ç»æ˜¯ç»å¯¹è·¯å¾„ï¼Œç›´æ¥è¿”å›
                if image_path.startswith('/') or image_path.startswith('http'):
                    return match.group(0)
                
                # å°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
                # æ ¼å¼ï¼š![](image_path) -> ![](/marker_outputs/{base_name}/{base_name}_images/{image_path})
                base_name = os.path.splitext(source_name)[0]
                absolute_path = f"/marker_outputs/{base_name}/{base_name}_images/{image_path}"
                return f"![]({absolute_path})"
            
            # æ›¿æ¢æ‰€æœ‰å›¾ç‰‡è·¯å¾„
            processed_content = re.sub(r'!\[([^\]]*)\]\(([^)]+)\)', replace_image_path, processed_content)
            
            return {
                "success": True,
                "message": f"æ–‡æ¡£ '{source_name}' é¢„è§ˆå†…å®¹ï¼ˆåŸæ–‡ä»¶ï¼‰",
                "data": {
                    "document_name": source_name,
                    "content": processed_content,
                    "content_type": "original"
                }
            }
        else:
            # å›é€€åˆ°åˆ†å—é‡ç»„æ–¹å¼ï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰
            logger.info("åŸæ–‡ä»¶å†…å®¹ä¸å­˜åœ¨ï¼Œä½¿ç”¨åˆ†å—é‡ç»„æ–¹å¼")
            return await preview_document_fallback(kb_name, document_name, client)
        
    except Exception as e:
        logger.error(f"é¢„è§ˆæ–‡æ¡£å¤±è´¥: {str(e)}")
        return {
            "success": False,
            "message": f"é¢„è§ˆæ–‡æ¡£å¤±è´¥: {str(e)}",
            "data": {}
        }


async def preview_document_fallback(kb_name: str, document_name: str, client):
    """é¢„è§ˆæ–‡æ¡£çš„å¤‡ç”¨æ–¹æ³• - åˆ†å—é‡ç»„"""
    try:
        # æœç´¢åŒ…å«è¯¥æ–‡æ¡£çš„æ‰€æœ‰ç‚¹ - æ”¯æŒéƒ¨åˆ†åŒ¹é…
        search_result = client.scroll(
            collection_name=kb_name,
            scroll_filter=qdrant_models.Filter(
                must=[
                    qdrant_models.FieldCondition(
                        key="metadata.source",
                        match=qdrant_models.MatchText(text=document_name)  # ä½¿ç”¨æ–‡æœ¬åŒ¹é…è€Œä¸æ˜¯ç²¾ç¡®åŒ¹é…
                    )
                ]
            ),
            limit=1000
        )
        
        if not search_result[0]:
            return {
                "success": False,
                "message": f"æ–‡æ¡£ '{document_name}' ä¸å­˜åœ¨",
                "data": {}
            }
        
        # åˆå¹¶æ‰€æœ‰å—çš„å†…å®¹
        chunks = []
        for point in search_result[0]:
            metadata = point.payload.get("metadata", {})
            chunks.append({
                "title": metadata.get("title", ""),
                "content": metadata.get("content", ""),
                "level": metadata.get("level", 1),
                "source": metadata.get("source", "")
            })
        
        # æŒ‰levelæ’åºï¼Œç¡®ä¿æ­£ç¡®çš„å±‚çº§ç»“æ„
        chunks.sort(key=lambda x: x.get("level", 1))
        
        # è·å–çœŸå®çš„æ–‡æ¡£å
        real_document_name = document_name
        if chunks:
            real_document_name = chunks[0].get("source", document_name)
        
        # ç”Ÿæˆé¢„è§ˆå†…å®¹
        preview_content = ""
        for chunk in chunks:
            if chunk["title"]:
                level = chunk.get("level", 1)
                preview_content += f"{'#' * level} {chunk['title']}\n\n"
            if chunk["content"]:
                preview_content += chunk["content"] + "\n\n"
        
        # å¤„ç†å›¾ç‰‡è·¯å¾„ï¼šå°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        import re
        def replace_image_path(match):
            image_path = match.group(1)
            # å¦‚æœå·²ç»æ˜¯ç»å¯¹è·¯å¾„ï¼Œç›´æ¥è¿”å›
            if image_path.startswith('/') or image_path.startswith('http'):
                return match.group(0)
            
            # å°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
            # æ ¼å¼ï¼š![](image_path) -> ![](/marker_outputs/{base_name}/{base_name}_images/{image_path})
            base_name = os.path.splitext(real_document_name)[0]
            absolute_path = f"/marker_outputs/{base_name}/{base_name}_images/{image_path}"
            return f"![]({absolute_path})"
        
        # æ›¿æ¢æ‰€æœ‰å›¾ç‰‡è·¯å¾„
        preview_content = re.sub(r'!\[([^\]]*)\]\(([^)]+)\)', replace_image_path, preview_content)
        
        return {
            "success": True,
            "message": f"æ–‡æ¡£ '{real_document_name}' é¢„è§ˆå†…å®¹ï¼ˆåˆ†å—é‡ç»„ï¼‰",
            "data": {
                "document_name": real_document_name,
                "content": preview_content,
                "content_type": "reconstructed",
                "chunks_count": len(chunks)
            }
        }
        
    except Exception as e:
        logger.error(f"åˆ†å—é‡ç»„é¢„è§ˆå¤±è´¥: {str(e)}")
        return {
            "success": False,
            "message": f"åˆ†å—é‡ç»„é¢„è§ˆå¤±è´¥: {str(e)}",
            "data": {}
        }

# æ³¨å†Œå­åº”ç”¨
# æ·»åŠ é™æ€æ–‡ä»¶æœåŠ¡ï¼ˆå¿…é¡»åœ¨è·¯ç”±ä¹‹å‰ï¼‰
import os
static_dir = os.path.join(os.path.dirname(__file__), "static")
print(f"é™æ€æ–‡ä»¶ç›®å½•: {static_dir}")
print(f"ustc.svgå­˜åœ¨: {os.path.exists(os.path.join(static_dir, 'ustc.svg'))}")

# ä½¿ç”¨ç»å¯¹è·¯å¾„æŒ‚è½½é™æ€æ–‡ä»¶
app.mount("/static", StaticFiles(directory=static_dir), name="static")

# æ·»åŠ ä¸€ä¸ªç®€å•çš„æµ‹è¯•è·¯ç”±æ¥éªŒè¯é™æ€æ–‡ä»¶
@app.get("/test-static")
async def test_static():
    return {"static_dir": static_dir, "exists": os.path.exists(os.path.join(static_dir, "ustc.svg"))}

# æ·»åŠ ä¸€ä¸ªç›´æ¥çš„è·¯ç”±æ¥æœåŠ¡SVGæ–‡ä»¶
@app.get("/ustc.svg")
async def serve_ustc_svg():
    svg_path = os.path.join(static_dir, "ustc.svg")
    if os.path.exists(svg_path):
        return FileResponse(svg_path, media_type="image/svg+xml")
    else:
        raise HTTPException(status_code=404, detail="SVG file not found")

# æ·»åŠ å­è·¯å¾„ç‰ˆæœ¬çš„SVGè·¯ç”±
@app.get("/nsrlchat/ustc.svg")
async def serve_ustc_svg_subpath():
    svg_path = os.path.join(static_dir, "ustc.svg")
    if os.path.exists(svg_path):
        return FileResponse(svg_path, media_type="image/svg+xml")
    else:
        raise HTTPException(status_code=404, detail="SVG file not found")

# æ·»åŠ markerè¾“å‡ºå›¾ç‰‡çš„é™æ€æ–‡ä»¶æœåŠ¡
app.mount("/marker_outputs", StaticFiles(directory="marker_outputs"), name="marker_outputs")

app.include_router(kb_router)
app.include_router(agent_router)
app.include_router(auth_router)

# æ·»åŠ æ ¹è·¯å¾„é‡å®šå‘åˆ°Webç•Œé¢ï¼ˆéœ€è¦è®¤è¯ï¼‰
@app.get("/")
async def read_root(current_user: UserResponse = Depends(get_current_user)):
    return FileResponse('ustc/static/index.html')

# æ¬¢è¿é¡µé¢è·¯ç”±ï¼ˆæš‚æ—¶ä¿ç•™ï¼Œç­‰USTCç”³è¯·å®Œæˆåå¯ç”¨ï¼‰
# @app.get("/welcome")
# async def welcome_page():
#     """æ˜¾ç¤ºæ¬¢è¿é¡µé¢"""
#     return FileResponse('ustc/static/welcome.html')

# æ·»åŠ ä¸Šä¼ é¡µé¢è·¯ç”±ï¼ˆéœ€è¦çŸ¥è¯†åº“è´¡çŒ®è€…æˆ–ç®¡ç†å‘˜æƒé™ï¼‰
@app.get("/upload.html")
async def upload_page(current_user: UserResponse = Depends(get_current_contributor_user)):
    return FileResponse('ustc/static/upload.html')

# æ·»åŠ æµ‹è¯•é¡µé¢è·¯ç”±
@app.get("/test_documents.html")
async def test_documents_page():
    return FileResponse('test_documents.html')

# æ·»åŠ ä¸Šä¼ æµ‹è¯•é¡µé¢è·¯ç”±
@app.get("/test_upload.html")
async def test_upload_page():
    return FileResponse('../test_upload.html')


# å¥åº·æ£€æŸ¥ç«¯ç‚¹ï¼ˆç»Ÿä¸€ï¼‰
@app.get("/health")
async def health_check():
    """ç»Ÿä¸€çš„å¥åº·æ£€æŸ¥"""
    try:
        # æ£€æŸ¥Qdrant
        global qdrant_client
        kb_status = "disconnected"
        total_kb = 0
        if qdrant_client:
            try:
                collections = qdrant_client.get_collections().collections
                kb_status = "connected"
                total_kb = len(collections)
            except Exception as e:
                kb_status = f"error: {str(e)}"
        
        # æ£€æŸ¥æ•°æ®åº“
        db_status = "connected" if checkpointer else "disconnected"
        
        # æ£€æŸ¥Agentå›¾
        agent_status = "initialized" if graph else "not_initialized"
        
        return {
            "status": "healthy",
            "knowledge_base": kb_status,
            "agent_database": db_status,
            "agent_status": agent_status,
            "total_knowledge_bases": total_kb,
            "service_info": {
                "port": 8000,
                "kb_api_prefix": "/kb",
                "agent_api_prefix": "/agent"
            }
        }
    except Exception as e:
        logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}", exc_info=True)
        return {
            "status": "unhealthy",
            "error": str(e),
            "service_info": {
                "port": 8000,
                "kb_api_prefix": "/kb",
                "agent_api_prefix": "/agent"
            }
        }

if __name__ == "__main__":
    import uvicorn
    logger.info("å¯åŠ¨ç»Ÿä¸€æœåŠ¡...")
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")