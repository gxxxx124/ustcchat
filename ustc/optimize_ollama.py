#!/usr/bin/env python3
"""
Ollamaæ˜¾å­˜ä¼˜åŒ–é…ç½®è„šæœ¬
å¸®åŠ©ä¼˜åŒ–OllamaæœåŠ¡çš„æ˜¾å­˜ä½¿ç”¨ï¼Œé˜²æ­¢æ˜¾å­˜æ³„æ¼
"""

import json
import subprocess
import time
import requests
from pathlib import Path
import os

class OllamaOptimizer:
    def __init__(self, ollama_url="http://localhost:11434"):
        self.ollama_url = ollama_url
        self.config_dir = Path.home() / ".ollama"
        
    def check_ollama_status(self):
        """æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                print("âœ… OllamaæœåŠ¡è¿è¡Œæ­£å¸¸")
                return True
            else:
                print(f"âš ï¸  OllamaæœåŠ¡å“åº”å¼‚å¸¸: {response.status_code}")
                return False
        except Exception as e:
            print(f"âŒ æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡: {e}")
            return False
    
    def get_loaded_models(self):
        """è·å–å·²åŠ è½½çš„æ¨¡å‹"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags")
            if response.status_code == 200:
                models = response.json().get("models", [])
                print(f"ğŸ“‹ å·²åŠ è½½æ¨¡å‹æ•°é‡: {len(models)}")
                for model in models:
                    print(f"   - {model.get('name', 'Unknown')} ({model.get('size', 0)} bytes)")
                return models
            return []
        except Exception as e:
            print(f"âŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def unload_unused_models(self):
        """å¸è½½æœªä½¿ç”¨çš„æ¨¡å‹ä»¥é‡Šæ”¾æ˜¾å­˜"""
        try:
            # è·å–æ­£åœ¨è¿è¡Œçš„æ¨¡å‹
            response = requests.get(f"{self.ollama_url}/api/ps")
            if response.status_code == 200:
                running_models = response.json().get("models", [])
                print(f"ğŸ”„ æ­£åœ¨è¿è¡Œçš„æ¨¡å‹: {len(running_models)}")
                
                # å¸è½½æ‰€æœ‰æ¨¡å‹ä»¥é‡Šæ”¾æ˜¾å­˜
                for model in running_models:
                    model_name = model.get("name")
                    if model_name:
                        print(f"   ğŸ—‘ï¸  å¸è½½æ¨¡å‹: {model_name}")
                        try:
                            unload_response = requests.post(f"{self.ollama_url}/api/generate", 
                                                         json={"model": model_name, "prompt": "", "stream": False})
                            if unload_response.status_code == 200:
                                print(f"      âœ… æ¨¡å‹ {model_name} å¸è½½æˆåŠŸ")
                            else:
                                print(f"      âš ï¸  æ¨¡å‹ {model_name} å¸è½½å¤±è´¥")
                        except Exception as e:
                            print(f"      âŒ å¸è½½æ¨¡å‹ {model_name} æ—¶å‡ºé”™: {e}")
                
                print("ğŸ§¹ æ˜¾å­˜æ¸…ç†å®Œæˆ")
                return True
            return False
        except Exception as e:
            print(f"âŒ å¸è½½æ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def optimize_model_config(self, model_name="qwen3:4b"):
        """ä¼˜åŒ–æ¨¡å‹é…ç½®ä»¥å‡å°‘æ˜¾å­˜å ç”¨"""
        config = {
            "model": model_name,
            "options": {
                "num_ctx": 2048,        # å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦
                "num_gpu": 1,           # é™åˆ¶GPUæ•°é‡
                "num_thread": 4,        # é™åˆ¶çº¿ç¨‹æ•°
                "f16": True,            # ä½¿ç”¨åŠç²¾åº¦æµ®ç‚¹æ•°
                "low_vram": True,       # å¯ç”¨ä½æ˜¾å­˜æ¨¡å¼
                "rope_scaling": {"type": "linear", "factor": 1.0},  # ä¼˜åŒ–ä½ç½®ç¼–ç 
                "mirostat": 2,          # å¯ç”¨mirostaté‡‡æ ·
                "mirostat_tau": 5.0,    # è®¾ç½®mirostatå‚æ•°
                "mirostat_eta": 0.1
            }
        }
        
        print(f"âš™ï¸  ä¼˜åŒ–æ¨¡å‹é…ç½®: {model_name}")
        print(f"   ä¸Šä¸‹æ–‡é•¿åº¦: {config['options']['num_ctx']}")
        print(f"   ä½æ˜¾å­˜æ¨¡å¼: {config['options']['low_vram']}")
        print(f"   åŠç²¾åº¦: {config['options']['f16']}")
        
        return config
    
    def create_optimized_pull_command(self, model_name="qwen3:4b"):
        """åˆ›å»ºä¼˜åŒ–çš„æ¨¡å‹æ‹‰å–å‘½ä»¤"""
        config = self.optimize_model_config(model_name)
        
        # æ„å»ºollama pullå‘½ä»¤
        cmd = f"ollama pull {model_name}"
        
        # åˆ›å»ºModelfile
        modelfile_content = f"""FROM {model_name}
PARAMETER num_ctx {config['options']['num_ctx']}
PARAMETER num_gpu {config['options']['num_gpu']}
PARAMETER num_thread {config['options']['num_thread']}
PARAMETER f16 {str(config['options']['f16']).lower()}
PARAMETER low_vram {str(config['options']['low_vram']).lower()}
PARAMETER rope_scaling {json.dumps(config['options']['rope_scaling'])}
PARAMETER mirostat {config['options']['mirostat']}
PARAMETER mirostat_tau {config['options']['mirostat_tau']}
PARAMETER mirostat_eta {config['options']['mirostat_eta']}
"""
        
        modelfile_path = self.config_dir / f"{model_name}.Modelfile"
        modelfile_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(modelfile_path, 'w') as f:
            f.write(modelfile_content)
        
        print(f"ğŸ“ å·²åˆ›å»ºä¼˜åŒ–é…ç½®æ–‡ä»¶: {modelfile_path}")
        print(f"ğŸš€ ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ‹‰å–ä¼˜åŒ–æ¨¡å‹:")
        print(f"   ollama create {model_name}-optimized -f {modelfile_path}")
        print(f"   ollama run {model_name}-optimized")
        
        return modelfile_path
    
    def monitor_memory_usage(self, duration=60):
        """ç›‘æ§æ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
        print(f"ğŸ“Š å¼€å§‹ç›‘æ§æ˜¾å­˜ä½¿ç”¨æƒ…å†µ ({duration}ç§’)")
        start_time = time.time()
        
        try:
            while time.time() - start_time < duration:
                # è·å–GPUä¿¡æ¯
                result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used,memory.free', '--format=csv,noheader,nounits'], 
                                      capture_output=True, text=True, check=True)
                
                if result.stdout.strip():
                    lines = result.stdout.strip().split('\n')
                    for i, line in enumerate(lines):
                        if line.strip():
                            parts = line.split(', ')
                            if len(parts) >= 2:
                                used = int(parts[0])
                                free = int(parts[1])
                                total = used + free
                                usage_percent = (used / total) * 100
                                
                                print(f"GPU {i}: {used}MB / {total}MB ({usage_percent:.1f}%) | å¯ç”¨: {free}MB")
                
                time.sleep(5)
                
        except KeyboardInterrupt:
            print("\nğŸ›‘ ç›‘æ§å·²åœæ­¢")
        except Exception as e:
            print(f"âŒ ç›‘æ§å‡ºé”™: {e}")
    
    def restart_ollama_service(self):
        """é‡å¯OllamaæœåŠ¡ä»¥æ¸…ç†æ˜¾å­˜"""
        print("ğŸ”„ é‡å¯OllamaæœåŠ¡...")
        
        try:
            # åœæ­¢OllamaæœåŠ¡
            subprocess.run(['pkill', '-f', 'ollama'], check=False)
            time.sleep(2)
            
            # å¯åŠ¨OllamaæœåŠ¡
            subprocess.Popen(['ollama', 'serve'], 
                           stdout=subprocess.DEVNULL, 
                           stderr=subprocess.DEVNULL)
            
            print("â³ ç­‰å¾…æœåŠ¡å¯åŠ¨...")
            time.sleep(10)
            
            # æ£€æŸ¥æœåŠ¡çŠ¶æ€
            if self.check_ollama_status():
                print("âœ… OllamaæœåŠ¡é‡å¯æˆåŠŸ")
                return True
            else:
                print("âŒ OllamaæœåŠ¡é‡å¯å¤±è´¥")
                return False
                
        except Exception as e:
            print(f"âŒ é‡å¯æœåŠ¡å¤±è´¥: {e}")
            return False

def main():
    print("ğŸš€ Ollamaæ˜¾å­˜ä¼˜åŒ–å·¥å…·")
    print("=" * 50)
    
    optimizer = OllamaOptimizer()
    
    # æ£€æŸ¥æœåŠ¡çŠ¶æ€
    if not optimizer.check_ollama_status():
        print("âŒ OllamaæœåŠ¡æœªè¿è¡Œï¼Œè¯·å…ˆå¯åŠ¨æœåŠ¡")
        return
    
    while True:
        print("\nğŸ“‹ è¯·é€‰æ‹©æ“ä½œ:")
        print("1. æ£€æŸ¥å½“å‰æ¨¡å‹çŠ¶æ€")
        print("2. å¸è½½æœªä½¿ç”¨æ¨¡å‹")
        print("3. åˆ›å»ºä¼˜åŒ–é…ç½®")
        print("4. ç›‘æ§æ˜¾å­˜ä½¿ç”¨")
        print("5. é‡å¯OllamaæœåŠ¡")
        print("6. é€€å‡º")
        
        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-6): ").strip()
        
        if choice == "1":
            optimizer.get_loaded_models()
        elif choice == "2":
            optimizer.unload_unused_models()
        elif choice == "3":
            model_name = input("è¯·è¾“å…¥æ¨¡å‹åç§° (é»˜è®¤: qwen3:4b): ").strip() or "qwen3:4b"
            optimizer.create_optimized_pull_command(model_name)
        elif choice == "4":
            duration = input("è¯·è¾“å…¥ç›‘æ§æ—¶é•¿(ç§’, é»˜è®¤60): ").strip()
            duration = int(duration) if duration.isdigit() else 60
            optimizer.monitor_memory_usage(duration)
        elif choice == "5":
            optimizer.restart_ollama_service()
        elif choice == "6":
            print("ğŸ‘‹ å†è§!")
            break
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")

if __name__ == "__main__":
    main()
